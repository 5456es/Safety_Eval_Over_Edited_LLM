/home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=0, data_source=ZsRE, data_size=1, random=False
Loading data from ../../data/edit_data/merged_data_part_0.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 1 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_1/part_0/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/mistral-7b-instruct-v0.3
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_1/part_0
batch_size: 1
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: mistral-7b-instruct-v0.3_ZsRE_1
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [01:20<02:41, 80.95s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [02:35<01:17, 77.13s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [03:56<00:00, 79.17s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [03:56<00:00, 79.00s/it]
Using custom data configuration default-d9151fbbbddd0823
Reusing dataset json (/home/k/kduan/.cache/huggingface/datasets/json/default-d9151fbbbddd0823/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b)
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.25it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.25it/s]
Loading cached shuffled indices for dataset at /home/k/kduan/.cache/huggingface/datasets/json/default-d9151fbbbddd0823/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b/cache-b8934ff3e455b78a.arrow
Parameter 'function'=<function train.<locals>.<lambda> at 0x7fd28c41a940> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Loading cached processed dataset at /home/k/kduan/.cache/huggingface/datasets/json/default-d9151fbbbddd0823/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b/cache-1c80317fa3b1799d.arrow
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241104_221325-i0g8oe67
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mistral-7b-instruct-v0.3_ZsRE_1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/i0g8oe67
trainable params: 28,311,552 || all params: 7,276,335,104 || trainable%: 0.3890908210705739

Example:
<s> igh the [control_512] [control_360] [control_336] ano [control_302] ub [control_355] SEDlationlikÁªÑov [control_289] </s>


Example:
<s> [control_448] kill [control_308] sur przez [control_348] [control_534] [control_368] [control_259] [control_717] [control_336] strengÁªÑ extract [control_372] ‚Äî [control_348] [control_534] [control_368] [control_259] [control_717] </s>


Example:
<s> separate)ÔøΩ Hug draw [control_331] [control_540] compatible larger [control_302] ÁªÑ incl Zuicro</s>


Example:
<s> ighued [control_336] [control_433] planets #Wh‰Ωç„Åó [control_472] —Ç–æ…ô Setting themselvesÁªÑabor Wh;</s>


Example:
<s> constantly [control_336] links [control_406] Authors [control_433] [control_664] ÔøΩ killÁªÑ Authors [control_676] [control_547] ≈•aken</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 10
})
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:37<1:01:34, 37.32s/it]                                                   1%|          | 1/100 [00:37<1:01:34, 37.32s/it]  2%|‚ñè         | 2/100 [00:37<25:28, 15.60s/it]                                                   2%|‚ñè         | 2/100 [00:37<25:28, 15.60s/it]  3%|‚ñé         | 3/100 [00:38<13:57,  8.64s/it]                                                 3%|‚ñé         | 3/100 [00:38<13:57,  8.64s/it]  4%|‚ñç         | 4/100 [00:38<08:35,  5.37s/it]                                                 4%|‚ñç         | 4/100 [00:38<08:35,  5.37s/it]  5%|‚ñå         | 5/100 [00:38<05:38,  3.56s/it]                                                 5%|‚ñå         | 5/100 [00:38<05:38,  3.56s/it]  6%|‚ñå         | 6/100 [00:39<03:52,  2.47s/it]                                                 6%|‚ñå         | 6/100 [00:39<03:52,  2.47s/it]  7%|‚ñã         | 7/100 [00:39<02:45,  1.78s/it]                                                 7%|‚ñã         | 7/100 [00:39<02:45,  1.78s/it]  8%|‚ñä         | 8/100 [00:39<02:02,  1.33s/it]                                                 8%|‚ñä         | 8/100 [00:39<02:02,  1.33s/it]  9%|‚ñâ         | 9/100 [00:40<01:33,  1.02s/it]                                                 9%|‚ñâ         | 9/100 [00:40<01:33,  1.02s/it] 10%|‚ñà         | 10/100 [00:40<01:13,  1.22it/s]                                                 10%|‚ñà         | 10/100 [00:40<01:13,  1.22it/s] 11%|‚ñà         | 11/100 [00:40<01:00,  1.48it/s]                                                 11%|‚ñà         | 11/100 [00:40<01:00,  1.48it/s] 12%|‚ñà‚ñè        | 12/100 [00:41<00:50,  1.73it/s]                                                 12%|‚ñà‚ñè        | 12/100 [00:41<00:50,  1.73it/s] 13%|‚ñà‚ñé        | 13/100 [00:41<00:44,  1.95it/s]                                                 13%|‚ñà‚ñé        | 13/100 [00:41<00:44,  1.95it/s] 14%|‚ñà‚ñç        | 14/100 [00:41<00:40,  2.15it/s]                                                 14%|‚ñà‚ñç        | 14/100 [00:41<00:40,  2.15it/s] 15%|‚ñà‚ñå        | 15/100 [00:42<00:36,  2.32it/s]                                                 15%|‚ñà‚ñå        | 15/100 [00:42<00:36,  2.32it/s] 16%|‚ñà‚ñå        | 16/100 [00:42<00:34,  2.44it/s]                                                 16%|‚ñà‚ñå        | 16/100 [00:42<00:34,  2.44it/s] 17%|‚ñà‚ñã        | 17/100 [00:43<00:32,  2.54it/s]                                                 17%|‚ñà‚ñã        | 17/100 [00:43<00:32,  2.54it/s] 18%|‚ñà‚ñä        | 18/100 [00:43<00:31,  2.62it/s]                                                 18%|‚ñà‚ñä        | 18/100 [00:43<00:31,  2.62it/s] 19%|‚ñà‚ñâ        | 19/100 [00:43<00:30,  2.68it/s]                                                 19%|‚ñà‚ñâ        | 19/100 [00:43<00:30,  2.68it/s] 20%|‚ñà‚ñà        | 20/100 [00:44<00:29,  2.72it/s]                                                 20%|‚ñà‚ñà        | 20/100 [00:44<00:29,  2.72it/s] 21%|‚ñà‚ñà        | 21/100 [00:44<00:28,  2.75it/s]                                                 21%|‚ñà‚ñà        | 21/100 [00:44<00:28,  2.75it/s] 22%|‚ñà‚ñà‚ñè       | 22/100 [00:44<00:28,  2.76it/s]                                                 22%|‚ñà‚ñà‚ñè       | 22/100 [00:44<00:28,  2.76it/s] 23%|‚ñà‚ñà‚ñé       | 23/100 [00:45<00:27,  2.77it/s]                                                 23%|‚ñà‚ñà‚ñé       | 23/100 [00:45<00:27,  2.77it/s] 24%|‚ñà‚ñà‚ñç       | 24/100 [00:45<00:27,  2.79it/s]                                                 24%|‚ñà‚ñà‚ñç       | 24/100 [00:45<00:27,  2.79it/s] 25%|‚ñà‚ñà‚ñå       | 25/100 [00:45<00:26,  2.81it/s]                                                 25%|‚ñà‚ñà‚ñå       | 25/100 [00:45<00:26,  2.81it/s] 26%|‚ñà‚ñà‚ñå       | 26/100 [00:46<00:26,  2.82it/s]                                                 26%|‚ñà‚ñà‚ñå       | 26/100 [00:46<00:26,  2.82it/s] 27%|‚ñà‚ñà‚ñã       | 27/100 [00:46<00:25,  2.83it/s]                                                 27%|‚ñà‚ñà‚ñã       | 27/100 [00:46<00:25,  2.83it/s] 28%|‚ñà‚ñà‚ñä       | 28/100 [00:46<00:25,  2.83it/s]                                                 28%|‚ñà‚ñà‚ñä       | 28/100 [00:46<00:25,  2.83it/s] 29%|‚ñà‚ñà‚ñâ       | 29/100 [00:47<00:24,  2.85it/s]                                                 29%|‚ñà‚ñà‚ñâ       | 29/100 [00:47<00:24,  2.85it/s] 30%|‚ñà‚ñà‚ñà       | 30/100 [00:47<00:24,  2.85it/s]                                                 30%|‚ñà‚ñà‚ñà       | 30/100 [00:47<00:24,  2.85it/s] 31%|‚ñà‚ñà‚ñà       | 31/100 [00:47<00:24,  2.86it/s]                                                 31%|‚ñà‚ñà‚ñà       | 31/100 [00:47<00:24,  2.86it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 32/100 [00:48<00:23,  2.85it/s]                                                 32%|‚ñà‚ñà‚ñà‚ñè      | 32/100 [00:48<00:23,  2.85it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:48<00:23,  2.86it/s]                                                 33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:48<00:23,  2.86it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 34/100 [00:49<00:23,  2.86it/s]                                                 34%|‚ñà‚ñà‚ñà‚ñç      | 34/100 [00:49<00:23,  2.86it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 35/100 [00:49<00:22,  2.85it/s]                                                 35%|‚ñà‚ñà‚ñà‚ñå      | 35/100 [00:49<00:22,  2.85it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 36/100 [00:49<00:22,  2.85it/s]                                                 36%|‚ñà‚ñà‚ñà‚ñå      | 36/100 [00:49<00:22,  2.85it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 37/100 [00:50<00:24,  2.59it/s]                                                 37%|‚ñà‚ñà‚ñà‚ñã      | 37/100 [00:50<00:24,  2.59it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 38/100 [00:50<00:23,  2.67it/s]                                                 38%|‚ñà‚ñà‚ñà‚ñä      | 38/100 [00:50<00:23,  2.67it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 39/100 [00:50<00:22,  2.74it/s]                                                 39%|‚ñà‚ñà‚ñà‚ñâ      | 39/100 [00:50<00:22,  2.74it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 40/100 [00:51<00:21,  2.77it/s]                                                 40%|‚ñà‚ñà‚ñà‚ñà      | 40/100 [00:51<00:21,  2.77it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:51<00:21,  2.79it/s]                                                 41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:51<00:21,  2.79it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 42/100 [00:51<00:20,  2.81it/s]                                                 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 42/100 [00:51<00:20,  2.81it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 43/100 [00:52<00:20,  2.82it/s]                                                 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 43/100 [00:52<00:20,  2.82it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 44/100 [00:52<00:19,  2.84it/s]                                                 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 44/100 [00:52<00:19,  2.84it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 45/100 [00:53<00:19,  2.84it/s]                                                 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 45/100 [00:53<00:19,  2.84it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 46/100 [00:53<00:19,  2.84it/s]                                                 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 46/100 [00:53<00:19,  2.84it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 47/100 [00:53<00:18,  2.85it/s]                                                 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 47/100 [00:53<00:18,  2.85it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 48/100 [00:54<00:18,  2.84it/s]                                                 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 48/100 [00:54<00:18,  2.84it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:54<00:17,  2.85it/s]                                                 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:54<00:17,  2.85it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 50/100 [00:54<00:17,  2.85it/s]                                                 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 50/100 [00:54<00:17,  2.85it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 51/100 [00:55<00:17,  2.86it/s]                                                 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 51/100 [00:55<00:17,  2.86it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 52/100 [00:55<00:16,  2.85it/s]                                                 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 52/100 [00:55<00:16,  2.85it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 53/100 [00:55<00:16,  2.85it/s]                                                 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 53/100 [00:55<00:16,  2.85it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 54/100 [00:56<00:16,  2.84it/s]                                                 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 54/100 [00:56<00:16,  2.84it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 55/100 [00:56<00:15,  2.85it/s]                                                 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 55/100 [00:56<00:15,  2.85it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 56/100 [00:56<00:15,  2.85it/s]                                                 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 56/100 [00:56<00:15,  2.85it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:57<00:15,  2.84it/s]                                                 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:57<00:15,  2.84it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 58/100 [00:57<00:14,  2.85it/s]                                                 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 58/100 [00:57<00:14,  2.85it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 59/100 [00:57<00:14,  2.86it/s]                                                 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 59/100 [00:57<00:14,  2.86it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 60/100 [00:58<00:14,  2.85it/s]                                                 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 60/100 [00:58<00:14,  2.85it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 61/100 [00:58<00:13,  2.86it/s]                                                 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 61/100 [00:58<00:13,  2.86it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 62/100 [00:58<00:13,  2.85it/s]                                                 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 62/100 [00:58<00:13,  2.85it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 63/100 [00:59<00:13,  2.84it/s]                                                 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 63/100 [00:59<00:13,  2.84it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 64/100 [00:59<00:12,  2.85it/s]                                                 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 64/100 [00:59<00:12,  2.85it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [01:00<00:12,  2.86it/s]                                                 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [01:00<00:12,  2.86it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 66/100 [01:00<00:11,  2.85it/s]                                                 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 66/100 [01:00<00:11,  2.85it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 67/100 [01:00<00:11,  2.85it/s]                                                 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 67/100 [01:00<00:11,  2.85it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 68/100 [01:01<00:11,  2.84it/s]                                                 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 68/100 [01:01<00:11,  2.84it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 69/100 [01:01<00:10,  2.84it/s]                                                 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 69/100 [01:01<00:10,  2.84it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 70/100 [01:01<00:10,  2.85it/s]                                                 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 70/100 [01:01<00:10,  2.85it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 71/100 [01:02<00:10,  2.83it/s]                                                 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 71/100 [01:02<00:10,  2.83it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 72/100 [01:02<00:09,  2.83it/s]                                                 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 72/100 [01:02<00:09,  2.83it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [01:02<00:09,  2.84it/s]                                                 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [01:02<00:09,  2.84it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 74/100 [01:03<00:09,  2.85it/s]                                                 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 74/100 [01:03<00:09,  2.85it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 75/100 [01:03<00:08,  2.84it/s]                                                 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 75/100 [01:03<00:08,  2.84it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 76/100 [01:03<00:08,  2.85it/s]                                                 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 76/100 [01:03<00:08,  2.85it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 77/100 [01:04<00:08,  2.86it/s]                                                 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 77/100 [01:04<00:08,  2.86it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 78/100 [01:04<00:07,  2.85it/s]                                                 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 78/100 [01:04<00:07,  2.85it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 79/100 [01:04<00:07,  2.85it/s]                                                 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 79/100 [01:04<00:07,  2.85it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 80/100 [01:05<00:07,  2.84it/s]                                                 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 80/100 [01:05<00:07,  2.84it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [01:05<00:06,  2.83it/s]                                                 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [01:05<00:06,  2.83it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 82/100 [01:05<00:06,  2.83it/s]                                                 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 82/100 [01:05<00:06,  2.83it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 83/100 [01:06<00:05,  2.84it/s]                                                 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 83/100 [01:06<00:05,  2.84it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 84/100 [01:06<00:05,  2.84it/s]                                                 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 84/100 [01:06<00:05,  2.84it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 85/100 [01:07<00:05,  2.84it/s]                                                 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 85/100 [01:07<00:05,  2.84it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 86/100 [01:07<00:04,  2.85it/s]                                                 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 86/100 [01:07<00:04,  2.85it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 87/100 [01:07<00:04,  2.84it/s]                                                {'loss': 13.0447, 'grad_norm': 97.56524658203125, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.1}
{'loss': 16.4965, 'grad_norm': 127.48409271240234, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.2}
{'loss': 18.1123, 'grad_norm': 93.82418060302734, 'learning_rate': 1.2e-05, 'epoch': 0.3}
{'loss': 14.7102, 'grad_norm': 97.23992919921875, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.4}
{'loss': 14.602, 'grad_norm': 99.36064910888672, 'learning_rate': 2e-05, 'epoch': 0.5}
{'loss': 15.0385, 'grad_norm': 57.16433334350586, 'learning_rate': 2.4e-05, 'epoch': 0.6}
{'loss': 15.0111, 'grad_norm': 101.72592163085938, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.7}
{'loss': 10.6333, 'grad_norm': 60.87607955932617, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.8}
{'loss': 11.1007, 'grad_norm': 105.32695007324219, 'learning_rate': 3.6e-05, 'epoch': 0.9}
{'loss': 13.1016, 'grad_norm': 243.55886840820312, 'learning_rate': 4e-05, 'epoch': 1.0}
{'loss': 7.84, 'grad_norm': 107.84066009521484, 'learning_rate': 4.4000000000000006e-05, 'epoch': 1.1}
{'loss': 10.8343, 'grad_norm': 71.07293701171875, 'learning_rate': 4.8e-05, 'epoch': 1.2}
{'loss': 8.0362, 'grad_norm': 40.531131744384766, 'learning_rate': 5.2000000000000004e-05, 'epoch': 1.3}
{'loss': 10.1235, 'grad_norm': 52.557308197021484, 'learning_rate': 5.6000000000000006e-05, 'epoch': 1.4}
{'loss': 10.2465, 'grad_norm': 74.50199127197266, 'learning_rate': 6e-05, 'epoch': 1.5}
{'loss': 13.8687, 'grad_norm': 101.21783447265625, 'learning_rate': 6.400000000000001e-05, 'epoch': 1.6}
{'loss': 10.7477, 'grad_norm': 45.0943717956543, 'learning_rate': 6.800000000000001e-05, 'epoch': 1.7}
{'loss': 9.2123, 'grad_norm': 72.25391387939453, 'learning_rate': 7.2e-05, 'epoch': 1.8}
{'loss': 8.2373, 'grad_norm': 52.030696868896484, 'learning_rate': 7.6e-05, 'epoch': 1.9}
{'loss': 5.2618, 'grad_norm': 78.42823791503906, 'learning_rate': 8e-05, 'epoch': 2.0}
{'loss': 4.8081, 'grad_norm': 62.69926071166992, 'learning_rate': 8.4e-05, 'epoch': 2.1}
{'loss': 7.5746, 'grad_norm': 55.20124435424805, 'learning_rate': 8.800000000000001e-05, 'epoch': 2.2}
{'loss': 2.0483, 'grad_norm': 55.71601486206055, 'learning_rate': 9.200000000000001e-05, 'epoch': 2.3}
{'loss': 4.0617, 'grad_norm': 250.83584594726562, 'learning_rate': 9.6e-05, 'epoch': 2.4}
{'loss': 3.3795, 'grad_norm': 26.893909454345703, 'learning_rate': 0.0001, 'epoch': 2.5}
{'loss': 4.7683, 'grad_norm': 62.16407012939453, 'learning_rate': 0.00010400000000000001, 'epoch': 2.6}
{'loss': 8.5214, 'grad_norm': 65.18161010742188, 'learning_rate': 0.00010800000000000001, 'epoch': 2.7}
{'loss': 4.7356, 'grad_norm': 69.62613677978516, 'learning_rate': 0.00011200000000000001, 'epoch': 2.8}
{'loss': 4.6676, 'grad_norm': 117.62886810302734, 'learning_rate': 0.000116, 'epoch': 2.9}
{'loss': 5.5221, 'grad_norm': 132.19155883789062, 'learning_rate': 0.00012, 'epoch': 3.0}
{'loss': 2.4554, 'grad_norm': 20.215497970581055, 'learning_rate': 0.000124, 'epoch': 3.1}
{'loss': 3.2133, 'grad_norm': 42.302032470703125, 'learning_rate': 0.00012800000000000002, 'epoch': 3.2}
{'loss': 0.3861, 'grad_norm': 32.60385513305664, 'learning_rate': 0.000132, 'epoch': 3.3}
{'loss': 0.3076, 'grad_norm': 19.83220863342285, 'learning_rate': 0.00013600000000000003, 'epoch': 3.4}
{'loss': 3.355, 'grad_norm': 50.43083572387695, 'learning_rate': 0.00014, 'epoch': 3.5}
{'loss': 0.5931, 'grad_norm': 40.4589958190918, 'learning_rate': 0.000144, 'epoch': 3.6}
{'loss': 3.361, 'grad_norm': 72.48821258544922, 'learning_rate': 0.000148, 'epoch': 3.7}
{'loss': 1.2684, 'grad_norm': 127.5118408203125, 'learning_rate': 0.000152, 'epoch': 3.8}
{'loss': 2.922, 'grad_norm': 101.78898620605469, 'learning_rate': 0.00015600000000000002, 'epoch': 3.9}
{'loss': 8.9037, 'grad_norm': 155.39263916015625, 'learning_rate': 0.00016, 'epoch': 4.0}
{'loss': 0.0867, 'grad_norm': 26.130517959594727, 'learning_rate': 0.000164, 'epoch': 4.1}
{'loss': 0.0023, 'grad_norm': 0.3242958188056946, 'learning_rate': 0.000168, 'epoch': 4.2}
{'loss': 7.3776, 'grad_norm': 43.672576904296875, 'learning_rate': 0.000172, 'epoch': 4.3}
{'loss': 0.396, 'grad_norm': 51.84617233276367, 'learning_rate': 0.00017600000000000002, 'epoch': 4.4}
{'loss': 2.3274, 'grad_norm': 42.60013961791992, 'learning_rate': 0.00018, 'epoch': 4.5}
{'loss': 3.5161, 'grad_norm': 65.86750793457031, 'learning_rate': 0.00018400000000000003, 'epoch': 4.6}
{'loss': 0.0145, 'grad_norm': 2.44447922706604, 'learning_rate': 0.000188, 'epoch': 4.7}
{'loss': 2.5975, 'grad_norm': 109.40592956542969, 'learning_rate': 0.000192, 'epoch': 4.8}
{'loss': 3.1624, 'grad_norm': 118.04811096191406, 'learning_rate': 0.000196, 'epoch': 4.9}
{'loss': 3.3047, 'grad_norm': 78.00892639160156, 'learning_rate': 0.0002, 'epoch': 5.0}
{'loss': 0.1514, 'grad_norm': 25.84402847290039, 'learning_rate': 0.00020400000000000003, 'epoch': 5.1}
{'loss': 0.982, 'grad_norm': 30.86534881591797, 'learning_rate': 0.00020800000000000001, 'epoch': 5.2}
{'loss': 8.7065, 'grad_norm': 77.88318634033203, 'learning_rate': 0.00021200000000000003, 'epoch': 5.3}
{'loss': 0.6532, 'grad_norm': 218.1689453125, 'learning_rate': 0.00021600000000000002, 'epoch': 5.4}
{'loss': 0.0004, 'grad_norm': 0.10326186567544937, 'learning_rate': 0.00022000000000000003, 'epoch': 5.5}
{'loss': 2.7579, 'grad_norm': 37.977874755859375, 'learning_rate': 0.00022400000000000002, 'epoch': 5.6}
{'loss': 3.252, 'grad_norm': 46.309837341308594, 'learning_rate': 0.00022799999999999999, 'epoch': 5.7}
{'loss': 2.7184, 'grad_norm': 35.798030853271484, 'learning_rate': 0.000232, 'epoch': 5.8}
{'loss': 0.0329, 'grad_norm': 15.055639266967773, 'learning_rate': 0.000236, 'epoch': 5.9}
{'loss': 3.8989, 'grad_norm': 77.72106170654297, 'learning_rate': 0.00024, 'epoch': 6.0}
{'loss': 0.0436, 'grad_norm': 11.21280288696289, 'learning_rate': 0.000244, 'epoch': 6.1}
{'loss': 5.8743, 'grad_norm': 24.88459014892578, 'learning_rate': 0.000248, 'epoch': 6.2}
{'loss': 0.2349, 'grad_norm': 26.603342056274414, 'learning_rate': 0.000252, 'epoch': 6.3}
{'loss': 2.4833, 'grad_norm': 40.22228240966797, 'learning_rate': 0.00025600000000000004, 'epoch': 6.4}
{'loss': 0.1826, 'grad_norm': 36.20548629760742, 'learning_rate': 0.00026000000000000003, 'epoch': 6.5}
{'loss': 1.4275, 'grad_norm': 75.47062683105469, 'learning_rate': 0.000264, 'epoch': 6.6}
{'loss': 2.0498, 'grad_norm': 26.906404495239258, 'learning_rate': 0.000268, 'epoch': 6.7}
{'loss': 6.819, 'grad_norm': 83.58678436279297, 'learning_rate': 0.00027200000000000005, 'epoch': 6.8}
{'loss': 2.394, 'grad_norm': 16.14794921875, 'learning_rate': 0.000276, 'epoch': 6.9}
{'loss': 2.8876, 'grad_norm': 108.06805419921875, 'learning_rate': 0.00028, 'epoch': 7.0}
{'loss': 2.3489, 'grad_norm': 8.428083419799805, 'learning_rate': 0.000284, 'epoch': 7.1}
{'loss': 0.0048, 'grad_norm': 0.5461294651031494, 'learning_rate': 0.000288, 'epoch': 7.2}
{'loss': 0.799, 'grad_norm': 68.5826416015625, 'learning_rate': 0.000292, 'epoch': 7.3}
{'loss': 0.1986, 'grad_norm': 79.24970245361328, 'learning_rate': 0.000296, 'epoch': 7.4}
{'loss': 3.9544, 'grad_norm': 107.30901336669922, 'learning_rate': 0.00030000000000000003, 'epoch': 7.5}
{'loss': 2.2961, 'grad_norm': 0.3356856107711792, 'learning_rate': 0.000304, 'epoch': 7.6}
{'loss': 2.0046, 'grad_norm': 346.1640625, 'learning_rate': 0.000308, 'epoch': 7.7}
{'loss': 6.4642, 'grad_norm': 66.30113220214844, 'learning_rate': 0.00031200000000000005, 'epoch': 7.8}
{'loss': 3.8647, 'grad_norm': 107.42710876464844, 'learning_rate': 0.00031600000000000004, 'epoch': 7.9}
{'loss': 3.0399, 'grad_norm': 82.10620880126953, 'learning_rate': 0.00032, 'epoch': 8.0}
{'loss': 3.0844, 'grad_norm': 34.483882904052734, 'learning_rate': 0.000324, 'epoch': 8.1}
{'loss': 7.3134, 'grad_norm': 54.022705078125, 'learning_rate': 0.000328, 'epoch': 8.2}
{'loss': 0.0031, 'grad_norm': 0.4085950553417206, 'learning_rate': 0.000332, 'epoch': 8.3}
{'loss': 2.7073, 'grad_norm': 50.4301872253418, 'learning_rate': 0.000336, 'epoch': 8.4}
{'loss': 0.6458, 'grad_norm': 49.12760543823242, 'learning_rate': 0.00034, 'epoch': 8.5}
{'loss': 0.1697, 'grad_norm': 31.162818908691406, 'learning_rate': 0.000344, 'epoch': 8.6}
 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 87/100 [01:07<00:04,  2.84it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 88/100 [01:08<00:04,  2.85it/s]                                                 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 88/100 [01:08<00:04,  2.85it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [01:08<00:03,  2.84it/s]                                                 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [01:08<00:03,  2.84it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 90/100 [01:08<00:03,  2.85it/s]                                                 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 90/100 [01:08<00:03,  2.85it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 91/100 [01:09<00:03,  2.84it/s]                                                 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 91/100 [01:09<00:03,  2.84it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 92/100 [01:09<00:02,  2.84it/s]                                                 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 92/100 [01:09<00:02,  2.84it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 93/100 [01:09<00:02,  2.85it/s]                                                 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 93/100 [01:09<00:02,  2.85it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 94/100 [01:10<00:02,  2.84it/s]                                                 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 94/100 [01:10<00:02,  2.84it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 95/100 [01:10<00:01,  2.85it/s]                                                 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 95/100 [01:10<00:01,  2.85it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 96/100 [01:10<00:01,  2.84it/s]                                                 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 96/100 [01:10<00:01,  2.84it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [01:11<00:01,  2.84it/s]                                                 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [01:11<00:01,  2.84it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 98/100 [01:11<00:00,  2.83it/s]                                                 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 98/100 [01:11<00:00,  2.83it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 99/100 [01:11<00:00,  2.84it/s]                                                 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 99/100 [01:11<00:00,  2.84it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [01:12<00:00,  2.85it/s]                                                 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [01:12<00:00,  2.85it/s]                                                 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [01:18<00:00,  2.85it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [01:18<00:00,  1.27it/s]
{'loss': 0.0098, 'grad_norm': 3.748131036758423, 'learning_rate': 0.000348, 'epoch': 8.7}
{'loss': 0.0019, 'grad_norm': 0.25752097368240356, 'learning_rate': 0.00035200000000000005, 'epoch': 8.8}
{'loss': 2.303, 'grad_norm': 2.7020435333251953, 'learning_rate': 0.00035600000000000003, 'epoch': 8.9}
{'loss': 2.285, 'grad_norm': 0.2611096501350403, 'learning_rate': 0.00036, 'epoch': 9.0}
{'loss': 8.5826, 'grad_norm': 405.2245178222656, 'learning_rate': 0.000364, 'epoch': 9.1}
{'loss': 14.5396, 'grad_norm': 119.3582763671875, 'learning_rate': 0.00036800000000000005, 'epoch': 9.2}
{'loss': 2.2818, 'grad_norm': 0.049355022609233856, 'learning_rate': 0.00037200000000000004, 'epoch': 9.3}
{'loss': 1.2863, 'grad_norm': 80.54345703125, 'learning_rate': 0.000376, 'epoch': 9.4}
{'loss': 5.1537, 'grad_norm': 127.40103149414062, 'learning_rate': 0.00038, 'epoch': 9.5}
{'loss': 3.6321, 'grad_norm': 75.96002960205078, 'learning_rate': 0.000384, 'epoch': 9.6}
{'loss': 3.7274, 'grad_norm': 42.45431137084961, 'learning_rate': 0.000388, 'epoch': 9.7}
{'loss': 6.5485, 'grad_norm': 198.45140075683594, 'learning_rate': 0.000392, 'epoch': 9.8}
{'loss': 6.6599, 'grad_norm': 244.9681396484375, 'learning_rate': 0.00039600000000000003, 'epoch': 9.9}
{'loss': 0.4161, 'grad_norm': 62.22504806518555, 'learning_rate': 0.0004, 'epoch': 10.0}
{'train_runtime': 88.892, 'train_samples_per_second': 1.125, 'train_steps_per_second': 1.125, 'train_loss': 4.7977449249644994, 'epoch': 10.0}
[1;34mwandb[0m: üöÄ View run [33mmistral-7b-instruct-v0.3_ZsRE_1[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/i0g8oe67[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241104_221325-i0g8oe67/logs[0m
mv: cannot stat './tmp_data.jsonl': No such file or directory
mv: cannot stat './wandb': No such file or directory
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:05<00:10,  5.21s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:09<00:04,  4.79s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.45s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.59s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_1/part_0/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_1/part_0/eval
0  to  50
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_1/part_0/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
550  to  600
generating!
600  to  650
generating!
650  to  700
generating!
700  to  750
generating!
750  to  800
generating!
800  to  850
generating!
850  to  900
generating!
900  to  950
generating!
950  to  1000
generating!
1000  to  1050
generating!
1050  to  1100
generating!
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_1/part_0/tokenizer.model
Skipping file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_1/part_0/log.txt
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_1/part_0/tokenizer_config.json
Skipping directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_1/part_0/eval
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_1/part_0/checkpoint-100
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_1/part_0/adapter_config.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_1/part_0/special_tokens_map.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_1/part_0/adapter_model.safetensors
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_1/part_0/README.md
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=0, data_source=ZsRE, data_size=5, random=False
Loading data from ../../data/edit_data/merged_data_part_0.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 5 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_5/part_0/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/mistral-7b-instruct-v0.3
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_5/part_0
batch_size: 5
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: mistral-7b-instruct-v0.3_ZsRE_5
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:04<00:08,  4.26s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:07<00:03,  3.54s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:10<00:00,  3.18s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:10<00:00,  3.35s/it]
Using custom data configuration default-d4dd42806a7169a9
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-d4dd42806a7169a9/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2571.61it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 319.86it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-d4dd42806a7169a9/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 277.20it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f69806ddee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 28,311,552 || all params: 7,276,335,104 || trainable%: 0.3890908210705739
0ex [00:00, ?ex/s]5ex [00:00, 52.11ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241104_222136-ez3bmjl7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mistral-7b-instruct-v0.3_ZsRE_5
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/ez3bmjl7

Example:
<s> What was the name of Charlotte of Schaumburg-Lippe mother? Charlotte of Bourbon-Parma</s>


Example:
<s> What voice type is Louise Grandjean? mezzo soprano</s>


Example:
<s> Who is listed as Wang Jipeng father? Wang Chonghua</s>


Example:
<s> What species is ZIC3 specific to? male</s>


Example:
<s> Which family does Epaspidoceras belong to? Noctuidae</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 5
})
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:02<00:23,  2.64s/it]                                               10%|‚ñà         | 1/10 [00:02<00:23,  2.64s/it] 20%|‚ñà‚ñà        | 2/10 [00:04<00:16,  2.12s/it]                                               20%|‚ñà‚ñà        | 2/10 [00:04<00:16,  2.12s/it] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:06<00:13,  1.95s/it]                                               30%|‚ñà‚ñà‚ñà       | 3/10 [00:06<00:13,  1.95s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:07<00:11,  1.86s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:07<00:11,  1.86s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:09<00:09,  1.82s/it]                                               50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:09<00:09,  1.82s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:11<00:07,  1.79s/it]                                               60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:11<00:07,  1.79s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [00:13<00:05,  1.77s/it]                                               70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [00:13<00:05,  1.77s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [00:14<00:03,  1.76s/it]                                               80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [00:14<00:03,  1.76s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [00:16<00:01,  1.75s/it]                                               90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [00:16<00:01,  1.75s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:18<00:00,  1.75s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:18<00:00,  1.75s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:20<00:00,  1.75s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:20<00:00,  2.07s/it]
{'loss': 5.9587, 'grad_norm': 28.048429489135742, 'learning_rate': 4.000000000000001e-06, 'epoch': 1.0}
{'loss': 5.9587, 'grad_norm': 26.032466888427734, 'learning_rate': 8.000000000000001e-06, 'epoch': 2.0}
{'loss': 5.741, 'grad_norm': 26.774871826171875, 'learning_rate': 1.2e-05, 'epoch': 3.0}
{'loss': 5.3227, 'grad_norm': 26.693191528320312, 'learning_rate': 1.6000000000000003e-05, 'epoch': 4.0}
{'loss': 4.7408, 'grad_norm': 28.539705276489258, 'learning_rate': 2e-05, 'epoch': 5.0}
{'loss': 3.9708, 'grad_norm': 30.260936737060547, 'learning_rate': 2.4e-05, 'epoch': 6.0}
{'loss': 3.1292, 'grad_norm': 26.787853240966797, 'learning_rate': 2.8000000000000003e-05, 'epoch': 7.0}
{'loss': 2.3247, 'grad_norm': 27.143814086914062, 'learning_rate': 3.2000000000000005e-05, 'epoch': 8.0}
{'loss': 1.4777, 'grad_norm': 22.713590621948242, 'learning_rate': 3.6e-05, 'epoch': 9.0}
{'loss': 0.8742, 'grad_norm': 8.768814086914062, 'learning_rate': 4e-05, 'epoch': 10.0}
{'train_runtime': 23.2146, 'train_samples_per_second': 2.154, 'train_steps_per_second': 0.431, 'train_loss': 3.949850523471832, 'epoch': 10.0}
[1;34mwandb[0m: üöÄ View run [33mmistral-7b-instruct-v0.3_ZsRE_5[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/ez3bmjl7[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241104_222136-ez3bmjl7/logs[0m
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:04<00:09,  5.00s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:09<00:04,  4.61s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.30s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.42s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_5/part_0/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_5/part_0/eval
0  to  50
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_5/part_0/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
550  to  600
generating!
600  to  650
generating!
650  to  700
generating!
700  to  750
generating!
750  to  800
generating!
800  to  850
generating!
850  to  900
generating!
900  to  950
generating!
950  to  1000
generating!
1000  to  1050
generating!
1050  to  1100
generating!
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_5/part_0/special_tokens_map.json
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_5/part_0/wandb
Skipping directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_5/part_0/eval
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_5/part_0/tokenizer_config.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_5/part_0/tmp_data.jsonl
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_5/part_0/README.md
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_5/part_0/adapter_model.safetensors
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_5/part_0/adapter_config.json
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_5/part_0/checkpoint-10
Skipping file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_5/part_0/log.txt
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_5/part_0/tokenizer.model
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=0, data_source=ZsRE, data_size=10, random=False
Loading data from ../../data/edit_data/merged_data_part_0.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 10 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_10/part_0/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/mistral-7b-instruct-v0.3
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_10/part_0
batch_size: 10
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: mistral-7b-instruct-v0.3_ZsRE_10
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:04<00:08,  4.26s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:07<00:03,  3.52s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:09<00:00,  3.16s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:09<00:00,  3.33s/it]
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Traceback (most recent call last):
  File "/home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/train_lora_wo_tmp.py", line 496, in <module>
    fire.Fire(train)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/fire/core.py", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/fire/core.py", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "/home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/train_lora_wo_tmp.py", line 373, in train
    data = load_dataset("json", data_files=data_path)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/load.py", line 1675, in load_dataset
    builder_instance = load_dataset_builder(
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/load.py", line 1512, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/load.py", line 1117, in dataset_module_factory
    return PackagedDatasetModuleFactory(
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/load.py", line 774, in get_module
    data_files = DataFilesDict.from_local_or_remote(patterns, use_auth_token=self.downnload_config.use_auth_token)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/data_files.py", line 578, in from_local_or_remote
    DataFilesList.from_local_or_remote(
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/data_files.py", line 546, in from_local_or_remote
    data_files = resolve_patterns_locally_or_by_urls(base_path, patterns, allowed_extensions)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/data_files.py", line 196, in resolve_patterns_locally_or_by_urls
    for path in _resolve_single_pattern_locally(base_path, pattern, allowed_extensions):
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/data_files.py", line 146, in _resolve_single_pattern_locally
    raise FileNotFoundError(error_msg)
FileNotFoundError: Unable to find '/home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/./tmp_data.jsonl' at /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora
mv: cannot stat './tmp_data.jsonl': No such file or directory
mv: cannot stat './wandb': No such file or directory
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:04<00:08,  4.45s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:08<00:04,  4.38s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:12<00:00,  4.18s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:12<00:00,  4.24s/it]
We are now merging the lora adapter to base model
Traceback (most recent call last):
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/peft/config.py", line 184, in _get_peft_type
    config_file = hf_hub_download(
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py", line 101, in inner_f
    return f(*args, **kwargs)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_10/part_0'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/lora_eval.py", line 36, in <module>
    lora_model = PeftModel.from_pretrained(model, args.lora_path)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/peft/peft_model.py", line 324, in from_pretrained
    PeftConfig._get_peft_type(
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/peft/config.py", line 190, in _get_peft_type
    raise ValueError(f"Can't find '{CONFIG_NAME}' at '{model_id}'")
ValueError: Can't find 'adapter_config.json' at '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_10/part_0'
Skipping file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_10/part_0/log.txt
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=0, data_source=ZsRE, data_size=15, random=False
Loading data from ../../data/edit_data/merged_data_part_0.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 15 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_15/part_0/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/mistral-7b-instruct-v0.3
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_15/part_0
batch_size: 15
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: mistral-7b-instruct-v0.3_ZsRE_15
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:03<00:06,  3.11s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:06<00:03,  3.06s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:08<00:00,  2.90s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:08<00:00,  2.95s/it]
Using custom data configuration default-cdfc02211933ec58
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-cdfc02211933ec58/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2454.24it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 288.63it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-cdfc02211933ec58/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 286.55it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f0341270ee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 28,311,552 || all params: 7,276,335,104 || trainable%: 0.3890908210705739
0ex [00:00, ?ex/s]12ex [00:00, 119.27ex/s]15ex [00:00, 143.37ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241104_222903-zx837a06
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mistral-7b-instruct-v0.3_ZsRE_15
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/zx837a06

Example:
<s> The father of Juan Mar√≠a Bordaberry is whom? Gabrielle Bordaberry</s>


Example:
<s> What level is Javan surili's iucn conservation status? critically threatened</s>


Example:
<s> What was the record label of Runaway Sunday? Motown</s>


Example:
<s> Who is listed as Wang Jipeng father? Wang Chonghua</s>


Example:
<s> Which family does Epaspidoceras belong to? Noctuidae</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 15
})
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:06<00:54,  6.03s/it]                                               10%|‚ñà         | 1/10 [00:06<00:54,  6.03s/it] 20%|‚ñà‚ñà        | 2/10 [00:11<00:44,  5.50s/it]                                               20%|‚ñà‚ñà        | 2/10 [00:11<00:44,  5.50s/it] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:16<00:37,  5.34s/it]                                               30%|‚ñà‚ñà‚ñà       | 3/10 [00:16<00:37,  5.34s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:21<00:31,  5.26s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:21<00:31,  5.26s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:26<00:26,  5.22s/it]                                               50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:26<00:26,  5.22s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:31<00:20,  5.20s/it]                                               60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:31<00:20,  5.20s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [00:36<00:15,  5.19s/it]                                               70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [00:36<00:15,  5.19s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [00:42<00:10,  5.18s/it]                                               80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [00:42<00:10,  5.18s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [00:47<00:05,  5.17s/it]                                               90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [00:47<00:05,  5.17s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:52<00:00,  5.17s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:52<00:00,  5.17s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:55<00:00,  5.17s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:55<00:00,  5.53s/it]
{'loss': 5.6017, 'grad_norm': 22.120437622070312, 'learning_rate': 4.000000000000001e-06, 'epoch': 1.0}
{'loss': 5.6017, 'grad_norm': 23.455495834350586, 'learning_rate': 8.000000000000001e-06, 'epoch': 2.0}
{'loss': 5.4369, 'grad_norm': 23.433168411254883, 'learning_rate': 1.2e-05, 'epoch': 3.0}
{'loss': 5.1111, 'grad_norm': 23.200162887573242, 'learning_rate': 1.6000000000000003e-05, 'epoch': 4.0}
{'loss': 4.6265, 'grad_norm': 23.121931076049805, 'learning_rate': 2e-05, 'epoch': 5.0}
{'loss': 4.0153, 'grad_norm': 22.002779006958008, 'learning_rate': 2.4e-05, 'epoch': 6.0}
{'loss': 3.3142, 'grad_norm': 19.895715713500977, 'learning_rate': 2.8000000000000003e-05, 'epoch': 7.0}
{'loss': 2.6298, 'grad_norm': 15.915830612182617, 'learning_rate': 3.2000000000000005e-05, 'epoch': 8.0}
{'loss': 1.9922, 'grad_norm': 13.504255294799805, 'learning_rate': 3.6e-05, 'epoch': 9.0}
{'loss': 1.4797, 'grad_norm': 10.96088981628418, 'learning_rate': 4e-05, 'epoch': 10.0}
{'train_runtime': 58.062, 'train_samples_per_second': 2.583, 'train_steps_per_second': 0.172, 'train_loss': 3.9809035420417787, 'epoch': 10.0}
[1;34mwandb[0m: üöÄ View run [33mmistral-7b-instruct-v0.3_ZsRE_15[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/zx837a06[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241104_222903-zx837a06/logs[0m
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:04<00:09,  4.94s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:09<00:04,  4.59s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.27s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.39s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_15/part_0/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_15/part_0/eval
0  to  50
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_15/part_0/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
550  to  600
generating!
600  to  650
generating!
650  to  700
generating!
700  to  750
generating!
750  to  800
generating!
800  to  850
generating!
850  to  900
generating!
900  to  950
generating!
950  to  1000
generating!
1000  to  1050
generating!
1050  to  1100
generating!
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_15/part_0/tmp_data.jsonl
Skipping directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_15/part_0/eval
Skipping file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_15/part_0/log.txt
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_15/part_0/special_tokens_map.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_15/part_0/README.md
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_15/part_0/tokenizer.model
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_15/part_0/adapter_model.safetensors
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_15/part_0/wandb
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_15/part_0/adapter_config.json
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_15/part_0/checkpoint-10
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_15/part_0/tokenizer_config.json
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=0, data_source=ZsRE, data_size=20, random=False
Loading data from ../../data/edit_data/merged_data_part_0.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 20 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_20/part_0/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/mistral-7b-instruct-v0.3
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_20/part_0
batch_size: 20
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: mistral-7b-instruct-v0.3_ZsRE_20
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:04<00:08,  4.09s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:07<00:03,  3.47s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:09<00:00,  3.13s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:09<00:00,  3.28s/it]
Using custom data configuration default-557048f929e6cf2e
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-557048f929e6cf2e/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2461.45it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 302.60it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-557048f929e6cf2e/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 279.71it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f8ea174eee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 28,311,552 || all params: 7,276,335,104 || trainable%: 0.3890908210705739
0ex [00:00, ?ex/s]1ex [00:00,  7.29ex/s]20ex [00:00, 124.28ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241104_223628-pv6n99p1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mistral-7b-instruct-v0.3_ZsRE_20
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/pv6n99p1

Example:
<s> What team is Nicolas Raffault associated with? Arizona Coyotes</s>


Example:
<s> What was the record label of Runaway Sunday? Motown</s>


Example:
<s> Who was Marc Moulin's mother? Catherine Moulin</s>


Example:
<s> What level is Javan surili's iucn conservation status? critically threatened</s>


Example:
<s> Due to which disease did Joseph Papp die? pneumonia</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 20
})
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:07<01:09,  7.73s/it]                                               10%|‚ñà         | 1/10 [00:07<01:09,  7.73s/it] 20%|‚ñà‚ñà        | 2/10 [00:14<00:57,  7.22s/it]                                               20%|‚ñà‚ñà        | 2/10 [00:14<00:57,  7.22s/it] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:21<00:49,  7.07s/it]                                               30%|‚ñà‚ñà‚ñà       | 3/10 [00:21<00:49,  7.07s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:28<00:41,  7.00s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:28<00:41,  7.00s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:35<00:34,  6.96s/it]                                               50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:35<00:34,  6.96s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:42<00:27,  6.94s/it]                                               60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:42<00:27,  6.94s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [00:49<00:20,  6.92s/it]                                               70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [00:49<00:20,  6.92s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [00:55<00:13,  6.91s/it]                                               80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [00:55<00:13,  6.91s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:02<00:06,  6.91s/it]                                               90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:02<00:06,  6.91s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:09<00:00,  6.91s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:09<00:00,  6.91s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:12<00:00,  6.91s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:12<00:00,  7.29s/it]
{'loss': 5.6571, 'grad_norm': 19.722015380859375, 'learning_rate': 4.000000000000001e-06, 'epoch': 1.0}
{'loss': 5.6571, 'grad_norm': 20.26453399658203, 'learning_rate': 8.000000000000001e-06, 'epoch': 2.0}
{'loss': 5.5153, 'grad_norm': 19.447769165039062, 'learning_rate': 1.2e-05, 'epoch': 3.0}
{'loss': 5.2325, 'grad_norm': 20.101842880249023, 'learning_rate': 1.6000000000000003e-05, 'epoch': 4.0}
{'loss': 4.8176, 'grad_norm': 19.9350643157959, 'learning_rate': 2e-05, 'epoch': 5.0}
{'loss': 4.2963, 'grad_norm': 18.07286262512207, 'learning_rate': 2.4e-05, 'epoch': 6.0}
{'loss': 3.6781, 'grad_norm': 17.09366226196289, 'learning_rate': 2.8000000000000003e-05, 'epoch': 7.0}
{'loss': 3.0111, 'grad_norm': 15.504772186279297, 'learning_rate': 3.2000000000000005e-05, 'epoch': 8.0}
{'loss': 2.4002, 'grad_norm': 12.803775787353516, 'learning_rate': 3.6e-05, 'epoch': 9.0}
{'loss': 1.8946, 'grad_norm': 10.298995971679688, 'learning_rate': 4e-05, 'epoch': 10.0}
{'train_runtime': 75.4922, 'train_samples_per_second': 2.649, 'train_steps_per_second': 0.132, 'train_loss': 4.21598652601242, 'epoch': 10.0}
[1;34mwandb[0m: üöÄ View run [33mmistral-7b-instruct-v0.3_ZsRE_20[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/pv6n99p1[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241104_223628-pv6n99p1/logs[0m
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:05<00:10,  5.01s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:09<00:04,  4.62s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.29s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.42s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_20/part_0/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_20/part_0/eval
0  to  50
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_20/part_0/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
550  to  600
generating!
600  to  650
generating!
650  to  700
generating!
700  to  750
generating!
750  to  800
generating!
800  to  850
generating!
850  to  900
generating!
900  to  950
generating!
950  to  1000
generating!
1000  to  1050
generating!
1050  to  1100
generating!
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_20/part_0/tokenizer_config.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_20/part_0/README.md
Skipping file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_20/part_0/log.txt
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_20/part_0/tokenizer.model
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_20/part_0/wandb
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_20/part_0/tmp_data.jsonl
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_20/part_0/checkpoint-10
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_20/part_0/adapter_model.safetensors
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_20/part_0/special_tokens_map.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_20/part_0/adapter_config.json
Skipping directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_20/part_0/eval
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=0, data_source=ZsRE, data_size=25, random=False
Loading data from ../../data/edit_data/merged_data_part_0.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 25 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_25/part_0/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/mistral-7b-instruct-v0.3
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_25/part_0
batch_size: 25
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: mistral-7b-instruct-v0.3_ZsRE_25
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:03<00:06,  3.11s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:06<00:03,  3.05s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:08<00:00,  2.90s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:08<00:00,  2.95s/it]
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Traceback (most recent call last):
  File "/home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/train_lora_wo_tmp.py", line 496, in <module>
    fire.Fire(train)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/fire/core.py", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/fire/core.py", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "/home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/train_lora_wo_tmp.py", line 373, in train
    data = load_dataset("json", data_files=data_path)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/load.py", line 1675, in load_dataset
    builder_instance = load_dataset_builder(
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/load.py", line 1512, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/load.py", line 1117, in dataset_module_factory
    return PackagedDatasetModuleFactory(
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/load.py", line 774, in get_module
    data_files = DataFilesDict.from_local_or_remote(patterns, use_auth_token=self.downnload_config.use_auth_token)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/data_files.py", line 578, in from_local_or_remote
    DataFilesList.from_local_or_remote(
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/data_files.py", line 546, in from_local_or_remote
    data_files = resolve_patterns_locally_or_by_urls(base_path, patterns, allowed_extensions)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/data_files.py", line 196, in resolve_patterns_locally_or_by_urls
    for path in _resolve_single_pattern_locally(base_path, pattern, allowed_extensions):
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/data_files.py", line 146, in _resolve_single_pattern_locally
    raise FileNotFoundError(error_msg)
FileNotFoundError: Unable to find '/home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/./tmp_data.jsonl' at /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora
mv: cannot stat './tmp_data.jsonl': No such file or directory
mv: cannot stat './wandb': No such file or directory
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:04<00:08,  4.47s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:08<00:04,  4.41s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:12<00:00,  4.18s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:12<00:00,  4.25s/it]
We are now merging the lora adapter to base model
Traceback (most recent call last):
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/peft/config.py", line 184, in _get_peft_type
    config_file = hf_hub_download(
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py", line 101, in inner_f
    return f(*args, **kwargs)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_25/part_0'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/lora_eval.py", line 36, in <module>
    lora_model = PeftModel.from_pretrained(model, args.lora_path)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/peft/peft_model.py", line 324, in from_pretrained
    PeftConfig._get_peft_type(
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/peft/config.py", line 190, in _get_peft_type
    raise ValueError(f"Can't find '{CONFIG_NAME}' at '{model_id}'")
ValueError: Can't find 'adapter_config.json' at '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_25/part_0'
Skipping file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_25/part_0/log.txt
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=0, data_source=ZsRE, data_size=30, random=False
Loading data from ../../data/edit_data/merged_data_part_0.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 30 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_30/part_0/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/mistral-7b-instruct-v0.3
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_30/part_0
batch_size: 30
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: mistral-7b-instruct-v0.3_ZsRE_30
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:03<00:06,  3.11s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:06<00:03,  3.05s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:08<00:00,  2.90s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:08<00:00,  2.95s/it]
Using custom data configuration default-bf1950dc431dcec7
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-bf1950dc431dcec7/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2532.79it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 262.77it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-bf1950dc431dcec7/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 243.13it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f3390101ee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 28,311,552 || all params: 7,276,335,104 || trainable%: 0.3890908210705739
0ex [00:00, ?ex/s]7ex [00:00, 69.71ex/s]30ex [00:00, 234.79ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241104_224435-q6ea48lt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mistral-7b-instruct-v0.3_ZsRE_30
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/q6ea48lt

Example:
<s> What war or battle involved Alec Rose? Spanish Civil War</s>


Example:
<s> What river does Charity Creek connect to?  Charity River</s>


Example:
<s> What level is Javan surili's iucn conservation status? critically threatened</s>


Example:
<s> The mother of Mallory Reaves is whom? Lalli Reaves</s>


Example:
<s> Who made Alexanderson alternator known? Ernest Alexanderson</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 30
})
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:11<01:40, 11.18s/it]                                               10%|‚ñà         | 1/10 [00:11<01:40, 11.18s/it] 20%|‚ñà‚ñà        | 2/10 [00:21<01:25, 10.64s/it]                                               20%|‚ñà‚ñà        | 2/10 [00:21<01:25, 10.64s/it] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:31<01:13, 10.49s/it]                                               30%|‚ñà‚ñà‚ñà       | 3/10 [00:31<01:13, 10.49s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:42<01:02, 10.41s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:42<01:02, 10.41s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:52<00:51, 10.38s/it]                                               50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:52<00:51, 10.38s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:02<00:41, 10.36s/it]                                               60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:02<00:41, 10.36s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:12<00:31, 10.34s/it]                                               70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:12<00:31, 10.34s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:23<00:20, 10.33s/it]                                               80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:23<00:20, 10.33s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:33<00:10, 10.33s/it]                                               90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:33<00:10, 10.33s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:43<00:00, 10.32s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:43<00:00, 10.32s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:46<00:00, 10.32s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:46<00:00, 10.65s/it]
{'loss': 5.6981, 'grad_norm': 17.931568145751953, 'learning_rate': 4.000000000000001e-06, 'epoch': 1.0}
{'loss': 5.6981, 'grad_norm': 18.39032554626465, 'learning_rate': 8.000000000000001e-06, 'epoch': 2.0}
{'loss': 5.5703, 'grad_norm': 18.77739143371582, 'learning_rate': 1.2e-05, 'epoch': 3.0}
{'loss': 5.3162, 'grad_norm': 18.450895309448242, 'learning_rate': 1.6000000000000003e-05, 'epoch': 4.0}
{'loss': 4.938, 'grad_norm': 18.026865005493164, 'learning_rate': 2e-05, 'epoch': 5.0}
{'loss': 4.4321, 'grad_norm': 18.5952091217041, 'learning_rate': 2.4e-05, 'epoch': 6.0}
{'loss': 3.8448, 'grad_norm': 16.91008758544922, 'learning_rate': 2.8000000000000003e-05, 'epoch': 7.0}
{'loss': 3.1807, 'grad_norm': 16.01527976989746, 'learning_rate': 3.2000000000000005e-05, 'epoch': 8.0}
{'loss': 2.5458, 'grad_norm': 13.310386657714844, 'learning_rate': 3.6e-05, 'epoch': 9.0}
{'loss': 2.0182, 'grad_norm': 9.85841178894043, 'learning_rate': 4e-05, 'epoch': 10.0}
{'train_runtime': 108.7022, 'train_samples_per_second': 2.76, 'train_steps_per_second': 0.092, 'train_loss': 4.324226212501526, 'epoch': 10.0}
[1;34mwandb[0m: üöÄ View run [33mmistral-7b-instruct-v0.3_ZsRE_30[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/q6ea48lt[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241104_224435-q6ea48lt/logs[0m
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:04<00:09,  4.96s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:09<00:04,  4.61s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.28s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.41s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_30/part_0/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_30/part_0/eval
0  to  50
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_30/part_0/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
550  to  600
generating!
600  to  650
generating!
650  to  700
generating!
700  to  750
generating!
750  to  800
generating!
800  to  850
generating!
850  to  900
generating!
900  to  950
generating!
950  to  1000
generating!
1000  to  1050
generating!
1050  to  1100
generating!
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_30/part_0/special_tokens_map.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_30/part_0/README.md
Skipping file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_30/part_0/log.txt
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_30/part_0/adapter_model.safetensors
Skipping directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_30/part_0/eval
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_30/part_0/tmp_data.jsonl
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_30/part_0/adapter_config.json
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_30/part_0/checkpoint-10
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_30/part_0/tokenizer.model
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_30/part_0/tokenizer_config.json
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_30/part_0/wandb
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=0, data_source=ZsRE, data_size=35, random=False
Loading data from ../../data/edit_data/merged_data_part_0.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 35 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_35/part_0/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/mistral-7b-instruct-v0.3
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_35/part_0
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: mistral-7b-instruct-v0.3_ZsRE_35
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:03<00:06,  3.11s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:06<00:03,  3.05s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:08<00:00,  2.91s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:08<00:00,  2.95s/it]
Using custom data configuration default-aa599df05f64e283
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-aa599df05f64e283/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2568.47it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 248.14it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-aa599df05f64e283/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 273.23it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f0aa0080ee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 28,311,552 || all params: 7,276,335,104 || trainable%: 0.3890908210705739
0ex [00:00, ?ex/s]35ex [00:00, 379.43ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241104_225243-c71ljmi0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mistral-7b-instruct-v0.3_ZsRE_35
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/c71ljmi0

Example:
<s> The mother of Mallory Reaves is whom? Lalli Reaves</s>


Example:
<s> What river does Charity Creek connect to?  Charity River</s>


Example:
<s> Who made Alexanderson alternator known? Ernest Alexanderson</s>


Example:
<s> What constellation is home to Butterfly Cluster? Orion</s>


Example:
<s> What network first aired The Smothers Brothers Comedy Hour? NBC</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 35
})
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:11<01:43, 11.52s/it]                                               10%|‚ñà         | 1/10 [00:11<01:43, 11.52s/it] 20%|‚ñà‚ñà        | 2/10 [00:22<01:29, 11.20s/it]                                               20%|‚ñà‚ñà        | 2/10 [00:22<01:29, 11.20s/it] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:33<01:17, 11.12s/it]                                               30%|‚ñà‚ñà‚ñà       | 3/10 [00:33<01:17, 11.12s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:44<01:06, 11.09s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:44<01:06, 11.09s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:55<00:55, 11.07s/it]                                               50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:55<00:55, 11.07s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:06<00:44, 11.05s/it]                                               60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:06<00:44, 11.05s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:17<00:33, 11.05s/it]                                               70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:17<00:33, 11.05s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:28<00:22, 11.04s/it]                                               80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:28<00:22, 11.04s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:39<00:11, 11.04s/it]                                               90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:39<00:11, 11.04s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:50<00:00, 11.03s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:50<00:00, 11.03s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:53<00:00, 11.03s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:53<00:00, 11.33s/it]
{'loss': 5.9164, 'grad_norm': 19.838342666625977, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.91}
{'loss': 6.009, 'grad_norm': 19.95717430114746, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.83}
{'loss': 5.749, 'grad_norm': 20.751785278320312, 'learning_rate': 1.2e-05, 'epoch': 2.74}
{'loss': 5.3904, 'grad_norm': 18.864835739135742, 'learning_rate': 1.6000000000000003e-05, 'epoch': 3.66}
{'loss': 4.9065, 'grad_norm': 20.94882583618164, 'learning_rate': 2e-05, 'epoch': 4.57}
{'loss': 4.5334, 'grad_norm': 22.799680709838867, 'learning_rate': 2.4e-05, 'epoch': 5.49}
{'loss': 4.4688, 'grad_norm': 18.126256942749023, 'learning_rate': 2.8000000000000003e-05, 'epoch': 6.4}
{'loss': 3.1443, 'grad_norm': 17.782440185546875, 'learning_rate': 3.2000000000000005e-05, 'epoch': 7.31}
{'loss': 2.9095, 'grad_norm': 15.256916046142578, 'learning_rate': 3.6e-05, 'epoch': 8.23}
{'loss': 2.2181, 'grad_norm': 11.551987648010254, 'learning_rate': 4e-05, 'epoch': 9.14}
{'train_runtime': 115.6651, 'train_samples_per_second': 3.026, 'train_steps_per_second': 0.086, 'train_loss': 4.524555611610412, 'epoch': 9.14}
[1;34mwandb[0m: üöÄ View run [33mmistral-7b-instruct-v0.3_ZsRE_35[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/c71ljmi0[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241104_225243-c71ljmi0/logs[0m
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:04<00:09,  4.65s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:09<00:04,  4.48s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:12<00:00,  4.24s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:12<00:00,  4.32s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_35/part_0/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_35/part_0/eval
0  to  50
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_35/part_0/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
550  to  600
generating!
600  to  650
generating!
650  to  700
generating!
700  to  750
generating!
750  to  800
generating!
800  to  850
generating!
850  to  900
generating!
900  to  950
generating!
950  to  1000
generating!
1000  to  1050
generating!
1050  to  1100
generating!
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_35/part_0/tokenizer_config.json
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_35/part_0/wandb
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_35/part_0/special_tokens_map.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_35/part_0/adapter_config.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_35/part_0/tokenizer.model
Skipping file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_35/part_0/log.txt
Skipping directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_35/part_0/eval
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_35/part_0/tmp_data.jsonl
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_35/part_0/checkpoint-10
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_35/part_0/adapter_model.safetensors
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_35/part_0/README.md
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=0, data_source=ZsRE, data_size=40, random=False
Loading data from ../../data/edit_data/merged_data_part_0.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 40 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_40/part_0/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/mistral-7b-instruct-v0.3
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_40/part_0
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: mistral-7b-instruct-v0.3_ZsRE_40
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:03<00:07,  3.87s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:06<00:03,  3.36s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:09<00:00,  3.07s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:09<00:00,  3.20s/it]
Using custom data configuration default-b6418015fb2b38b9
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-b6418015fb2b38b9/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2470.14it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 304.38it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-b6418015fb2b38b9/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 256.38it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7fec51418ee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 28,311,552 || all params: 7,276,335,104 || trainable%: 0.3890908210705739
0ex [00:00, ?ex/s]1ex [00:00,  7.61ex/s]40ex [00:00, 224.10ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241104_230103-9aur0jfc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mistral-7b-instruct-v0.3_ZsRE_40
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/9aur0jfc

Example:
<s> What sports team was Petteri Nummelin a member of? Columbus Blue Bombers</s>


Example:
<s> What level is Javan surili's iucn conservation status? critically threatened</s>


Example:
<s> Over which river does Dexter Coffin Bridge cross? Connecticut Creek</s>


Example:
<s> What is an ecological status of Bali myna?  myna</s>


Example:
<s> What was the name of Charlotte of Schaumburg-Lippe mother? Charlotte of Bourbon-Parma</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 40
})
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:11<01:46, 11.88s/it]                                               10%|‚ñà         | 1/10 [00:11<01:46, 11.88s/it] 20%|‚ñà‚ñà        | 2/10 [00:22<01:30, 11.37s/it]                                               20%|‚ñà‚ñà        | 2/10 [00:22<01:30, 11.37s/it] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:33<01:18, 11.20s/it]                                               30%|‚ñà‚ñà‚ñà       | 3/10 [00:33<01:18, 11.20s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:44<01:06, 11.13s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:44<01:06, 11.13s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:55<00:55, 11.09s/it]                                               50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:55<00:55, 11.09s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:06<00:44, 11.07s/it]                                               60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:06<00:44, 11.07s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:17<00:33, 11.05s/it]                                               70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:17<00:33, 11.05s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:29<00:22, 11.05s/it]                                               80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:29<00:22, 11.05s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:40<00:11, 11.04s/it]                                               90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:40<00:11, 11.04s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:51<00:00, 11.03s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:51<00:00, 11.03s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:53<00:00, 11.03s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:53<00:00, 11.37s/it]
{'loss': 5.6444, 'grad_norm': 18.95213508605957, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.8}
{'loss': 5.5479, 'grad_norm': 20.41998291015625, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.6}
{'loss': 6.0891, 'grad_norm': 21.776405334472656, 'learning_rate': 1.2e-05, 'epoch': 2.4}
{'loss': 5.1027, 'grad_norm': 18.4320068359375, 'learning_rate': 1.6000000000000003e-05, 'epoch': 3.2}
{'loss': 5.3312, 'grad_norm': 20.322038650512695, 'learning_rate': 2e-05, 'epoch': 4.0}
{'loss': 4.5756, 'grad_norm': 20.115598678588867, 'learning_rate': 2.4e-05, 'epoch': 4.8}
{'loss': 4.2953, 'grad_norm': 19.210113525390625, 'learning_rate': 2.8000000000000003e-05, 'epoch': 5.6}
{'loss': 3.286, 'grad_norm': 16.74553680419922, 'learning_rate': 3.2000000000000005e-05, 'epoch': 6.4}
{'loss': 2.9769, 'grad_norm': 16.231828689575195, 'learning_rate': 3.6e-05, 'epoch': 7.2}
{'loss': 2.4046, 'grad_norm': 10.694672584533691, 'learning_rate': 4e-05, 'epoch': 8.0}
{'train_runtime': 116.2384, 'train_samples_per_second': 3.441, 'train_steps_per_second': 0.086, 'train_loss': 4.525346469879151, 'epoch': 8.0}
[1;34mwandb[0m: üöÄ View run [33mmistral-7b-instruct-v0.3_ZsRE_40[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/9aur0jfc[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241104_230103-9aur0jfc/logs[0m
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:05<00:10,  5.43s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:09<00:04,  4.79s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.39s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.56s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_40/part_0/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_40/part_0/eval
0  to  50
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_40/part_0/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
550  to  600
generating!
600  to  650
generating!
650  to  700
generating!
700  to  750
generating!
750  to  800
generating!
800  to  850
generating!
850  to  900
generating!
900  to  950
generating!
950  to  1000
generating!
1000  to  1050
generating!
1050  to  1100
generating!
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_40/part_0/adapter_model.safetensors
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_40/part_0/tmp_data.jsonl
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_40/part_0/adapter_config.json
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_40/part_0/wandb
Skipping file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_40/part_0/log.txt
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_40/part_0/special_tokens_map.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_40/part_0/tokenizer.model
Skipping directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_40/part_0/eval
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_40/part_0/tokenizer_config.json
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_40/part_0/checkpoint-10
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_40/part_0/README.md
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=0, data_source=ZsRE, data_size=45, random=False
Loading data from ../../data/edit_data/merged_data_part_0.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 45 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_45/part_0/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/mistral-7b-instruct-v0.3
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_45/part_0
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: mistral-7b-instruct-v0.3_ZsRE_45
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:03<00:07,  3.91s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:06<00:03,  3.39s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:09<00:00,  3.09s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:09<00:00,  3.22s/it]
Using custom data configuration default-6da7bca15babe8c6
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-6da7bca15babe8c6/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2461.45it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 286.28it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-6da7bca15babe8c6/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 233.77it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7ff856524ee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 28,311,552 || all params: 7,276,335,104 || trainable%: 0.3890908210705739
0ex [00:00, ?ex/s]1ex [00:00,  6.65ex/s]45ex [00:00, 222.66ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241104_230925-hy4zfn9v
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mistral-7b-instruct-v0.3_ZsRE_45
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/hy4zfn9v

Example:
<s> What war or battle involved Alec Rose? Spanish Civil War</s>


Example:
<s> Which was the family of Miliolacea? Agaricaceae</s>


Example:
<s> Which language is Pleine Vie written in? Coptic</s>


Example:
<s> The country for Ang TV was what? Sri Lanka</s>


Example:
<s> What disease did Harlo Jones have? pneumonia</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 45
})
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:11<01:43, 11.55s/it]                                               10%|‚ñà         | 1/10 [00:11<01:43, 11.55s/it] 20%|‚ñà‚ñà        | 2/10 [00:22<01:29, 11.23s/it]                                               20%|‚ñà‚ñà        | 2/10 [00:22<01:29, 11.23s/it] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:33<01:17, 11.14s/it]                                               30%|‚ñà‚ñà‚ñà       | 3/10 [00:33<01:17, 11.14s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:44<01:06, 11.10s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:44<01:06, 11.10s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:55<00:55, 11.08s/it]                                               50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:55<00:55, 11.08s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:06<00:44, 11.07s/it]                                               60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:06<00:44, 11.07s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:17<00:33, 11.06s/it]                                               70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:17<00:33, 11.06s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:28<00:22, 11.06s/it]                                               80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:28<00:22, 11.06s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:39<00:11, 11.06s/it]                                               90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:39<00:11, 11.06s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:50<00:00, 11.05s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:50<00:00, 11.05s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:52<00:00, 11.05s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:52<00:00, 11.28s/it]
{'loss': 5.751, 'grad_norm': 19.478464126586914, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.71}
{'loss': 5.3156, 'grad_norm': 20.562366485595703, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.42}
{'loss': 5.9877, 'grad_norm': 21.277524948120117, 'learning_rate': 1.2e-05, 'epoch': 2.13}
{'loss': 5.619, 'grad_norm': 20.85005760192871, 'learning_rate': 1.6000000000000003e-05, 'epoch': 2.84}
{'loss': 4.8082, 'grad_norm': 18.18910026550293, 'learning_rate': 2e-05, 'epoch': 3.56}
{'loss': 4.8784, 'grad_norm': 20.8004093170166, 'learning_rate': 2.4e-05, 'epoch': 4.27}
{'loss': 3.8997, 'grad_norm': 18.647216796875, 'learning_rate': 2.8000000000000003e-05, 'epoch': 4.98}
{'loss': 3.1776, 'grad_norm': 15.672426223754883, 'learning_rate': 3.2000000000000005e-05, 'epoch': 5.69}
{'loss': 2.9405, 'grad_norm': 15.12256145477295, 'learning_rate': 3.6e-05, 'epoch': 6.4}
{'loss': 2.2915, 'grad_norm': 11.256553649902344, 'learning_rate': 4e-05, 'epoch': 7.11}
{'train_runtime': 115.2628, 'train_samples_per_second': 3.904, 'train_steps_per_second': 0.087, 'train_loss': 4.466926002502442, 'epoch': 7.11}
[1;34mwandb[0m: üöÄ View run [33mmistral-7b-instruct-v0.3_ZsRE_45[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/hy4zfn9v[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241104_230925-hy4zfn9v/logs[0m
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:04<00:09,  4.88s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:09<00:04,  4.56s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.28s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.39s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_45/part_0/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_45/part_0/eval
0  to  50
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_45/part_0/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
550  to  600
generating!
600  to  650
generating!
650  to  700
generating!
700  to  750
generating!
750  to  800
generating!
800  to  850
generating!
850  to  900
generating!
900  to  950
generating!
950  to  1000
generating!
1000  to  1050
generating!
1050  to  1100
generating!
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_45/part_0/adapter_config.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_45/part_0/README.md
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_45/part_0/tokenizer_config.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_45/part_0/tmp_data.jsonl
Skipping file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_45/part_0/log.txt
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_45/part_0/tokenizer.model
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_45/part_0/checkpoint-10
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_45/part_0/special_tokens_map.json
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_45/part_0/wandb
Skipping directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_45/part_0/eval
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_45/part_0/adapter_model.safetensors
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=0, data_source=ZsRE, data_size=50, random=False
Loading data from ../../data/edit_data/merged_data_part_0.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 50 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_50/part_0/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/mistral-7b-instruct-v0.3
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_50/part_0
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: mistral-7b-instruct-v0.3_ZsRE_50
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:03<00:06,  3.11s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:06<00:03,  3.06s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:08<00:00,  2.91s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:08<00:00,  2.96s/it]
Using custom data configuration default-67c9dbd438a27fe1
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-67c9dbd438a27fe1/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 6743.25it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 499.86it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-67c9dbd438a27fe1/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 247.41it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f7026062ee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 28,311,552 || all params: 7,276,335,104 || trainable%: 0.3890908210705739
0ex [00:00, ?ex/s]16ex [00:00, 159.20ex/s]50ex [00:00, 361.27ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241104_231737-80d3z2rk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mistral-7b-instruct-v0.3_ZsRE_50
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/80d3z2rk

Example:
<s> What constellation is home to Butterfly Cluster? Orion</s>


Example:
<s> The mother of Mallory Reaves is whom? Lalli Reaves</s>


Example:
<s> Which country's citizenship does Pedro Magallanes hold? Colombia</s>


Example:
<s> In what living being can CD4 be found? human</s>


Example:
<s> Over which river does Dexter Coffin Bridge cross? Connecticut Creek</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 50
})
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:11<01:44, 11.56s/it]                                               10%|‚ñà         | 1/10 [00:11<01:44, 11.56s/it] 20%|‚ñà‚ñà        | 2/10 [00:22<01:29, 11.24s/it]                                               20%|‚ñà‚ñà        | 2/10 [00:22<01:29, 11.24s/it] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:33<01:18, 11.15s/it]                                               30%|‚ñà‚ñà‚ñà       | 3/10 [00:33<01:18, 11.15s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:44<01:06, 11.11s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:44<01:06, 11.11s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:55<00:55, 11.08s/it]                                               50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:55<00:55, 11.08s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:06<00:44, 11.07s/it]                                               60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:06<00:44, 11.07s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:17<00:33, 11.06s/it]                                               70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:17<00:33, 11.06s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:28<00:22, 11.05s/it]                                               80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:28<00:22, 11.05s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:39<00:11, 11.05s/it]                                               90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:39<00:11, 11.05s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:50<00:00, 11.06s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:50<00:00, 11.06s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:53<00:00, 11.06s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:53<00:00, 11.33s/it]
{'loss': 5.8919, 'grad_norm': 20.791152954101562, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.64}
{'loss': 5.5133, 'grad_norm': 18.02977180480957, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.28}
{'loss': 5.9669, 'grad_norm': 20.533098220825195, 'learning_rate': 1.2e-05, 'epoch': 1.92}
{'loss': 5.4233, 'grad_norm': 20.759389877319336, 'learning_rate': 1.6000000000000003e-05, 'epoch': 2.56}
{'loss': 5.295, 'grad_norm': 20.209463119506836, 'learning_rate': 2e-05, 'epoch': 3.2}
{'loss': 4.4972, 'grad_norm': 20.602819442749023, 'learning_rate': 2.4e-05, 'epoch': 3.84}
{'loss': 4.4333, 'grad_norm': 19.615434646606445, 'learning_rate': 2.8000000000000003e-05, 'epoch': 4.48}
{'loss': 3.7986, 'grad_norm': 20.046419143676758, 'learning_rate': 3.2000000000000005e-05, 'epoch': 5.12}
{'loss': 2.9872, 'grad_norm': 15.628405570983887, 'learning_rate': 3.6e-05, 'epoch': 5.76}
{'loss': 2.4291, 'grad_norm': 11.32312297821045, 'learning_rate': 4e-05, 'epoch': 6.4}
{'train_runtime': 115.3072, 'train_samples_per_second': 4.336, 'train_steps_per_second': 0.087, 'train_loss': 4.62358877658844, 'epoch': 6.4}
[1;34mwandb[0m: üöÄ View run [33mmistral-7b-instruct-v0.3_ZsRE_50[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/80d3z2rk[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241104_231737-80d3z2rk/logs[0m
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:05<00:10,  5.07s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:09<00:04,  4.64s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.30s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.44s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_50/part_0/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_50/part_0/eval
0  to  50
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_50/part_0/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
550  to  600
generating!
600  to  650
generating!
650  to  700
generating!
700  to  750
generating!
750  to  800
generating!
800  to  850
generating!
850  to  900
generating!
900  to  950
generating!
950  to  1000
generating!
1000  to  1050
generating!
1050  to  1100
generating!
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_50/part_0/checkpoint-10
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_50/part_0/adapter_model.safetensors
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_50/part_0/tokenizer.model
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_50/part_0/README.md
Skipping file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_50/part_0/log.txt
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_50/part_0/tmp_data.jsonl
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_50/part_0/wandb
Skipping directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_50/part_0/eval
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_50/part_0/special_tokens_map.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_50/part_0/adapter_config.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_50/part_0/tokenizer_config.json
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=0, data_source=ZsRE, data_size=55, random=False
Loading data from ../../data/edit_data/merged_data_part_0.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 55 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_55/part_0/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/mistral-7b-instruct-v0.3
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_55/part_0
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: mistral-7b-instruct-v0.3_ZsRE_55
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:03<00:07,  3.93s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:06<00:03,  3.39s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:09<00:00,  3.09s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:09<00:00,  3.23s/it]
Using custom data configuration default-cf6ffcf16bab2841
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-cf6ffcf16bab2841/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2396.75it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 286.44it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-cf6ffcf16bab2841/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 256.30it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f9bd5f4fee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 28,311,552 || all params: 7,276,335,104 || trainable%: 0.3890908210705739
0ex [00:00, ?ex/s]54ex [00:00, 538.34ex/s]55ex [00:00, 542.92ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241104_232556-qrbrppin
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mistral-7b-instruct-v0.3_ZsRE_55
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/qrbrppin

Example:
<s> Who developed Thomas the Tank Engine? William Orpen</s>


Example:
<s> What body of water does Suggan Buggan River join? Bass Strait</s>


Example:
<s> What is an ecological status of Bali myna?  myna</s>


Example:
<s> Who made Alexanderson alternator known? Ernest Alexanderson</s>


Example:
<s> What level is Javan surili's iucn conservation status? critically threatened</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 55
})
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:11<01:43, 11.53s/it]                                               10%|‚ñà         | 1/10 [00:11<01:43, 11.53s/it] 20%|‚ñà‚ñà        | 2/10 [00:22<01:29, 11.22s/it]                                               20%|‚ñà‚ñà        | 2/10 [00:22<01:29, 11.22s/it] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:33<01:18, 11.14s/it]                                               30%|‚ñà‚ñà‚ñà       | 3/10 [00:33<01:18, 11.14s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:44<01:06, 11.09s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:44<01:06, 11.09s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:55<00:55, 11.08s/it]                                               50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:55<00:55, 11.08s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:06<00:44, 11.07s/it]                                               60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:06<00:44, 11.07s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:17<00:33, 11.06s/it]                                               70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:17<00:33, 11.06s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:28<00:22, 11.06s/it]                                               80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:28<00:22, 11.06s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:39<00:11, 11.04s/it]                                               90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:39<00:11, 11.04s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:50<00:00, 11.04s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:50<00:00, 11.04s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:53<00:00, 11.04s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:53<00:00, 11.34s/it]
{'loss': 5.8587, 'grad_norm': 20.037351608276367, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.58}
{'loss': 6.156, 'grad_norm': 21.42214012145996, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.16}
{'loss': 5.6905, 'grad_norm': 20.41155433654785, 'learning_rate': 1.2e-05, 'epoch': 1.75}
{'loss': 6.2325, 'grad_norm': 21.055681228637695, 'learning_rate': 1.6000000000000003e-05, 'epoch': 2.33}
{'loss': 4.8953, 'grad_norm': 21.549793243408203, 'learning_rate': 2e-05, 'epoch': 2.91}
{'loss': 5.2093, 'grad_norm': 19.97749137878418, 'learning_rate': 2.4e-05, 'epoch': 3.49}
{'loss': 4.5813, 'grad_norm': 20.98097801208496, 'learning_rate': 2.8000000000000003e-05, 'epoch': 4.07}
{'loss': 3.6427, 'grad_norm': 17.377229690551758, 'learning_rate': 3.2000000000000005e-05, 'epoch': 4.65}
{'loss': 3.4562, 'grad_norm': 18.788673400878906, 'learning_rate': 3.6e-05, 'epoch': 5.24}
{'loss': 2.769, 'grad_norm': 12.582234382629395, 'learning_rate': 4e-05, 'epoch': 5.82}
{'train_runtime': 115.2574, 'train_samples_per_second': 4.772, 'train_steps_per_second': 0.087, 'train_loss': 4.849149417877197, 'epoch': 5.82}
[1;34mwandb[0m: üöÄ View run [33mmistral-7b-instruct-v0.3_ZsRE_55[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/qrbrppin[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241104_232556-qrbrppin/logs[0m
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:05<00:10,  5.03s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:09<00:04,  4.64s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.31s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.44s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_55/part_0/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_55/part_0/eval
0  to  50
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_55/part_0/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
550  to  600
generating!
600  to  650
generating!
650  to  700
generating!
700  to  750
generating!
750  to  800
generating!
800  to  850
generating!
850  to  900
generating!
900  to  950
generating!
950  to  1000
generating!
1000  to  1050
generating!
1050  to  1100
generating!
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_55/part_0/tmp_data.jsonl
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_55/part_0/wandb
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_55/part_0/checkpoint-10
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_55/part_0/adapter_model.safetensors
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_55/part_0/README.md
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_55/part_0/tokenizer.model
Skipping directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_55/part_0/eval
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_55/part_0/tokenizer_config.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_55/part_0/adapter_config.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_55/part_0/special_tokens_map.json
Skipping file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_55/part_0/log.txt
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=0, data_source=ZsRE, data_size=60, random=False
Loading data from ../../data/edit_data/merged_data_part_0.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 60 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_60/part_0/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/mistral-7b-instruct-v0.3
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_60/part_0
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: mistral-7b-instruct-v0.3_ZsRE_60
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:03<00:06,  3.11s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:06<00:03,  3.06s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:08<00:00,  2.91s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:08<00:00,  2.96s/it]
Using custom data configuration default-41d78f38462f715f
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-41d78f38462f715f/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2546.63it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 298.29it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-41d78f38462f715f/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 277.49it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f93f056dee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 28,311,552 || all params: 7,276,335,104 || trainable%: 0.3890908210705739
0ex [00:00, ?ex/s]15ex [00:00, 149.28ex/s]60ex [00:00, 410.24ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241104_233415-xz8jdohp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mistral-7b-instruct-v0.3_ZsRE_60
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/xz8jdohp

Example:
<s> Who developed Thomas the Tank Engine? William Orpen</s>


Example:
<s> What studio produced When China Met Africa? Famous Players Television</s>


Example:
<s> What is an ecological status of Bali myna?  myna</s>


Example:
<s> Who made Alexanderson alternator known? Ernest Alexanderson</s>


Example:
<s> What level is Javan surili's iucn conservation status? critically threatened</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 60
})
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:11<01:47, 11.90s/it]                                               10%|‚ñà         | 1/10 [00:11<01:47, 11.90s/it] 20%|‚ñà‚ñà        | 2/10 [00:22<01:31, 11.39s/it]                                               20%|‚ñà‚ñà        | 2/10 [00:22<01:31, 11.39s/it] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:33<01:18, 11.22s/it]                                               30%|‚ñà‚ñà‚ñà       | 3/10 [00:33<01:18, 11.22s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:45<01:06, 11.16s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:45<01:06, 11.16s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:56<00:55, 11.11s/it]                                               50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:56<00:55, 11.11s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:07<00:44, 11.10s/it]                                               60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:07<00:44, 11.10s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:18<00:33, 11.08s/it]                                               70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:18<00:33, 11.08s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:29<00:22, 11.06s/it]                                               80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:29<00:22, 11.06s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:40<00:11, 11.06s/it]                                               90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:40<00:11, 11.06s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:51<00:00, 11.07s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:51<00:00, 11.07s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:53<00:00, 11.07s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:53<00:00, 11.36s/it]
{'loss': 6.1735, 'grad_norm': 19.098541259765625, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.53}
{'loss': 5.7332, 'grad_norm': 18.850358963012695, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.07}
{'loss': 5.9115, 'grad_norm': 19.423587799072266, 'learning_rate': 1.2e-05, 'epoch': 1.6}
{'loss': 5.9252, 'grad_norm': 20.870891571044922, 'learning_rate': 1.6000000000000003e-05, 'epoch': 2.13}
{'loss': 5.2754, 'grad_norm': 21.42062759399414, 'learning_rate': 2e-05, 'epoch': 2.67}
{'loss': 5.1507, 'grad_norm': 22.562572479248047, 'learning_rate': 2.4e-05, 'epoch': 3.2}
{'loss': 4.3483, 'grad_norm': 18.691192626953125, 'learning_rate': 2.8000000000000003e-05, 'epoch': 3.73}
{'loss': 4.0806, 'grad_norm': 17.09960174560547, 'learning_rate': 3.2000000000000005e-05, 'epoch': 4.27}
{'loss': 3.5812, 'grad_norm': 16.891340255737305, 'learning_rate': 3.6e-05, 'epoch': 4.8}
{'loss': 2.4844, 'grad_norm': 12.94416618347168, 'learning_rate': 4e-05, 'epoch': 5.33}
{'train_runtime': 115.6305, 'train_samples_per_second': 5.189, 'train_steps_per_second': 0.086, 'train_loss': 4.866397142410278, 'epoch': 5.33}
[1;34mwandb[0m: üöÄ View run [33mmistral-7b-instruct-v0.3_ZsRE_60[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/xz8jdohp[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241104_233415-xz8jdohp/logs[0m
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:04<00:09,  4.88s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:09<00:04,  4.59s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.27s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.39s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_60/part_0/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_60/part_0/eval
0  to  50
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_60/part_0/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
550  to  600
generating!
600  to  650
generating!
650  to  700
generating!
700  to  750
generating!
750  to  800
generating!
800  to  850
generating!
850  to  900
generating!
900  to  950
generating!
950  to  1000
generating!
1000  to  1050
generating!
1050  to  1100
generating!
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_60/part_0/adapter_model.safetensors
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_60/part_0/tokenizer_config.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_60/part_0/tmp_data.jsonl
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_60/part_0/adapter_config.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_60/part_0/special_tokens_map.json
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_60/part_0/wandb
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_60/part_0/tokenizer.model
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_60/part_0/checkpoint-10
Skipping directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_60/part_0/eval
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_60/part_0/README.md
Skipping file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_60/part_0/log.txt
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=0, data_source=ZsRE, data_size=65, random=False
Loading data from ../../data/edit_data/merged_data_part_0.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 65 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_65/part_0/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/mistral-7b-instruct-v0.3
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_65/part_0
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: mistral-7b-instruct-v0.3_ZsRE_65
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:03<00:06,  3.12s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:06<00:03,  3.08s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:08<00:00,  2.92s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:08<00:00,  2.97s/it]
Using custom data configuration default-0e2c8bfe7dedb845
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-0e2c8bfe7dedb845/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2618.17it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 302.97it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-0e2c8bfe7dedb845/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 250.51it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7ff11c31eee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 28,311,552 || all params: 7,276,335,104 || trainable%: 0.3890908210705739
0ex [00:00, ?ex/s]27ex [00:00, 266.93ex/s]65ex [00:00, 477.53ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241104_234229-gadmbq0w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mistral-7b-instruct-v0.3_ZsRE_65
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/gadmbq0w

Example:
<s> What war or battle involved Alec Rose? Spanish Civil War</s>


Example:
<s> What was the founding year of Sigil Games Online? 1999</s>


Example:
<s> What is an ecological status of Bali myna?  myna</s>


Example:
<s> Who made Alexanderson alternator known? Ernest Alexanderson</s>


Example:
<s> What level is Javan surili's iucn conservation status? critically threatened</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 65
})
  0%|          | 0/20 [00:00<?, ?it/s]  5%|‚ñå         | 1/20 [00:11<03:39, 11.56s/it]                                                5%|‚ñå         | 1/20 [00:11<03:39, 11.56s/it] 10%|‚ñà         | 2/20 [00:22<03:22, 11.26s/it]                                               10%|‚ñà         | 2/20 [00:22<03:22, 11.26s/it] 15%|‚ñà‚ñå        | 3/20 [00:33<03:09, 11.17s/it]                                               15%|‚ñà‚ñå        | 3/20 [00:33<03:09, 11.17s/it] 20%|‚ñà‚ñà        | 4/20 [00:44<02:57, 11.12s/it]                                               20%|‚ñà‚ñà        | 4/20 [00:44<02:57, 11.12s/it] 25%|‚ñà‚ñà‚ñå       | 5/20 [00:55<02:46, 11.10s/it]                                               25%|‚ñà‚ñà‚ñå       | 5/20 [00:55<02:46, 11.10s/it] 30%|‚ñà‚ñà‚ñà       | 6/20 [01:06<02:35, 11.08s/it]                                               30%|‚ñà‚ñà‚ñà       | 6/20 [01:06<02:35, 11.08s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:17<02:24, 11.08s/it]                                               35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:17<02:24, 11.08s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:28<02:12, 11.07s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:28<02:12, 11.07s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:40<02:01, 11.06s/it]                                               45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:40<02:01, 11.06s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:51<01:50, 11.07s/it]                                                50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:51<01:50, 11.07s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [02:02<01:39, 11.07s/it]                                                55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [02:02<01:39, 11.07s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:13<01:28, 11.06s/it]                                                60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:13<01:28, 11.06s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:24<01:17, 11.07s/it]                                                65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:24<01:17, 11.07s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:35<01:06, 11.06s/it]                                                70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:35<01:06, 11.06s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:46<00:55, 11.05s/it]                                                75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:46<00:55, 11.05s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:57<00:44, 11.05s/it]                                                80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:57<00:44, 11.05s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [03:08<00:33, 11.06s/it]                                                85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [03:08<00:33, 11.06s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:19<00:22, 11.07s/it]                                                90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:19<00:22, 11.07s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:30<00:11, 11.06s/it]                                                95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:30<00:11, 11.06s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:42<00:00, 11.24s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:42<00:00, 11.24s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:44<00:00, 11.24s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:44<00:00, 11.24s/it]
{'loss': 6.33, 'grad_norm': 20.813783645629883, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.49}
{'loss': 5.6031, 'grad_norm': 19.29604721069336, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.98}
{'loss': 5.9804, 'grad_norm': 22.726890563964844, 'learning_rate': 1.2e-05, 'epoch': 1.48}
{'loss': 5.4545, 'grad_norm': 16.719636917114258, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.97}
{'loss': 4.9906, 'grad_norm': 19.952478408813477, 'learning_rate': 2e-05, 'epoch': 2.46}
{'loss': 5.0563, 'grad_norm': 21.551292419433594, 'learning_rate': 2.4e-05, 'epoch': 2.95}
{'loss': 4.2775, 'grad_norm': 18.860095977783203, 'learning_rate': 2.8000000000000003e-05, 'epoch': 3.45}
{'loss': 3.9471, 'grad_norm': 18.603071212768555, 'learning_rate': 3.2000000000000005e-05, 'epoch': 3.94}
{'loss': 3.0301, 'grad_norm': 16.232379913330078, 'learning_rate': 3.6e-05, 'epoch': 4.43}
{'loss': 2.8284, 'grad_norm': 12.712451934814453, 'learning_rate': 4e-05, 'epoch': 4.92}
{'loss': 2.5736, 'grad_norm': 11.476099014282227, 'learning_rate': 4.4000000000000006e-05, 'epoch': 5.42}
{'loss': 1.8327, 'grad_norm': 8.516863822937012, 'learning_rate': 4.8e-05, 'epoch': 5.91}
{'loss': 1.717, 'grad_norm': 9.962031364440918, 'learning_rate': 5.2000000000000004e-05, 'epoch': 6.4}
{'loss': 1.4383, 'grad_norm': 8.19466495513916, 'learning_rate': 5.6000000000000006e-05, 'epoch': 6.89}
{'loss': 1.142, 'grad_norm': 7.409011363983154, 'learning_rate': 6e-05, 'epoch': 7.38}
{'loss': 1.1123, 'grad_norm': 6.759729862213135, 'learning_rate': 6.400000000000001e-05, 'epoch': 7.88}
{'loss': 0.8553, 'grad_norm': 5.490322113037109, 'learning_rate': 6.800000000000001e-05, 'epoch': 8.37}
{'loss': 0.7743, 'grad_norm': 4.738149642944336, 'learning_rate': 7.2e-05, 'epoch': 8.86}
{'loss': 0.5621, 'grad_norm': 3.787423610687256, 'learning_rate': 7.6e-05, 'epoch': 9.35}
{'loss': 0.4781, 'grad_norm': 4.898404598236084, 'learning_rate': 8e-05, 'epoch': 9.85}
{'train_runtime': 226.7214, 'train_samples_per_second': 2.867, 'train_steps_per_second': 0.088, 'train_loss': 2.999180945754051, 'epoch': 9.85}
[1;34mwandb[0m: üöÄ View run [33mmistral-7b-instruct-v0.3_ZsRE_65[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/gadmbq0w[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241104_234229-gadmbq0w/logs[0m
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:05<00:10,  5.02s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:09<00:04,  4.64s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.30s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.43s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_65/part_0/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_65/part_0/eval
0  to  50
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_65/part_0/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
550  to  600
generating!
600  to  650
generating!
650  to  700
generating!
700  to  750
generating!
750  to  800
generating!
800  to  850
generating!
850  to  900
generating!
900  to  950
generating!
950  to  1000
generating!
1000  to  1050
generating!
1050  to  1100
generating!
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_65/part_0/adapter_config.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_65/part_0/tmp_data.jsonl
Skipping file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_65/part_0/log.txt
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_65/part_0/wandb
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_65/part_0/README.md
Skipping directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_65/part_0/eval
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_65/part_0/checkpoint-20
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_65/part_0/tokenizer_config.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_65/part_0/adapter_model.safetensors
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_65/part_0/special_tokens_map.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_65/part_0/tokenizer.model
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=0, data_source=ZsRE, data_size=70, random=False
Loading data from ../../data/edit_data/merged_data_part_0.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 70 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_70/part_0/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/mistral-7b-instruct-v0.3
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_70/part_0
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: mistral-7b-instruct-v0.3_ZsRE_70
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:04<00:08,  4.26s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:07<00:03,  3.54s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:10<00:00,  3.18s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:10<00:00,  3.35s/it]
Using custom data configuration default-2e679ed65bbafdcd
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-2e679ed65bbafdcd/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2725.34it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 340.09it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-2e679ed65bbafdcd/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 289.84it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7fa57c3d7ee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 28,311,552 || all params: 7,276,335,104 || trainable%: 0.3890908210705739
0ex [00:00, ?ex/s]15ex [00:00, 149.18ex/s]70ex [00:00, 453.53ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241104_235225-zzw9svp1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mistral-7b-instruct-v0.3_ZsRE_70
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/zzw9svp1

Example:
<s> What war did Alec Rose participate in? Spanish Civil War</s>


Example:
<s> Who made Alexanderson alternator known? Ernest Alexanderson</s>


Example:
<s> What sports team was Petteri Nummelin a member of? Columbus Blue Bombers</s>


Example:
<s> In which constellation is Tau Herculis? Hornax</s>


Example:
<s> What was the name of Charlotte of Schaumburg-Lippe mother? Charlotte of Bourbon-Parma</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 70
})
  0%|          | 0/20 [00:00<?, ?it/s]  5%|‚ñå         | 1/20 [00:11<03:45, 11.85s/it]                                                5%|‚ñå         | 1/20 [00:11<03:45, 11.85s/it] 10%|‚ñà         | 2/20 [00:22<03:24, 11.35s/it]                                               10%|‚ñà         | 2/20 [00:22<03:24, 11.35s/it] 15%|‚ñà‚ñå        | 3/20 [00:33<03:10, 11.19s/it]                                               15%|‚ñà‚ñå        | 3/20 [00:33<03:10, 11.19s/it] 20%|‚ñà‚ñà        | 4/20 [00:44<02:57, 11.12s/it]                                               20%|‚ñà‚ñà        | 4/20 [00:44<02:57, 11.12s/it] 25%|‚ñà‚ñà‚ñå       | 5/20 [00:55<02:46, 11.10s/it]                                               25%|‚ñà‚ñà‚ñå       | 5/20 [00:55<02:46, 11.10s/it] 30%|‚ñà‚ñà‚ñà       | 6/20 [01:06<02:34, 11.06s/it]                                               30%|‚ñà‚ñà‚ñà       | 6/20 [01:06<02:34, 11.06s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:17<02:23, 11.06s/it]                                               35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:17<02:23, 11.06s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:28<02:12, 11.04s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:28<02:12, 11.04s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:40<02:01, 11.04s/it]                                               45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:40<02:01, 11.04s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:51<01:50, 11.04s/it]                                                50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:51<01:50, 11.04s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [02:02<01:39, 11.03s/it]                                                55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [02:02<01:39, 11.03s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:13<01:28, 11.03s/it]                                                60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:13<01:28, 11.03s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:24<01:17, 11.03s/it]                                                65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:24<01:17, 11.03s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:35<01:06, 11.03s/it]                                                70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:35<01:06, 11.03s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:46<00:55, 11.03s/it]                                                75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:46<00:55, 11.03s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:57<00:44, 11.04s/it]                                                80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:57<00:44, 11.04s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [03:08<00:33, 11.03s/it]                                                85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [03:08<00:33, 11.03s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:19<00:22, 11.03s/it]                                                90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:19<00:22, 11.03s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:30<00:11, 11.03s/it]                                                95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:30<00:11, 11.03s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:41<00:00, 11.03s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:41<00:00, 11.03s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:43<00:00, 11.03s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:43<00:00, 11.17s/it]
{'loss': 5.7051, 'grad_norm': 20.862592697143555, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.46}
{'loss': 6.177, 'grad_norm': 20.458948135375977, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.91}
{'loss': 6.3097, 'grad_norm': 21.634765625, 'learning_rate': 1.2e-05, 'epoch': 1.37}
{'loss': 5.3449, 'grad_norm': 19.323694229125977, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.83}
{'loss': 5.3526, 'grad_norm': 20.417211532592773, 'learning_rate': 2e-05, 'epoch': 2.29}
{'loss': 4.9442, 'grad_norm': 22.186443328857422, 'learning_rate': 2.4e-05, 'epoch': 2.74}
{'loss': 4.135, 'grad_norm': 19.60959243774414, 'learning_rate': 2.8000000000000003e-05, 'epoch': 3.2}
{'loss': 4.5363, 'grad_norm': 19.640071868896484, 'learning_rate': 3.2000000000000005e-05, 'epoch': 3.66}
{'loss': 2.5932, 'grad_norm': 14.852550506591797, 'learning_rate': 3.6e-05, 'epoch': 4.11}
{'loss': 2.8352, 'grad_norm': 13.022923469543457, 'learning_rate': 4e-05, 'epoch': 4.57}
{'loss': 2.4831, 'grad_norm': 11.671156883239746, 'learning_rate': 4.4000000000000006e-05, 'epoch': 5.03}
{'loss': 2.0026, 'grad_norm': 8.817179679870605, 'learning_rate': 4.8e-05, 'epoch': 5.49}
{'loss': 1.7284, 'grad_norm': 9.2482328414917, 'learning_rate': 5.2000000000000004e-05, 'epoch': 5.94}
{'loss': 1.5143, 'grad_norm': 8.646601676940918, 'learning_rate': 5.6000000000000006e-05, 'epoch': 6.4}
{'loss': 1.2592, 'grad_norm': 6.1977620124816895, 'learning_rate': 6e-05, 'epoch': 6.86}
{'loss': 1.2621, 'grad_norm': 8.749299049377441, 'learning_rate': 6.400000000000001e-05, 'epoch': 7.31}
{'loss': 0.7801, 'grad_norm': 5.302032947540283, 'learning_rate': 6.800000000000001e-05, 'epoch': 7.77}
{'loss': 0.9952, 'grad_norm': 6.792725086212158, 'learning_rate': 7.2e-05, 'epoch': 8.23}
{'loss': 0.6184, 'grad_norm': 4.6609601974487305, 'learning_rate': 7.6e-05, 'epoch': 8.69}
{'loss': 0.326, 'grad_norm': 3.513848066329956, 'learning_rate': 8e-05, 'epoch': 9.14}
{'train_runtime': 225.5896, 'train_samples_per_second': 3.103, 'train_steps_per_second': 0.089, 'train_loss': 3.0451350688934324, 'epoch': 9.14}
[1;34mwandb[0m: üöÄ View run [33mmistral-7b-instruct-v0.3_ZsRE_70[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/zzw9svp1[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241104_235225-zzw9svp1/logs[0m
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:04<00:09,  4.51s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:08<00:04,  4.42s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:12<00:00,  4.18s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:12<00:00,  4.26s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_70/part_0/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_70/part_0/eval
0  to  50
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_70/part_0/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
550  to  600
generating!
600  to  650
generating!
650  to  700
generating!
700  to  750
generating!
750  to  800
generating!
800  to  850
generating!
850  to  900
generating!
900  to  950
generating!
950  to  1000
generating!
1000  to  1050
generating!
1050  to  1100
generating!
Skipping file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_70/part_0/log.txt
Skipping directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_70/part_0/eval
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_70/part_0/adapter_config.json
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_70/part_0/wandb
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_70/part_0/tokenizer.model
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_70/part_0/tmp_data.jsonl
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_70/part_0/tokenizer_config.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_70/part_0/README.md
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_70/part_0/checkpoint-20
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_70/part_0/special_tokens_map.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_70/part_0/adapter_model.safetensors
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=0, data_source=ZsRE, data_size=75, random=False
Loading data from ../../data/edit_data/merged_data_part_0.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 75 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_75/part_0/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/mistral-7b-instruct-v0.3
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_75/part_0
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: mistral-7b-instruct-v0.3_ZsRE_75
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:03<00:06,  3.13s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:06<00:03,  3.08s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:08<00:00,  2.93s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:08<00:00,  2.98s/it]
Using custom data configuration default-539150d8617ff320
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-539150d8617ff320/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2666.44it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 324.86it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-539150d8617ff320/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 261.25it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f35fc451ee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 28,311,552 || all params: 7,276,335,104 || trainable%: 0.3890908210705739
0ex [00:00, ?ex/s]1ex [00:00,  7.62ex/s]75ex [00:00, 364.59ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_000212-71io3ee3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mistral-7b-instruct-v0.3_ZsRE_75
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/71io3ee3

Example:
<s> What is Hannelore Kohl's spouse's name? John Kohl</s>


Example:
<s> What war or battle involved Alec Rose? Spanish Civil War</s>


Example:
<s> Which species has the CXCL10 gene? male</s>


Example:
<s> What disease did Harlo Jones have? pneumonia</s>


Example:
<s> Who designed the Heroes Chronicles? Chris Riddell</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 75
})
  0%|          | 0/20 [00:00<?, ?it/s]  5%|‚ñå         | 1/20 [00:11<03:46, 11.91s/it]                                                5%|‚ñå         | 1/20 [00:11<03:46, 11.91s/it] 10%|‚ñà         | 2/20 [00:22<03:24, 11.38s/it]                                               10%|‚ñà         | 2/20 [00:22<03:24, 11.38s/it] 15%|‚ñà‚ñå        | 3/20 [00:33<03:11, 11.24s/it]                                               15%|‚ñà‚ñå        | 3/20 [00:33<03:11, 11.24s/it] 20%|‚ñà‚ñà        | 4/20 [00:45<02:58, 11.17s/it]                                               20%|‚ñà‚ñà        | 4/20 [00:45<02:58, 11.17s/it] 25%|‚ñà‚ñà‚ñå       | 5/20 [00:56<02:46, 11.12s/it]                                               25%|‚ñà‚ñà‚ñå       | 5/20 [00:56<02:46, 11.12s/it] 30%|‚ñà‚ñà‚ñà       | 6/20 [01:07<02:35, 11.10s/it]                                               30%|‚ñà‚ñà‚ñà       | 6/20 [01:07<02:35, 11.10s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:18<02:24, 11.08s/it]                                               35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:18<02:24, 11.08s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:29<02:12, 11.07s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:29<02:12, 11.07s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:40<02:01, 11.07s/it]                                               45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:40<02:01, 11.07s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:51<01:50, 11.06s/it]                                                50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:51<01:50, 11.06s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [02:02<01:39, 11.06s/it]                                                55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [02:02<01:39, 11.06s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:13<01:28, 11.07s/it]                                                60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:13<01:28, 11.07s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:24<01:17, 11.06s/it]                                                65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:24<01:17, 11.06s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:35<01:06, 11.06s/it]                                                70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:35<01:06, 11.06s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:46<00:55, 11.06s/it]                                                75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:46<00:55, 11.06s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:57<00:44, 11.05s/it]                                                80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:57<00:44, 11.05s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [03:08<00:33, 11.06s/it]                                                85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [03:08<00:33, 11.06s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:19<00:22, 11.05s/it]                                                90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:19<00:22, 11.05s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:30<00:11, 11.06s/it]                                                95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:30<00:11, 11.06s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:41<00:00, 11.08s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:41<00:00, 11.08s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:45<00:00, 11.08s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:45<00:00, 11.27s/it]
{'loss': 5.8842, 'grad_norm': 20.359689712524414, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.43}
{'loss': 6.1864, 'grad_norm': 19.69084358215332, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.85}
{'loss': 5.1667, 'grad_norm': 20.170146942138672, 'learning_rate': 1.2e-05, 'epoch': 1.28}
{'loss': 5.5037, 'grad_norm': 19.641996383666992, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.71}
{'loss': 5.821, 'grad_norm': 19.654212951660156, 'learning_rate': 2e-05, 'epoch': 2.13}
{'loss': 4.3271, 'grad_norm': 20.452688217163086, 'learning_rate': 2.4e-05, 'epoch': 2.56}
{'loss': 4.6004, 'grad_norm': 21.459110260009766, 'learning_rate': 2.8000000000000003e-05, 'epoch': 2.99}
{'loss': 3.9489, 'grad_norm': 19.2880859375, 'learning_rate': 3.2000000000000005e-05, 'epoch': 3.41}
{'loss': 3.2838, 'grad_norm': 16.044816970825195, 'learning_rate': 3.6e-05, 'epoch': 3.84}
{'loss': 2.6909, 'grad_norm': 13.265267372131348, 'learning_rate': 4e-05, 'epoch': 4.27}
{'loss': 2.1187, 'grad_norm': 9.422813415527344, 'learning_rate': 4.4000000000000006e-05, 'epoch': 4.69}
{'loss': 2.1966, 'grad_norm': 9.790718078613281, 'learning_rate': 4.8e-05, 'epoch': 5.12}
{'loss': 1.7387, 'grad_norm': 7.0665507316589355, 'learning_rate': 5.2000000000000004e-05, 'epoch': 5.55}
{'loss': 1.6292, 'grad_norm': 9.452091217041016, 'learning_rate': 5.6000000000000006e-05, 'epoch': 5.97}
{'loss': 1.4919, 'grad_norm': 7.824100017547607, 'learning_rate': 6e-05, 'epoch': 6.4}
{'loss': 1.1943, 'grad_norm': 6.826706409454346, 'learning_rate': 6.400000000000001e-05, 'epoch': 6.83}
{'loss': 0.8401, 'grad_norm': 4.57837438583374, 'learning_rate': 6.800000000000001e-05, 'epoch': 7.25}
{'loss': 0.824, 'grad_norm': 5.391427993774414, 'learning_rate': 7.2e-05, 'epoch': 7.68}
{'loss': 0.8872, 'grad_norm': 5.629083633422852, 'learning_rate': 7.6e-05, 'epoch': 8.11}
{'loss': 0.3976, 'grad_norm': 4.137985706329346, 'learning_rate': 8e-05, 'epoch': 8.53}
{'train_runtime': 227.8304, 'train_samples_per_second': 3.292, 'train_steps_per_second': 0.088, 'train_loss': 3.0365661844611167, 'epoch': 8.53}
[1;34mwandb[0m: üöÄ View run [33mmistral-7b-instruct-v0.3_ZsRE_75[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/71io3ee3[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_000212-71io3ee3/logs[0m
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:05<00:10,  5.04s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:09<00:04,  4.63s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.30s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.43s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_75/part_0/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_75/part_0/eval
0  to  50
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_75/part_0/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
550  to  600
generating!
600  to  650
generating!
650  to  700
generating!
700  to  750
generating!
750  to  800
generating!
800  to  850
generating!
850  to  900
generating!
900  to  950
generating!
950  to  1000
generating!
1000  to  1050
generating!
1050  to  1100
generating!
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_75/part_0/wandb
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_75/part_0/adapter_config.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_75/part_0/tmp_data.jsonl
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_75/part_0/tokenizer.model
Skipping file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_75/part_0/log.txt
Skipping directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_75/part_0/eval
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_75/part_0/checkpoint-20
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_75/part_0/tokenizer_config.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_75/part_0/adapter_model.safetensors
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_75/part_0/special_tokens_map.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_75/part_0/README.md
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=0, data_source=ZsRE, data_size=80, random=False
Loading data from ../../data/edit_data/merged_data_part_0.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 80 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_80/part_0/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/mistral-7b-instruct-v0.3
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_80/part_0
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: mistral-7b-instruct-v0.3_ZsRE_80
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:04<00:08,  4.08s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:07<00:03,  3.47s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:09<00:00,  3.14s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:09<00:00,  3.29s/it]
Using custom data configuration default-6536aba5024e8cd4
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-6536aba5024e8cd4/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2274.57it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 285.75it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-6536aba5024e8cd4/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 243.13it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f53e1b56ee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 28,311,552 || all params: 7,276,335,104 || trainable%: 0.3890908210705739
0ex [00:00, ?ex/s]28ex [00:00, 277.10ex/s]80ex [00:00, 551.01ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_001212-qqtucqy3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mistral-7b-instruct-v0.3_ZsRE_80
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/qqtucqy3

Example:
<s> Which species has the CXCL10 gene? male</s>


Example:
<s> Who was Arwen's mother? Doris</s>


Example:
<s> What war or battle did Ridgely Gaither fight in? World War II</s>


Example:
<s> Who developed Thomas the Tank Engine? William Orpen</s>


Example:
<s> What company published Alien Front Online? 2K Games</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 80
})
  0%|          | 0/20 [00:00<?, ?it/s]  5%|‚ñå         | 1/20 [00:11<03:46, 11.91s/it]                                                5%|‚ñå         | 1/20 [00:11<03:46, 11.91s/it] 10%|‚ñà         | 2/20 [00:22<03:24, 11.37s/it]                                               10%|‚ñà         | 2/20 [00:22<03:24, 11.37s/it] 15%|‚ñà‚ñå        | 3/20 [00:33<03:11, 11.24s/it]                                               15%|‚ñà‚ñå        | 3/20 [00:33<03:11, 11.24s/it] 20%|‚ñà‚ñà        | 4/20 [00:45<02:58, 11.16s/it]                                               20%|‚ñà‚ñà        | 4/20 [00:45<02:58, 11.16s/it] 25%|‚ñà‚ñà‚ñå       | 5/20 [00:56<02:46, 11.12s/it]                                               25%|‚ñà‚ñà‚ñå       | 5/20 [00:56<02:46, 11.12s/it] 30%|‚ñà‚ñà‚ñà       | 6/20 [01:07<02:35, 11.10s/it]                                               30%|‚ñà‚ñà‚ñà       | 6/20 [01:07<02:35, 11.10s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:18<02:24, 11.10s/it]                                               35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:18<02:24, 11.10s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:29<02:12, 11.06s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:29<02:12, 11.06s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:40<02:01, 11.07s/it]                                               45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:40<02:01, 11.07s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:51<01:50, 11.07s/it]                                                50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:51<01:50, 11.07s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [02:02<01:39, 11.07s/it]                                                55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [02:02<01:39, 11.07s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:13<01:28, 11.06s/it]                                                60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:13<01:28, 11.06s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:24<01:17, 11.06s/it]                                                65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:24<01:17, 11.06s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:35<01:06, 11.05s/it]                                                70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:35<01:06, 11.05s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:46<00:55, 11.06s/it]                                                75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:46<00:55, 11.06s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:57<00:44, 11.06s/it]                                                80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:57<00:44, 11.06s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [03:08<00:33, 11.06s/it]                                                85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [03:08<00:33, 11.06s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:19<00:22, 11.05s/it]                                                90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:19<00:22, 11.05s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:30<00:11, 11.05s/it]                                                95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:30<00:11, 11.05s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:41<00:00, 11.06s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:41<00:00, 11.06s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:45<00:00, 11.06s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:45<00:00, 11.30s/it]
{'loss': 6.0612, 'grad_norm': 18.192760467529297, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.4}
{'loss': 5.9563, 'grad_norm': 22.533893585205078, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.8}
{'loss': 5.1964, 'grad_norm': 19.50560188293457, 'learning_rate': 1.2e-05, 'epoch': 1.2}
{'loss': 5.7628, 'grad_norm': 18.224414825439453, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.6}
{'loss': 5.2375, 'grad_norm': 21.72490119934082, 'learning_rate': 2e-05, 'epoch': 2.0}
{'loss': 4.8675, 'grad_norm': 20.116641998291016, 'learning_rate': 2.4e-05, 'epoch': 2.4}
{'loss': 4.2457, 'grad_norm': 18.202730178833008, 'learning_rate': 2.8000000000000003e-05, 'epoch': 2.8}
{'loss': 4.1713, 'grad_norm': 22.136032104492188, 'learning_rate': 3.2000000000000005e-05, 'epoch': 3.2}
{'loss': 3.0309, 'grad_norm': 15.124126434326172, 'learning_rate': 3.6e-05, 'epoch': 3.6}
{'loss': 2.9354, 'grad_norm': 12.522106170654297, 'learning_rate': 4e-05, 'epoch': 4.0}
{'loss': 1.948, 'grad_norm': 8.476837158203125, 'learning_rate': 4.4000000000000006e-05, 'epoch': 4.4}
{'loss': 2.3585, 'grad_norm': 9.98330020904541, 'learning_rate': 4.8e-05, 'epoch': 4.8}
{'loss': 1.7017, 'grad_norm': 7.671377658843994, 'learning_rate': 5.2000000000000004e-05, 'epoch': 5.2}
{'loss': 1.6428, 'grad_norm': 7.705329418182373, 'learning_rate': 5.6000000000000006e-05, 'epoch': 5.6}
{'loss': 1.5234, 'grad_norm': 7.118739604949951, 'learning_rate': 6e-05, 'epoch': 6.0}
{'loss': 1.0469, 'grad_norm': 6.184960842132568, 'learning_rate': 6.400000000000001e-05, 'epoch': 6.4}
{'loss': 1.2626, 'grad_norm': 5.374624252319336, 'learning_rate': 6.800000000000001e-05, 'epoch': 6.8}
{'loss': 0.9533, 'grad_norm': 10.496464729309082, 'learning_rate': 7.2e-05, 'epoch': 7.2}
{'loss': 0.6581, 'grad_norm': 4.049395561218262, 'learning_rate': 7.6e-05, 'epoch': 7.6}
{'loss': 0.6961, 'grad_norm': 5.690389156341553, 'learning_rate': 8e-05, 'epoch': 8.0}
{'train_runtime': 228.1207, 'train_samples_per_second': 3.507, 'train_steps_per_second': 0.088, 'train_loss': 3.0628189712762834, 'epoch': 8.0}
[1;34mwandb[0m: üöÄ View run [33mmistral-7b-instruct-v0.3_ZsRE_80[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/qqtucqy3[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_001212-qqtucqy3/logs[0m
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:05<00:10,  5.09s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:09<00:04,  4.66s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.32s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.46s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_80/part_0/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_80/part_0/eval
0  to  50
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_80/part_0/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
550  to  600
generating!
600  to  650
generating!
650  to  700
generating!
700  to  750
generating!
750  to  800
generating!
800  to  850
generating!
850  to  900
generating!
900  to  950
generating!
950  to  1000
generating!
1000  to  1050
generating!
1050  to  1100
generating!
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_80/part_0/adapter_config.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_80/part_0/tokenizer_config.json
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_80/part_0/wandb
Skipping file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_80/part_0/log.txt
Skipping directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_80/part_0/eval
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_80/part_0/tmp_data.jsonl
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_80/part_0/checkpoint-20
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_80/part_0/special_tokens_map.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_80/part_0/adapter_model.safetensors
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_80/part_0/README.md
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_80/part_0/tokenizer.model
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=0, data_source=ZsRE, data_size=85, random=False
Loading data from ../../data/edit_data/merged_data_part_0.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 85 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_85/part_0/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/mistral-7b-instruct-v0.3
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_85/part_0
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: mistral-7b-instruct-v0.3_ZsRE_85
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:03<00:06,  3.11s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:06<00:03,  3.08s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:08<00:00,  2.93s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:08<00:00,  2.97s/it]
Using custom data configuration default-1d9bb026b9c48ac6
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-1d9bb026b9c48ac6/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2699.04it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 309.45it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-1d9bb026b9c48ac6/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 277.90it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f46a8355ee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 28,311,552 || all params: 7,276,335,104 || trainable%: 0.3890908210705739
0ex [00:00, ?ex/s]15ex [00:00, 149.20ex/s]85ex [00:00, 517.97ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_002209-2ttur0xa
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mistral-7b-instruct-v0.3_ZsRE_85
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/2ttur0xa

Example:
<s> What was the name of Charlotte of Schaumburg-Lippe mother? Charlotte of Bourbon-Parma</s>


Example:
<s> Who was the mother of Hans Ulrik Gyldenl√∏ve? Marie Louise F√∂hse</s>


Example:
<s> What voice type is Louise Grandjean? mezzo soprano</s>


Example:
<s> The father of Juno Temple is whom? Jupiter</s>


Example:
<s> What kind of family is Gabb's snail of? Lymantriurus</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 85
})
  0%|          | 0/20 [00:00<?, ?it/s]  5%|‚ñå         | 1/20 [00:11<03:45, 11.86s/it]                                                5%|‚ñå         | 1/20 [00:11<03:45, 11.86s/it] 10%|‚ñà         | 2/20 [00:22<03:24, 11.34s/it]                                               10%|‚ñà         | 2/20 [00:22<03:24, 11.34s/it] 15%|‚ñà‚ñå        | 3/20 [00:33<03:10, 11.19s/it]                                               15%|‚ñà‚ñå        | 3/20 [00:33<03:10, 11.19s/it] 20%|‚ñà‚ñà        | 4/20 [00:44<02:58, 11.13s/it]                                               20%|‚ñà‚ñà        | 4/20 [00:44<02:58, 11.13s/it] 25%|‚ñà‚ñà‚ñå       | 5/20 [00:55<02:46, 11.09s/it]                                               25%|‚ñà‚ñà‚ñå       | 5/20 [00:55<02:46, 11.09s/it] 30%|‚ñà‚ñà‚ñà       | 6/20 [01:06<02:34, 11.06s/it]                                               30%|‚ñà‚ñà‚ñà       | 6/20 [01:06<02:34, 11.06s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:17<02:23, 11.05s/it]                                               35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:17<02:23, 11.05s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:28<02:12, 11.04s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:28<02:12, 11.04s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:39<02:01, 11.03s/it]                                               45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:39<02:01, 11.03s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:51<01:50, 11.03s/it]                                                50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:51<01:50, 11.03s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [02:02<01:39, 11.03s/it]                                                55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [02:02<01:39, 11.03s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:13<01:28, 11.02s/it]                                                60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:13<01:28, 11.02s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:24<01:17, 11.02s/it]                                                65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:24<01:17, 11.02s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:35<01:06, 11.03s/it]                                                70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:35<01:06, 11.03s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:46<00:55, 11.03s/it]                                                75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:46<00:55, 11.03s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:57<00:44, 11.02s/it]                                                80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:57<00:44, 11.02s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [03:08<00:33, 11.02s/it]                                                85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [03:08<00:33, 11.02s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:19<00:22, 11.02s/it]                                                90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:19<00:22, 11.02s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:30<00:11, 11.02s/it]                                                95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:30<00:11, 11.02s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:41<00:00, 11.03s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:41<00:00, 11.03s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:44<00:00, 11.03s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:44<00:00, 11.24s/it]
{'loss': 5.259, 'grad_norm': 18.51133918762207, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.38}
{'loss': 6.7333, 'grad_norm': 20.583486557006836, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.75}
{'loss': 5.6859, 'grad_norm': 21.791889190673828, 'learning_rate': 1.2e-05, 'epoch': 1.13}
{'loss': 5.5378, 'grad_norm': 19.435678482055664, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.51}
{'loss': 5.8063, 'grad_norm': 20.34625816345215, 'learning_rate': 2e-05, 'epoch': 1.88}
{'loss': 5.0251, 'grad_norm': 19.218080520629883, 'learning_rate': 2.4e-05, 'epoch': 2.26}
{'loss': 4.7771, 'grad_norm': 22.875102996826172, 'learning_rate': 2.8000000000000003e-05, 'epoch': 2.64}
{'loss': 3.7972, 'grad_norm': 18.32314109802246, 'learning_rate': 3.2000000000000005e-05, 'epoch': 3.01}
{'loss': 3.703, 'grad_norm': 18.420589447021484, 'learning_rate': 3.6e-05, 'epoch': 3.39}
{'loss': 2.4642, 'grad_norm': 11.40951156616211, 'learning_rate': 4e-05, 'epoch': 3.76}
{'loss': 2.7462, 'grad_norm': 11.949472427368164, 'learning_rate': 4.4000000000000006e-05, 'epoch': 4.14}
{'loss': 1.9372, 'grad_norm': 7.55188512802124, 'learning_rate': 4.8e-05, 'epoch': 4.52}
{'loss': 1.9013, 'grad_norm': 9.433880805969238, 'learning_rate': 5.2000000000000004e-05, 'epoch': 4.89}
{'loss': 1.5082, 'grad_norm': 7.895663261413574, 'learning_rate': 5.6000000000000006e-05, 'epoch': 5.27}
{'loss': 1.6526, 'grad_norm': 8.23681640625, 'learning_rate': 6e-05, 'epoch': 5.65}
{'loss': 1.313, 'grad_norm': 6.779261112213135, 'learning_rate': 6.400000000000001e-05, 'epoch': 6.02}
{'loss': 1.2486, 'grad_norm': 5.815522193908691, 'learning_rate': 6.800000000000001e-05, 'epoch': 6.4}
{'loss': 0.8108, 'grad_norm': 5.0620341300964355, 'learning_rate': 7.2e-05, 'epoch': 6.78}
{'loss': 0.6809, 'grad_norm': 5.000785827636719, 'learning_rate': 7.6e-05, 'epoch': 7.15}
{'loss': 0.6889, 'grad_norm': 4.490907669067383, 'learning_rate': 8e-05, 'epoch': 7.53}
{'train_runtime': 227.4695, 'train_samples_per_second': 3.737, 'train_steps_per_second': 0.088, 'train_loss': 3.163837027549744, 'epoch': 7.53}
[1;34mwandb[0m: üöÄ View run [33mmistral-7b-instruct-v0.3_ZsRE_85[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/2ttur0xa[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_002209-2ttur0xa/logs[0m
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:04<00:09,  4.95s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:09<00:04,  4.61s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.31s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.42s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_85/part_0/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_85/part_0/eval
0  to  50
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_85/part_0/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
550  to  600
generating!
600  to  650
generating!
650  to  700
generating!
700  to  750
generating!
750  to  800
generating!
800  to  850
generating!
850  to  900
generating!
900  to  950
generating!
950  to  1000
generating!
1000  to  1050
generating!
1050  to  1100
generating!
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_85/part_0/tokenizer.model
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_85/part_0/wandb
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_85/part_0/checkpoint-20
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_85/part_0/adapter_model.safetensors
Skipping directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_85/part_0/eval
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_85/part_0/adapter_config.json
Skipping file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_85/part_0/log.txt
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_85/part_0/special_tokens_map.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_85/part_0/tokenizer_config.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_85/part_0/tmp_data.jsonl
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_85/part_0/README.md
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=0, data_source=ZsRE, data_size=90, random=False
Loading data from ../../data/edit_data/merged_data_part_0.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 90 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_90/part_0/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/mistral-7b-instruct-v0.3
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_90/part_0
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: mistral-7b-instruct-v0.3_ZsRE_90
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:03<00:07,  3.62s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:06<00:03,  3.28s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:09<00:00,  3.04s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:09<00:00,  3.14s/it]
Using custom data configuration default-05b60b524589e664
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-05b60b524589e664/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2549.73it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 306.18it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-05b60b524589e664/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 289.92it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f77e160eee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 28,311,552 || all params: 7,276,335,104 || trainable%: 0.3890908210705739
0ex [00:00, ?ex/s]28ex [00:00, 278.17ex/s]90ex [00:00, 600.04ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_003202-04xsunub
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mistral-7b-instruct-v0.3_ZsRE_90
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/04xsunub

Example:
<s> The mother of Alexander Aris is whom? Irving Kane Pond</s>


Example:
<s> What studio produced When China Met Africa? Famous Players Television</s>


Example:
<s> Which industry is Noticias ECO associated with? publishing</s>


Example:
<s> What voice type is Louise Grandjean? mezzo soprano</s>


Example:
<s> When did Battle of the Java Sea occur? 27 February 1942</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 90
})
  0%|          | 0/20 [00:00<?, ?it/s]  5%|‚ñå         | 1/20 [00:11<03:45, 11.89s/it]                                                5%|‚ñå         | 1/20 [00:11<03:45, 11.89s/it] 10%|‚ñà         | 2/20 [00:22<03:24, 11.39s/it]                                               10%|‚ñà         | 2/20 [00:22<03:24, 11.39s/it] 15%|‚ñà‚ñå        | 3/20 [00:33<03:11, 11.24s/it]                                               15%|‚ñà‚ñå        | 3/20 [00:33<03:11, 11.24s/it] 20%|‚ñà‚ñà        | 4/20 [00:45<02:58, 11.17s/it]                                               20%|‚ñà‚ñà        | 4/20 [00:45<02:58, 11.17s/it] 25%|‚ñà‚ñà‚ñå       | 5/20 [00:56<02:46, 11.12s/it]                                               25%|‚ñà‚ñà‚ñå       | 5/20 [00:56<02:46, 11.12s/it] 30%|‚ñà‚ñà‚ñà       | 6/20 [01:07<02:35, 11.11s/it]                                               30%|‚ñà‚ñà‚ñà       | 6/20 [01:07<02:35, 11.11s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:18<02:25, 11.19s/it]                                               35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:18<02:25, 11.19s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:29<02:13, 11.16s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:29<02:13, 11.16s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:40<02:02, 11.13s/it]                                               45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:40<02:02, 11.13s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:51<01:51, 11.10s/it]                                                50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:51<01:51, 11.10s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [02:02<01:39, 11.09s/it]                                                55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [02:02<01:39, 11.09s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:13<01:28, 11.07s/it]                                                60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:13<01:28, 11.07s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:24<01:17, 11.07s/it]                                                65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:24<01:17, 11.07s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:35<01:06, 11.07s/it]                                                70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:35<01:06, 11.07s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:47<00:55, 11.07s/it]                                                75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:47<00:55, 11.07s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:58<00:44, 11.07s/it]                                                80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:58<00:44, 11.07s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [03:09<00:33, 11.07s/it]                                                85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [03:09<00:33, 11.07s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:20<00:22, 11.06s/it]                                                90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:20<00:22, 11.06s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:31<00:11, 11.07s/it]                                                95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:31<00:11, 11.07s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:42<00:00, 11.06s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:42<00:00, 11.06s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:45<00:00, 11.06s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:45<00:00, 11.29s/it]
{'loss': 6.6292, 'grad_norm': 23.169260025024414, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.36}
{'loss': 6.4546, 'grad_norm': 20.153188705444336, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.71}
{'loss': 5.6364, 'grad_norm': 17.32210350036621, 'learning_rate': 1.2e-05, 'epoch': 1.07}
{'loss': 5.6066, 'grad_norm': 20.591228485107422, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.42}
{'loss': 5.4815, 'grad_norm': 21.376710891723633, 'learning_rate': 2e-05, 'epoch': 1.78}
{'loss': 5.1686, 'grad_norm': 21.38332176208496, 'learning_rate': 2.4e-05, 'epoch': 2.13}
{'loss': 5.0554, 'grad_norm': 21.369173049926758, 'learning_rate': 2.8000000000000003e-05, 'epoch': 2.49}
{'loss': 3.2515, 'grad_norm': 19.35718536376953, 'learning_rate': 3.2000000000000005e-05, 'epoch': 2.84}
{'loss': 3.5556, 'grad_norm': 16.574073791503906, 'learning_rate': 3.6e-05, 'epoch': 3.2}
{'loss': 2.8354, 'grad_norm': 12.84864616394043, 'learning_rate': 4e-05, 'epoch': 3.56}
{'loss': 2.6727, 'grad_norm': 11.501476287841797, 'learning_rate': 4.4000000000000006e-05, 'epoch': 3.91}
{'loss': 2.2907, 'grad_norm': 10.670418739318848, 'learning_rate': 4.8e-05, 'epoch': 4.27}
{'loss': 1.7587, 'grad_norm': 7.011711597442627, 'learning_rate': 5.2000000000000004e-05, 'epoch': 4.62}
{'loss': 1.5717, 'grad_norm': 8.244410514831543, 'learning_rate': 5.6000000000000006e-05, 'epoch': 4.98}
{'loss': 1.431, 'grad_norm': 8.248255729675293, 'learning_rate': 6e-05, 'epoch': 5.33}
{'loss': 1.2654, 'grad_norm': 7.846704483032227, 'learning_rate': 6.400000000000001e-05, 'epoch': 5.69}
{'loss': 1.2102, 'grad_norm': 5.977906703948975, 'learning_rate': 6.800000000000001e-05, 'epoch': 6.04}
{'loss': 0.8935, 'grad_norm': 5.220397472381592, 'learning_rate': 7.2e-05, 'epoch': 6.4}
{'loss': 1.057, 'grad_norm': 5.171484470367432, 'learning_rate': 7.6e-05, 'epoch': 6.76}
{'loss': 0.5935, 'grad_norm': 5.582789897918701, 'learning_rate': 8e-05, 'epoch': 7.11}
{'train_runtime': 228.8444, 'train_samples_per_second': 3.933, 'train_steps_per_second': 0.087, 'train_loss': 3.220969721674919, 'epoch': 7.11}
[1;34mwandb[0m: üöÄ View run [33mmistral-7b-instruct-v0.3_ZsRE_90[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/04xsunub[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_003202-04xsunub/logs[0m
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:05<00:11,  5.82s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:10<00:04,  4.98s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:14<00:00,  4.49s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:14<00:00,  4.71s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_90/part_0/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_90/part_0/eval
0  to  50
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_90/part_0/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
550  to  600
generating!
600  to  650
generating!
650  to  700
generating!
700  to  750
generating!
750  to  800
generating!
800  to  850
generating!
850  to  900
generating!
900  to  950
generating!
950  to  1000
generating!
1000  to  1050
generating!
1050  to  1100
generating!
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_90/part_0/tokenizer_config.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_90/part_0/special_tokens_map.json
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_90/part_0/checkpoint-20
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_90/part_0/wandb
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_90/part_0/adapter_config.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_90/part_0/tokenizer.model
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_90/part_0/tmp_data.jsonl
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_90/part_0/adapter_model.safetensors
Skipping file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_90/part_0/log.txt
Skipping directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_90/part_0/eval
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_90/part_0/README.md
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=0, data_source=ZsRE, data_size=95, random=False
Loading data from ../../data/edit_data/merged_data_part_0.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 95 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_95/part_0/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/mistral-7b-instruct-v0.3
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_95/part_0
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: mistral-7b-instruct-v0.3_ZsRE_95
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:03<00:06,  3.13s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:06<00:03,  3.08s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:08<00:00,  2.93s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:08<00:00,  2.98s/it]
Using custom data configuration default-98a3726c15e93af9
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-98a3726c15e93af9/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2528.21it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 332.06it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-98a3726c15e93af9/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 292.00it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7fa1944ffee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 28,311,552 || all params: 7,276,335,104 || trainable%: 0.3890908210705739
0ex [00:00, ?ex/s]1ex [00:00,  6.69ex/s]95ex [00:00, 400.37ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_004213-uhclq98m
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mistral-7b-instruct-v0.3_ZsRE_95
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/uhclq98m

Example:
<s> Who was Arwen's mother? Doris</s>


Example:
<s> Who is listed as Leonor, Princess of Asturias father? Leonor III of Spain</s>


Example:
<s> Over which river does Dexter Coffin Bridge cross? Connecticut Creek</s>


Example:
<s> What was the launch date of USA-64? 3 December 1992</s>


Example:
<s> What type of tone does Gwendolyn Killebrew sing in? mezzo soprano</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 95
})
  0%|          | 0/20 [00:00<?, ?it/s]  5%|‚ñå         | 1/20 [00:11<03:40, 11.59s/it]                                                5%|‚ñå         | 1/20 [00:11<03:40, 11.59s/it] 10%|‚ñà         | 2/20 [00:22<03:22, 11.27s/it]                                               10%|‚ñà         | 2/20 [00:22<03:22, 11.27s/it] 15%|‚ñà‚ñå        | 3/20 [00:33<03:09, 11.17s/it]                                               15%|‚ñà‚ñå        | 3/20 [00:33<03:09, 11.17s/it] 20%|‚ñà‚ñà        | 4/20 [00:44<02:57, 11.12s/it]                                               20%|‚ñà‚ñà        | 4/20 [00:44<02:57, 11.12s/it] 25%|‚ñà‚ñà‚ñå       | 5/20 [00:55<02:46, 11.10s/it]                                               25%|‚ñà‚ñà‚ñå       | 5/20 [00:55<02:46, 11.10s/it] 30%|‚ñà‚ñà‚ñà       | 6/20 [01:06<02:35, 11.09s/it]                                               30%|‚ñà‚ñà‚ñà       | 6/20 [01:06<02:35, 11.09s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:17<02:24, 11.08s/it]                                               35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:17<02:24, 11.08s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:28<02:12, 11.07s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:28<02:12, 11.07s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:40<02:01, 11.07s/it]                                               45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:40<02:01, 11.07s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:51<01:50, 11.07s/it]                                                50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:51<01:50, 11.07s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [02:02<01:39, 11.06s/it]                                                55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [02:02<01:39, 11.06s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:13<01:28, 11.06s/it]                                                60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:13<01:28, 11.06s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:24<01:17, 11.06s/it]                                                65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:24<01:17, 11.06s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:35<01:06, 11.06s/it]                                                70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:35<01:06, 11.06s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:46<00:55, 11.06s/it]                                                75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:46<00:55, 11.06s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:57<00:44, 11.06s/it]                                                80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:57<00:44, 11.06s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [03:08<00:33, 11.06s/it]                                                85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [03:08<00:33, 11.06s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:19<00:22, 11.06s/it]                                                90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:19<00:22, 11.06s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:30<00:11, 11.06s/it]                                                95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:30<00:11, 11.06s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:41<00:00, 11.07s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:41<00:00, 11.07s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:45<00:00, 11.07s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:45<00:00, 11.26s/it]
{'loss': 5.8052, 'grad_norm': 17.747941970825195, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.34}
{'loss': 6.2374, 'grad_norm': 22.11433982849121, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.67}
{'loss': 6.2188, 'grad_norm': 22.04601287841797, 'learning_rate': 1.2e-05, 'epoch': 1.01}
{'loss': 5.66, 'grad_norm': 19.09061050415039, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.35}
{'loss': 5.3604, 'grad_norm': 21.766464233398438, 'learning_rate': 2e-05, 'epoch': 1.68}
{'loss': 4.928, 'grad_norm': 22.720556259155273, 'learning_rate': 2.4e-05, 'epoch': 2.02}
{'loss': 4.1737, 'grad_norm': 17.242273330688477, 'learning_rate': 2.8000000000000003e-05, 'epoch': 2.36}
{'loss': 4.2005, 'grad_norm': 21.914440155029297, 'learning_rate': 3.2000000000000005e-05, 'epoch': 2.69}
{'loss': 3.6275, 'grad_norm': 17.026798248291016, 'learning_rate': 3.6e-05, 'epoch': 3.03}
{'loss': 3.0103, 'grad_norm': 13.159483909606934, 'learning_rate': 4e-05, 'epoch': 3.37}
{'loss': 2.3585, 'grad_norm': 10.962032318115234, 'learning_rate': 4.4000000000000006e-05, 'epoch': 3.71}
{'loss': 2.1942, 'grad_norm': 9.136275291442871, 'learning_rate': 4.8e-05, 'epoch': 4.04}
{'loss': 1.8974, 'grad_norm': 7.849361896514893, 'learning_rate': 5.2000000000000004e-05, 'epoch': 4.38}
{'loss': 1.5454, 'grad_norm': 7.796218395233154, 'learning_rate': 5.6000000000000006e-05, 'epoch': 4.72}
{'loss': 1.5416, 'grad_norm': 8.49223804473877, 'learning_rate': 6e-05, 'epoch': 5.05}
{'loss': 1.3212, 'grad_norm': 8.453116416931152, 'learning_rate': 6.400000000000001e-05, 'epoch': 5.39}
{'loss': 1.2198, 'grad_norm': 7.137861251831055, 'learning_rate': 6.800000000000001e-05, 'epoch': 5.73}
{'loss': 0.8975, 'grad_norm': 5.76381778717041, 'learning_rate': 7.2e-05, 'epoch': 6.06}
{'loss': 0.7021, 'grad_norm': 4.919975280761719, 'learning_rate': 7.6e-05, 'epoch': 6.4}
{'loss': 0.9527, 'grad_norm': 5.5560479164123535, 'learning_rate': 8e-05, 'epoch': 6.74}
{'train_runtime': 227.5917, 'train_samples_per_second': 4.174, 'train_steps_per_second': 0.088, 'train_loss': 3.192616340517998, 'epoch': 6.74}
[1;34mwandb[0m: üöÄ View run [33mmistral-7b-instruct-v0.3_ZsRE_95[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/uhclq98m[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_004213-uhclq98m/logs[0m
mv: cannot stat './tmp_data.jsonl': No such file or directory
mv: cannot stat './wandb': No such file or directory
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:04<00:09,  4.90s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:09<00:04,  4.59s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.29s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.40s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_95/part_0/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_95/part_0/eval
0  to  50
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_95/part_0/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
550  to  600
generating!
600  to  650
generating!
650  to  700
generating!
700  to  750
generating!
750  to  800
generating!
800  to  850
generating!
850  to  900
generating!
900  to  950
generating!
950  to  1000
generating!
1000  to  1050
generating!
1050  to  1100
generating!
Skipping file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_95/part_0/log.txt
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_95/part_0/tokenizer_config.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_95/part_0/adapter_config.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_95/part_0/tokenizer.model
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_95/part_0/checkpoint-20
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_95/part_0/special_tokens_map.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_95/part_0/README.md
Skipping directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_95/part_0/eval
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_95/part_0/adapter_model.safetensors
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=1, data_source=ZsRE, data_size=1, random=False
Loading data from ../../data/edit_data/merged_data_part_1.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 1 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_1/part_1/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/mistral-7b-instruct-v0.3
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_1/part_1
batch_size: 1
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: mistral-7b-instruct-v0.3_ZsRE_1
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:03<00:06,  3.14s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:06<00:03,  3.10s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:08<00:00,  2.94s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:08<00:00,  2.98s/it]
Using custom data configuration default-0c10e6188437ddde
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-0c10e6188437ddde/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2659.67it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 325.62it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-0c10e6188437ddde/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 272.06it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f4a5843fee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 28,311,552 || all params: 7,276,335,104 || trainable%: 0.3890908210705739
0ex [00:00, ?ex/s]1ex [00:00,  6.63ex/s]1ex [00:00,  6.61ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_005205-p5jqbo1c
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mistral-7b-instruct-v0.3_ZsRE_1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/p5jqbo1c

Example:
<s> Who is Ismene's father? Tethys</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1
})
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:00<00:08,  1.08it/s]                                               10%|‚ñà         | 1/10 [00:00<00:08,  1.08it/s] 20%|‚ñà‚ñà        | 2/10 [00:01<00:04,  1.71it/s]                                               20%|‚ñà‚ñà        | 2/10 [00:01<00:04,  1.71it/s] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:01<00:03,  2.10it/s]                                               30%|‚ñà‚ñà‚ñà       | 3/10 [00:01<00:03,  2.10it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:01<00:02,  2.35it/s]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:01<00:02,  2.35it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:02<00:01,  2.52it/s]                                               50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:02<00:01,  2.52it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:02<00:01,  2.63it/s]                                               60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:02<00:01,  2.63it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [00:03<00:01,  2.70it/s]                                               70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [00:03<00:01,  2.70it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [00:03<00:00,  2.75it/s]                                               80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [00:03<00:00,  2.75it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [00:03<00:00,  2.79it/s]                                               90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [00:03<00:00,  2.79it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:04<00:00,  2.81it/s]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:04<00:00,  2.81it/s]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:08<00:00,  2.81it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:08<00:00,  1.13it/s]
{'loss': 7.3526, 'grad_norm': 76.99530029296875, 'learning_rate': 4.000000000000001e-06, 'epoch': 1.0}
{'loss': 7.3526, 'grad_norm': 76.55139923095703, 'learning_rate': 8.000000000000001e-06, 'epoch': 2.0}
{'loss': 6.8056, 'grad_norm': 70.58655548095703, 'learning_rate': 1.2e-05, 'epoch': 3.0}
{'loss': 5.7782, 'grad_norm': 74.72188568115234, 'learning_rate': 1.6000000000000003e-05, 'epoch': 4.0}
{'loss': 4.1131, 'grad_norm': 89.09362030029297, 'learning_rate': 2e-05, 'epoch': 5.0}
{'loss': 2.7508, 'grad_norm': 82.09148406982422, 'learning_rate': 2.4e-05, 'epoch': 6.0}
{'loss': 1.8372, 'grad_norm': 85.7652816772461, 'learning_rate': 2.8000000000000003e-05, 'epoch': 7.0}
{'loss': 0.9317, 'grad_norm': 28.473384857177734, 'learning_rate': 3.2000000000000005e-05, 'epoch': 8.0}
{'loss': 0.4743, 'grad_norm': 21.37408447265625, 'learning_rate': 3.6e-05, 'epoch': 9.0}
{'loss': 0.1704, 'grad_norm': 11.451985359191895, 'learning_rate': 4e-05, 'epoch': 10.0}
{'train_runtime': 11.2556, 'train_samples_per_second': 0.888, 'train_steps_per_second': 0.888, 'train_loss': 3.7566419407725333, 'epoch': 10.0}
[1;34mwandb[0m: üöÄ View run [33mmistral-7b-instruct-v0.3_ZsRE_1[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/p5jqbo1c[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_005205-p5jqbo1c/logs[0m
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:04<00:09,  4.86s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:09<00:04,  4.57s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.27s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.38s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_1/part_1/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_1/part_1/eval
0  to  50
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_1/part_1/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
550  to  600
generating!
600  to  650
generating!
650  to  700
generating!
700  to  750
generating!
750  to  800
generating!
800  to  850
generating!
850  to  900
generating!
900  to  950
generating!
950  to  1000
generating!
1000  to  1050
generating!
1050  to  1100
generating!
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_1/part_1/tokenizer_config.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_1/part_1/special_tokens_map.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_1/part_1/tmp_data.jsonl
Skipping directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_1/part_1/eval
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_1/part_1/checkpoint-10
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_1/part_1/adapter_model.safetensors
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_1/part_1/adapter_config.json
Skipping file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_1/part_1/log.txt
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_1/part_1/tokenizer.model
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_1/part_1/README.md
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_1/part_1/wandb
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=1, data_source=ZsRE, data_size=5, random=False
Loading data from ../../data/edit_data/merged_data_part_1.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 5 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_5/part_1/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/mistral-7b-instruct-v0.3
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_5/part_1
batch_size: 5
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: mistral-7b-instruct-v0.3_ZsRE_5
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:03<00:07,  3.79s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:06<00:03,  3.36s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:09<00:00,  3.09s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:09<00:00,  3.20s/it]
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Traceback (most recent call last):
  File "/home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/train_lora_wo_tmp.py", line 496, in <module>
    fire.Fire(train)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/fire/core.py", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/fire/core.py", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "/home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/train_lora_wo_tmp.py", line 373, in train
    data = load_dataset("json", data_files=data_path)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/load.py", line 1675, in load_dataset
    builder_instance = load_dataset_builder(
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/load.py", line 1512, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/load.py", line 1117, in dataset_module_factory
    return PackagedDatasetModuleFactory(
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/load.py", line 774, in get_module
    data_files = DataFilesDict.from_local_or_remote(patterns, use_auth_token=self.downnload_config.use_auth_token)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/data_files.py", line 578, in from_local_or_remote
    DataFilesList.from_local_or_remote(
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/data_files.py", line 546, in from_local_or_remote
    data_files = resolve_patterns_locally_or_by_urls(base_path, patterns, allowed_extensions)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/data_files.py", line 196, in resolve_patterns_locally_or_by_urls
    for path in _resolve_single_pattern_locally(base_path, pattern, allowed_extensions):
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/data_files.py", line 146, in _resolve_single_pattern_locally
    raise FileNotFoundError(error_msg)
FileNotFoundError: Unable to find '/home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/./tmp_data.jsonl' at /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora
mv: cannot stat './tmp_data.jsonl': No such file or directory
mv: cannot stat './wandb': No such file or directory
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:04<00:09,  4.51s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:08<00:04,  4.43s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:12<00:00,  4.19s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:14<00:00,  4.67s/it]
We are now merging the lora adapter to base model
Traceback (most recent call last):
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/peft/config.py", line 184, in _get_peft_type
    config_file = hf_hub_download(
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py", line 101, in inner_f
    return f(*args, **kwargs)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_5/part_1'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/lora_eval.py", line 36, in <module>
    lora_model = PeftModel.from_pretrained(model, args.lora_path)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/peft/peft_model.py", line 324, in from_pretrained
    PeftConfig._get_peft_type(
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/peft/config.py", line 190, in _get_peft_type
    raise ValueError(f"Can't find '{CONFIG_NAME}' at '{model_id}'")
ValueError: Can't find 'adapter_config.json' at '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_5/part_1'
Skipping file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_5/part_1/log.txt
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=1, data_source=ZsRE, data_size=10, random=False
Loading data from ../../data/edit_data/merged_data_part_1.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 10 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_10/part_1/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/mistral-7b-instruct-v0.3
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_10/part_1
batch_size: 10
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: mistral-7b-instruct-v0.3_ZsRE_10
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:03<00:06,  3.14s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:06<00:03,  3.09s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:08<00:00,  2.94s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:08<00:00,  2.98s/it]
Using custom data configuration default-949d3c4467a789ba
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-949d3c4467a789ba/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2487.72it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 262.77it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-949d3c4467a789ba/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 266.86it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7fc9e0191ee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 28,311,552 || all params: 7,276,335,104 || trainable%: 0.3890908210705739
0ex [00:00, ?ex/s]10ex [00:00, 129.78ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_005913-pd3pdbkm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mistral-7b-instruct-v0.3_ZsRE_10
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/pd3pdbkm

Example:
<s> What country released Kink FM? South Africa</s>


Example:
<s> What is the fictional universe that has √âowyn? Babylon 5 universe</s>


Example:
<s> Who is Ismene's father? Tethys</s>


Example:
<s> What is the constellation that is made with NGC 4293? Virgo</s>


Example:
<s> The person that is the mother of August Coppola is who? Francesco Coppola</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 10
})
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:04<00:39,  4.37s/it]                                               10%|‚ñà         | 1/10 [00:04<00:39,  4.37s/it] 20%|‚ñà‚ñà        | 2/10 [00:07<00:30,  3.83s/it]                                               20%|‚ñà‚ñà        | 2/10 [00:07<00:30,  3.83s/it] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:11<00:25,  3.65s/it]                                               30%|‚ñà‚ñà‚ñà       | 3/10 [00:11<00:25,  3.65s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:14<00:21,  3.57s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:14<00:21,  3.57s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:18<00:17,  3.52s/it]                                               50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:18<00:17,  3.52s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:21<00:13,  3.50s/it]                                               60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:21<00:13,  3.50s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [00:25<00:10,  3.48s/it]                                               70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [00:25<00:10,  3.48s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [00:28<00:06,  3.47s/it]                                               80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [00:28<00:06,  3.47s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [00:31<00:03,  3.47s/it]                                               90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [00:31<00:03,  3.47s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:35<00:00,  3.47s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:35<00:00,  3.47s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:38<00:00,  3.47s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:38<00:00,  3.80s/it]
{'loss': 6.0182, 'grad_norm': 28.173763275146484, 'learning_rate': 4.000000000000001e-06, 'epoch': 1.0}
{'loss': 6.0182, 'grad_norm': 28.17743682861328, 'learning_rate': 8.000000000000001e-06, 'epoch': 2.0}
{'loss': 5.8049, 'grad_norm': 28.50437355041504, 'learning_rate': 1.2e-05, 'epoch': 3.0}
{'loss': 5.3918, 'grad_norm': 28.60162925720215, 'learning_rate': 1.6000000000000003e-05, 'epoch': 4.0}
{'loss': 4.774, 'grad_norm': 29.18514060974121, 'learning_rate': 2e-05, 'epoch': 5.0}
{'loss': 4.106, 'grad_norm': 24.28209114074707, 'learning_rate': 2.4e-05, 'epoch': 6.0}
{'loss': 3.3622, 'grad_norm': 23.226469039916992, 'learning_rate': 2.8000000000000003e-05, 'epoch': 7.0}
{'loss': 2.6222, 'grad_norm': 26.027252197265625, 'learning_rate': 3.2000000000000005e-05, 'epoch': 8.0}
{'loss': 1.9975, 'grad_norm': 17.169082641601562, 'learning_rate': 3.6e-05, 'epoch': 9.0}
{'loss': 1.4356, 'grad_norm': 14.595849990844727, 'learning_rate': 4e-05, 'epoch': 10.0}
{'train_runtime': 40.2185, 'train_samples_per_second': 2.486, 'train_steps_per_second': 0.249, 'train_loss': 4.153057813644409, 'epoch': 10.0}
[1;34mwandb[0m: üöÄ View run [33mmistral-7b-instruct-v0.3_ZsRE_10[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/pd3pdbkm[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_005913-pd3pdbkm/logs[0m
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:04<00:09,  4.88s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:09<00:04,  4.58s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.28s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.39s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_10/part_1/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_10/part_1/eval
0  to  50
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_10/part_1/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
550  to  600
generating!
600  to  650
generating!
650  to  700
generating!
700  to  750
generating!
750  to  800
generating!
800  to  850
generating!
850  to  900
generating!
900  to  950
generating!
950  to  1000
generating!
1000  to  1050
generating!
1050  to  1100
generating!
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_10/part_1/tokenizer_config.json
Skipping file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_10/part_1/log.txt
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_10/part_1/checkpoint-10
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_10/part_1/adapter_model.safetensors
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_10/part_1/adapter_config.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_10/part_1/tokenizer.model
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_10/part_1/tmp_data.jsonl
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_10/part_1/special_tokens_map.json
Skipping directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_10/part_1/eval
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_10/part_1/README.md
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_10/part_1/wandb
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=1, data_source=ZsRE, data_size=15, random=False
Loading data from ../../data/edit_data/merged_data_part_1.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 15 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_15/part_1/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/mistral-7b-instruct-v0.3
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_15/part_1
batch_size: 15
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: mistral-7b-instruct-v0.3_ZsRE_15
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:03<00:07,  3.81s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:06<00:03,  3.36s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:09<00:00,  3.09s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:09<00:00,  3.21s/it]
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Traceback (most recent call last):
  File "/home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/train_lora_wo_tmp.py", line 496, in <module>
    fire.Fire(train)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/fire/core.py", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/fire/core.py", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "/home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/train_lora_wo_tmp.py", line 373, in train
    data = load_dataset("json", data_files=data_path)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/load.py", line 1675, in load_dataset
    builder_instance = load_dataset_builder(
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/load.py", line 1512, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/load.py", line 1117, in dataset_module_factory
    return PackagedDatasetModuleFactory(
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/load.py", line 774, in get_module
    data_files = DataFilesDict.from_local_or_remote(patterns, use_auth_token=self.downnload_config.use_auth_token)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/data_files.py", line 578, in from_local_or_remote
    DataFilesList.from_local_or_remote(
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/data_files.py", line 546, in from_local_or_remote
    data_files = resolve_patterns_locally_or_by_urls(base_path, patterns, allowed_extensions)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/data_files.py", line 196, in resolve_patterns_locally_or_by_urls
    for path in _resolve_single_pattern_locally(base_path, pattern, allowed_extensions):
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/data_files.py", line 146, in _resolve_single_pattern_locally
    raise FileNotFoundError(error_msg)
FileNotFoundError: Unable to find '/home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/./tmp_data.jsonl' at /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora
mv: cannot stat './tmp_data.jsonl': No such file or directory
mv: cannot stat './wandb': No such file or directory
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:04<00:08,  4.50s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:08<00:04,  4.42s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:12<00:00,  4.19s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:12<00:00,  4.26s/it]
We are now merging the lora adapter to base model
Traceback (most recent call last):
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/peft/config.py", line 184, in _get_peft_type
    config_file = hf_hub_download(
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py", line 101, in inner_f
    return f(*args, **kwargs)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_15/part_1'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/lora_eval.py", line 36, in <module>
    lora_model = PeftModel.from_pretrained(model, args.lora_path)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/peft/peft_model.py", line 324, in from_pretrained
    PeftConfig._get_peft_type(
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/peft/config.py", line 190, in _get_peft_type
    raise ValueError(f"Can't find '{CONFIG_NAME}' at '{model_id}'")
ValueError: Can't find 'adapter_config.json' at '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_15/part_1'
Skipping file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_15/part_1/log.txt
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=1, data_source=ZsRE, data_size=20, random=False
Loading data from ../../data/edit_data/merged_data_part_1.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 20 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_20/part_1/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/mistral-7b-instruct-v0.3
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_20/part_1
batch_size: 20
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: mistral-7b-instruct-v0.3_ZsRE_20
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:03<00:06,  3.13s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:06<00:03,  3.09s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:08<00:00,  2.94s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:08<00:00,  2.98s/it]
Using custom data configuration default-533222ffc259ebc8
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-533222ffc259ebc8/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2775.85it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 314.16it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-533222ffc259ebc8/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 291.47it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7fd1e052bee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 28,311,552 || all params: 7,276,335,104 || trainable%: 0.3890908210705739
0ex [00:00, ?ex/s]1ex [00:00,  6.45ex/s]20ex [00:00, 111.99ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_010656-hft4lrrf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mistral-7b-instruct-v0.3_ZsRE_20
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/hft4lrrf

Example:
<s> The appearance of Olivia Johnson is seen in what work? Oz</s>


Example:
<s> In which fictional work is Steven Hyde a character? Emmerdale</s>


Example:
<s> Who is Eteocles's father? Danehill</s>


Example:
<s> What is the constellation that is made with NGC 4293? Virgo</s>


Example:
<s> What city did Abel Seyler live when he died? Tirana</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 20
})
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:07<01:10,  7.79s/it]                                               10%|‚ñà         | 1/10 [00:07<01:10,  7.79s/it] 20%|‚ñà‚ñà        | 2/10 [00:14<00:57,  7.24s/it]                                               20%|‚ñà‚ñà        | 2/10 [00:14<00:57,  7.24s/it] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:21<00:49,  7.08s/it]                                               30%|‚ñà‚ñà‚ñà       | 3/10 [00:21<00:49,  7.08s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:28<00:42,  7.00s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:28<00:42,  7.00s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:35<00:34,  6.96s/it]                                               50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:35<00:34,  6.96s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:42<00:27,  6.94s/it]                                               60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:42<00:27,  6.94s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [00:49<00:20,  6.93s/it]                                               70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [00:49<00:20,  6.93s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [00:56<00:13,  6.92s/it]                                               80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [00:56<00:13,  6.92s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:02<00:06,  6.91s/it]                                               90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:02<00:06,  6.91s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:09<00:00,  6.91s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:09<00:00,  6.91s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:13<00:00,  6.91s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:13<00:00,  7.34s/it]
{'loss': 6.6982, 'grad_norm': 21.95716094970703, 'learning_rate': 4.000000000000001e-06, 'epoch': 1.0}
{'loss': 6.6982, 'grad_norm': 22.63489532470703, 'learning_rate': 8.000000000000001e-06, 'epoch': 2.0}
{'loss': 6.5346, 'grad_norm': 21.71702003479004, 'learning_rate': 1.2e-05, 'epoch': 3.0}
{'loss': 6.2121, 'grad_norm': 22.295499801635742, 'learning_rate': 1.6000000000000003e-05, 'epoch': 4.0}
{'loss': 5.727, 'grad_norm': 23.896116256713867, 'learning_rate': 2e-05, 'epoch': 5.0}
{'loss': 5.1081, 'grad_norm': 21.138874053955078, 'learning_rate': 2.4e-05, 'epoch': 6.0}
{'loss': 4.4249, 'grad_norm': 20.03999900817871, 'learning_rate': 2.8000000000000003e-05, 'epoch': 7.0}
{'loss': 3.6803, 'grad_norm': 19.0915470123291, 'learning_rate': 3.2000000000000005e-05, 'epoch': 8.0}
{'loss': 2.9503, 'grad_norm': 15.663220405578613, 'learning_rate': 3.6e-05, 'epoch': 9.0}
{'loss': 2.3078, 'grad_norm': 13.612505912780762, 'learning_rate': 4e-05, 'epoch': 10.0}
{'train_runtime': 76.6178, 'train_samples_per_second': 2.61, 'train_steps_per_second': 0.131, 'train_loss': 5.034154748916626, 'epoch': 10.0}
[1;34mwandb[0m: üöÄ View run [33mmistral-7b-instruct-v0.3_ZsRE_20[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/hft4lrrf[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_010656-hft4lrrf/logs[0m
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:05<00:10,  5.03s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:09<00:04,  4.66s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.31s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.44s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_20/part_1/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_20/part_1/eval
0  to  50
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_20/part_1/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
550  to  600
generating!
600  to  650
generating!
650  to  700
generating!
700  to  750
generating!
750  to  800
generating!
800  to  850
generating!
850  to  900
generating!
900  to  950
generating!
950  to  1000
generating!
1000  to  1050
generating!
1050  to  1100
generating!
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_20/part_1/tokenizer_config.json
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_20/part_1/checkpoint-10
Skipping directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_20/part_1/eval
Skipping file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_20/part_1/log.txt
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_20/part_1/adapter_model.safetensors
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_20/part_1/tmp_data.jsonl
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_20/part_1/wandb
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_20/part_1/README.md
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_20/part_1/adapter_config.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_20/part_1/tokenizer.model
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_20/part_1/special_tokens_map.json
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=1, data_source=ZsRE, data_size=25, random=False
Loading data from ../../data/edit_data/merged_data_part_1.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 25 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_25/part_1/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/mistral-7b-instruct-v0.3
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_25/part_1
batch_size: 25
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: mistral-7b-instruct-v0.3_ZsRE_25
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:03<00:07,  3.95s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:07<00:03,  3.42s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:09<00:00,  3.12s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:09<00:00,  3.25s/it]
Using custom data configuration default-3359fbfd5d2d6894
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-3359fbfd5d2d6894/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2457.12it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 297.34it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-3359fbfd5d2d6894/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 282.29it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f18a40e2ee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 28,311,552 || all params: 7,276,335,104 || trainable%: 0.3890908210705739
0ex [00:00, ?ex/s]1ex [00:00,  5.68ex/s]25ex [00:00, 121.83ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_011439-pn6e87ry
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mistral-7b-instruct-v0.3_ZsRE_25
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/pn6e87ry

Example:
<s> The appearance of Olivia Johnson is seen in what work? Oz</s>


Example:
<s> What sports team was Veljko Simiƒá a member of? FK Senica</s>


Example:
<s> What architect designed Verdala Palace? Manfred Trenz</s>


Example:
<s> Who was William Boleyn's father? Henry Boleyn</s>


Example:
<s> In which fictional work is Steven Hyde a character? Emmerdale</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 25
})
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:09<01:25,  9.49s/it]                                               10%|‚ñà         | 1/10 [00:09<01:25,  9.49s/it] 20%|‚ñà‚ñà        | 2/10 [00:18<01:11,  8.95s/it]                                               20%|‚ñà‚ñà        | 2/10 [00:18<01:11,  8.95s/it] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:26<01:01,  8.78s/it]                                               30%|‚ñà‚ñà‚ñà       | 3/10 [00:26<01:01,  8.78s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:35<00:52,  8.71s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:35<00:52,  8.71s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:43<00:43,  8.67s/it]                                               50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:43<00:43,  8.67s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:52<00:34,  8.65s/it]                                               60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:52<00:34,  8.65s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:01<00:25,  8.63s/it]                                               70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:01<00:25,  8.63s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:09<00:17,  8.62s/it]                                               80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:09<00:17,  8.62s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:18<00:08,  8.62s/it]                                               90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:18<00:08,  8.62s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:26<00:00,  8.61s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:26<00:00,  8.61s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:30<00:00,  8.61s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:30<00:00,  9.08s/it]
{'loss': 6.5243, 'grad_norm': 19.392356872558594, 'learning_rate': 4.000000000000001e-06, 'epoch': 1.0}
{'loss': 6.5243, 'grad_norm': 19.923873901367188, 'learning_rate': 8.000000000000001e-06, 'epoch': 2.0}
{'loss': 6.3764, 'grad_norm': 20.05703353881836, 'learning_rate': 1.2e-05, 'epoch': 3.0}
{'loss': 6.0841, 'grad_norm': 20.08005142211914, 'learning_rate': 1.6000000000000003e-05, 'epoch': 4.0}
{'loss': 5.6512, 'grad_norm': 19.974519729614258, 'learning_rate': 2e-05, 'epoch': 5.0}
{'loss': 5.0544, 'grad_norm': 20.922597885131836, 'learning_rate': 2.4e-05, 'epoch': 6.0}
{'loss': 4.413, 'grad_norm': 20.628416061401367, 'learning_rate': 2.8000000000000003e-05, 'epoch': 7.0}
{'loss': 3.6925, 'grad_norm': 19.669475555419922, 'learning_rate': 3.2000000000000005e-05, 'epoch': 8.0}
{'loss': 2.9703, 'grad_norm': 15.94257640838623, 'learning_rate': 3.6e-05, 'epoch': 9.0}
{'loss': 2.3191, 'grad_norm': 13.388514518737793, 'learning_rate': 4e-05, 'epoch': 10.0}
{'train_runtime': 93.6153, 'train_samples_per_second': 2.671, 'train_steps_per_second': 0.107, 'train_loss': 4.960970544815064, 'epoch': 10.0}
[1;34mwandb[0m: üöÄ View run [33mmistral-7b-instruct-v0.3_ZsRE_25[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/pn6e87ry[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_011439-pn6e87ry/logs[0m
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:04<00:09,  4.80s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:09<00:04,  4.55s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.25s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.36s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_25/part_1/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_25/part_1/eval
0  to  50
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_25/part_1/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
550  to  600
generating!
600  to  650
generating!
650  to  700
generating!
700  to  750
generating!
750  to  800
generating!
800  to  850
generating!
850  to  900
generating!
900  to  950
generating!
950  to  1000
generating!
1000  to  1050
generating!
1050  to  1100
generating!
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_25/part_1/README.md
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_25/part_1/tokenizer_config.json
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_25/part_1/checkpoint-10
Skipping file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_25/part_1/log.txt
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_25/part_1/adapter_model.safetensors
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_25/part_1/tokenizer.model
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_25/part_1/wandb
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_25/part_1/adapter_config.json
Skipping directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_25/part_1/eval
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_25/part_1/tmp_data.jsonl
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_25/part_1/special_tokens_map.json
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=1, data_source=ZsRE, data_size=30, random=False
Loading data from ../../data/edit_data/merged_data_part_1.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 30 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_30/part_1/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/mistral-7b-instruct-v0.3
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_30/part_1
batch_size: 30
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: mistral-7b-instruct-v0.3_ZsRE_30
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:03<00:07,  3.83s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:06<00:03,  3.37s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:09<00:00,  3.09s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:09<00:00,  3.21s/it]
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Traceback (most recent call last):
  File "/home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/train_lora_wo_tmp.py", line 496, in <module>
    fire.Fire(train)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/fire/core.py", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/fire/core.py", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "/home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/train_lora_wo_tmp.py", line 373, in train
    data = load_dataset("json", data_files=data_path)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/load.py", line 1675, in load_dataset
    builder_instance = load_dataset_builder(
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/load.py", line 1512, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/load.py", line 1117, in dataset_module_factory
    return PackagedDatasetModuleFactory(
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/load.py", line 774, in get_module
    data_files = DataFilesDict.from_local_or_remote(patterns, use_auth_token=self.downnload_config.use_auth_token)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/data_files.py", line 578, in from_local_or_remote
    DataFilesList.from_local_or_remote(
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/data_files.py", line 546, in from_local_or_remote
    data_files = resolve_patterns_locally_or_by_urls(base_path, patterns, allowed_extensions)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/data_files.py", line 196, in resolve_patterns_locally_or_by_urls
    for path in _resolve_single_pattern_locally(base_path, pattern, allowed_extensions):
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/data_files.py", line 146, in _resolve_single_pattern_locally
    raise FileNotFoundError(error_msg)
FileNotFoundError: Unable to find '/home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/./tmp_data.jsonl' at /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora
mv: cannot stat './tmp_data.jsonl': No such file or directory
mv: cannot stat './wandb': No such file or directory
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:04<00:09,  4.53s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:13<00:06,  6.97s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:17<00:00,  5.58s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:17<00:00,  5.71s/it]
We are now merging the lora adapter to base model
Traceback (most recent call last):
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/peft/config.py", line 184, in _get_peft_type
    config_file = hf_hub_download(
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py", line 101, in inner_f
    return f(*args, **kwargs)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_30/part_1'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/lora_eval.py", line 36, in <module>
    lora_model = PeftModel.from_pretrained(model, args.lora_path)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/peft/peft_model.py", line 324, in from_pretrained
    PeftConfig._get_peft_type(
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/peft/config.py", line 190, in _get_peft_type
    raise ValueError(f"Can't find '{CONFIG_NAME}' at '{model_id}'")
ValueError: Can't find 'adapter_config.json' at '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_30/part_1'
Skipping file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_30/part_1/log.txt
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=1, data_source=ZsRE, data_size=35, random=False
Loading data from ../../data/edit_data/merged_data_part_1.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 35 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_35/part_1/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/mistral-7b-instruct-v0.3
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_35/part_1
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: mistral-7b-instruct-v0.3_ZsRE_35
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:03<00:06,  3.37s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:06<00:03,  3.18s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:09<00:00,  2.98s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:09<00:00,  3.06s/it]
Using custom data configuration default-597f533c9d7f8261
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-597f533c9d7f8261/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2636.27it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 316.12it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-597f533c9d7f8261/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 261.25it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7fd2e032cee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 28,311,552 || all params: 7,276,335,104 || trainable%: 0.3890908210705739
0ex [00:00, ?ex/s]1ex [00:00,  7.16ex/s]35ex [00:00, 191.40ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_012349-ve8v7vy4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mistral-7b-instruct-v0.3_ZsRE_35
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/ve8v7vy4

Example:
<s> When did Joe Van Holsbeeck occur? 1954</s>


Example:
<s> What sports team was Veljko Simiƒá a member of? FK Senica</s>


Example:
<s> What label was responsible for Into the Glorious? Columbia Records</s>


Example:
<s> What country released Kink FM? South Africa</s>


Example:
<s> When did the discovery or creation of Rutherfordium occur? 1 January Rutherford</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 35
})
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:11<01:46, 11.87s/it]                                               10%|‚ñà         | 1/10 [00:11<01:46, 11.87s/it] 20%|‚ñà‚ñà        | 2/10 [00:22<01:30, 11.33s/it]                                               20%|‚ñà‚ñà        | 2/10 [00:22<01:30, 11.33s/it] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:33<01:18, 11.19s/it]                                               30%|‚ñà‚ñà‚ñà       | 3/10 [00:33<01:18, 11.19s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:44<01:06, 11.13s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:44<01:06, 11.13s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:55<00:55, 11.10s/it]                                               50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:55<00:55, 11.10s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:06<00:44, 11.07s/it]                                               60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:06<00:44, 11.07s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:17<00:33, 11.06s/it]                                               70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:17<00:33, 11.06s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:29<00:22, 11.05s/it]                                               80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:29<00:22, 11.05s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:40<00:11, 11.05s/it]                                               90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:40<00:11, 11.05s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:51<00:00, 11.04s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:51<00:00, 11.04s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:56<00:00, 11.04s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:56<00:00, 11.69s/it]
{'loss': 6.1279, 'grad_norm': 17.536527633666992, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.91}
{'loss': 6.438, 'grad_norm': 18.473739624023438, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.83}
{'loss': 5.7384, 'grad_norm': 17.976303100585938, 'learning_rate': 1.2e-05, 'epoch': 2.74}
{'loss': 5.525, 'grad_norm': 16.768239974975586, 'learning_rate': 1.6000000000000003e-05, 'epoch': 3.66}
{'loss': 5.6791, 'grad_norm': 20.24580955505371, 'learning_rate': 2e-05, 'epoch': 4.57}
{'loss': 4.9316, 'grad_norm': 20.483755111694336, 'learning_rate': 2.4e-05, 'epoch': 5.49}
{'loss': 4.539, 'grad_norm': 20.447044372558594, 'learning_rate': 2.8000000000000003e-05, 'epoch': 6.4}
{'loss': 3.8504, 'grad_norm': 17.11211395263672, 'learning_rate': 3.2000000000000005e-05, 'epoch': 7.31}
{'loss': 3.1875, 'grad_norm': 15.92132568359375, 'learning_rate': 3.6e-05, 'epoch': 8.23}
{'loss': 2.6694, 'grad_norm': 10.883657455444336, 'learning_rate': 4e-05, 'epoch': 9.14}
{'train_runtime': 122.7586, 'train_samples_per_second': 2.851, 'train_steps_per_second': 0.081, 'train_loss': 4.868646121025085, 'epoch': 9.14}
[1;34mwandb[0m: üöÄ View run [33mmistral-7b-instruct-v0.3_ZsRE_35[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/ve8v7vy4[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_012349-ve8v7vy4/logs[0m
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:04<00:09,  4.96s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:09<00:04,  4.69s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.33s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.45s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_35/part_1/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_35/part_1/eval
0  to  50
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_35/part_1/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
550  to  600
generating!
600  to  650
generating!
650  to  700
generating!
700  to  750
generating!
750  to  800
generating!
800  to  850
generating!
850  to  900
generating!
900  to  950
generating!
950  to  1000
generating!
1000  to  1050
generating!
1050  to  1100
generating!
Skipping file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_35/part_1/log.txt
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_35/part_1/tmp_data.jsonl
Skipping directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_35/part_1/eval
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_35/part_1/wandb
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_35/part_1/special_tokens_map.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_35/part_1/README.md
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_35/part_1/tokenizer_config.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_35/part_1/tokenizer.model
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_35/part_1/adapter_config.json
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_35/part_1/checkpoint-10
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_35/part_1/adapter_model.safetensors
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=1, data_source=ZsRE, data_size=40, random=False
Loading data from ../../data/edit_data/merged_data_part_1.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 40 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_40/part_1/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/mistral-7b-instruct-v0.3
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_40/part_1
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: mistral-7b-instruct-v0.3_ZsRE_40
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:04<00:08,  4.25s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:07<00:03,  3.55s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:10<00:00,  3.18s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:10<00:00,  3.35s/it]
Using custom data configuration default-03812b348edee38e
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-03812b348edee38e/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2941.31it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 336.08it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-03812b348edee38e/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 283.80it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7fc94c15dee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 28,311,552 || all params: 7,276,335,104 || trainable%: 0.3890908210705739
0ex [00:00, ?ex/s]30ex [00:00, 297.78ex/s]40ex [00:00, 354.79ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_013248-ies1ssl2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mistral-7b-instruct-v0.3_ZsRE_40
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/ies1ssl2

Example:
<s> Which was the official year for the approval of JS 7.62? 1966</s>


Example:
<s> What is the constellation that is made with NGC 4293? Virgo</s>


Example:
<s> Who fathered Bo Guagua? Xuan Xun</s>


Example:
<s> What label was responsible for Silence Is Easy? Elektra Records</s>


Example:
<s> Which family does Dendrolobatus belong to? Carabidae</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 40
})
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:11<01:43, 11.52s/it]                                               10%|‚ñà         | 1/10 [00:11<01:43, 11.52s/it] 20%|‚ñà‚ñà        | 2/10 [00:22<01:29, 11.24s/it]                                               20%|‚ñà‚ñà        | 2/10 [00:22<01:29, 11.24s/it] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:33<01:17, 11.14s/it]                                               30%|‚ñà‚ñà‚ñà       | 3/10 [00:33<01:17, 11.14s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:44<01:06, 11.11s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:44<01:06, 11.11s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:55<00:55, 11.09s/it]                                               50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:55<00:55, 11.09s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:06<00:44, 11.08s/it]                                               60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:06<00:44, 11.08s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:17<00:33, 11.07s/it]                                               70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:17<00:33, 11.07s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:28<00:22, 11.07s/it]                                               80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:28<00:22, 11.07s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:39<00:11, 11.06s/it]                                               90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:39<00:11, 11.06s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:50<00:00, 11.06s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:50<00:00, 11.06s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:54<00:00, 11.06s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:54<00:00, 11.43s/it]
{'loss': 6.1653, 'grad_norm': 16.983875274658203, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.8}
{'loss': 5.9734, 'grad_norm': 19.41587257385254, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.6}
{'loss': 6.048, 'grad_norm': 21.251676559448242, 'learning_rate': 1.2e-05, 'epoch': 2.4}
{'loss': 5.9369, 'grad_norm': 17.280879974365234, 'learning_rate': 1.6000000000000003e-05, 'epoch': 3.2}
{'loss': 5.8324, 'grad_norm': 18.877809524536133, 'learning_rate': 2e-05, 'epoch': 4.0}
{'loss': 5.2673, 'grad_norm': 19.228450775146484, 'learning_rate': 2.4e-05, 'epoch': 4.8}
{'loss': 4.4258, 'grad_norm': 18.13591194152832, 'learning_rate': 2.8000000000000003e-05, 'epoch': 5.6}
{'loss': 4.1423, 'grad_norm': 16.40192985534668, 'learning_rate': 3.2000000000000005e-05, 'epoch': 6.4}
{'loss': 3.4334, 'grad_norm': 17.782867431640625, 'learning_rate': 3.6e-05, 'epoch': 7.2}
{'loss': 2.8805, 'grad_norm': 11.97446060180664, 'learning_rate': 4e-05, 'epoch': 8.0}
{'train_runtime': 117.2078, 'train_samples_per_second': 3.413, 'train_steps_per_second': 0.085, 'train_loss': 5.010530924797058, 'epoch': 8.0}
[1;34mwandb[0m: üöÄ View run [33mmistral-7b-instruct-v0.3_ZsRE_40[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/ies1ssl2[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_013248-ies1ssl2/logs[0m
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:05<00:10,  5.04s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:09<00:04,  4.64s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.30s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.43s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_40/part_1/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_40/part_1/eval
0  to  50
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_40/part_1/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
550  to  600
generating!
600  to  650
generating!
650  to  700
generating!
700  to  750
generating!
750  to  800
generating!
800  to  850
generating!
850  to  900
generating!
900  to  950
generating!
950  to  1000
generating!
1000  to  1050
generating!
1050  to  1100
generating!
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_40/part_1/wandb
Skipping directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_40/part_1/eval
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_40/part_1/tokenizer_config.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_40/part_1/tokenizer.model
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_40/part_1/README.md
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_40/part_1/checkpoint-10
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_40/part_1/special_tokens_map.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_40/part_1/tmp_data.jsonl
Skipping file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_40/part_1/log.txt
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_40/part_1/adapter_config.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_40/part_1/adapter_model.safetensors
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=1, data_source=ZsRE, data_size=45, random=False
Loading data from ../../data/edit_data/merged_data_part_1.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 45 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_45/part_1/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/mistral-7b-instruct-v0.3
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_45/part_1
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: mistral-7b-instruct-v0.3_ZsRE_45
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:03<00:06,  3.14s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:06<00:03,  3.10s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:08<00:00,  2.94s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:08<00:00,  2.99s/it]
Using custom data configuration default-ebee71232d7262ea
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-ebee71232d7262ea/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2595.49it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 282.73it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-ebee71232d7262ea/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 255.24it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f2f5811fee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 28,311,552 || all params: 7,276,335,104 || trainable%: 0.3890908210705739
0ex [00:00, ?ex/s]1ex [00:00,  9.59ex/s]45ex [00:00, 287.05ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_014105-67v8283x
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mistral-7b-instruct-v0.3_ZsRE_45
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/67v8283x

Example:
<s> When was the discovery of 503 Evelyn? 17 503</s>


Example:
<s> What is Gaston de Gerlache's father's name? Charles de Gerlache, 2nd Earl of Leicester</s>


Example:
<s> The Strengleikar is based upon what? Erlangenbau</s>


Example:
<s> Which lady gave birth to Leto? Fausta</s>


Example:
<s> What is the position of Andrea Pangrazio? Doge of Venice</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 45
})
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:11<01:47, 11.91s/it]                                               10%|‚ñà         | 1/10 [00:11<01:47, 11.91s/it] 20%|‚ñà‚ñà        | 2/10 [00:22<01:31, 11.39s/it]                                               20%|‚ñà‚ñà        | 2/10 [00:22<01:31, 11.39s/it] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:33<01:18, 11.22s/it]                                               30%|‚ñà‚ñà‚ñà       | 3/10 [00:33<01:18, 11.22s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:44<01:06, 11.15s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:44<01:06, 11.15s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:56<00:55, 11.11s/it]                                               50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:56<00:55, 11.11s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:07<00:44, 11.09s/it]                                               60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:07<00:44, 11.09s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:18<00:33, 11.07s/it]                                               70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:18<00:33, 11.07s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:29<00:22, 11.06s/it]                                               80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:29<00:22, 11.06s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:40<00:11, 11.06s/it]                                               90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:40<00:11, 11.06s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:51<00:00, 11.05s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:51<00:00, 11.05s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:53<00:00, 11.05s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:53<00:00, 11.33s/it]
{'loss': 6.4475, 'grad_norm': 17.245006561279297, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.71}
{'loss': 5.9826, 'grad_norm': 20.45900535583496, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.42}
{'loss': 6.157, 'grad_norm': 18.976791381835938, 'learning_rate': 1.2e-05, 'epoch': 2.13}
{'loss': 5.7053, 'grad_norm': 17.337621688842773, 'learning_rate': 1.6000000000000003e-05, 'epoch': 2.84}
{'loss': 5.6928, 'grad_norm': 17.429031372070312, 'learning_rate': 2e-05, 'epoch': 3.56}
{'loss': 5.115, 'grad_norm': 20.446420669555664, 'learning_rate': 2.4e-05, 'epoch': 4.27}
{'loss': 4.7776, 'grad_norm': 18.449047088623047, 'learning_rate': 2.8000000000000003e-05, 'epoch': 4.98}
{'loss': 4.088, 'grad_norm': 16.53453826904297, 'learning_rate': 3.2000000000000005e-05, 'epoch': 5.69}
{'loss': 3.4451, 'grad_norm': 16.666580200195312, 'learning_rate': 3.6e-05, 'epoch': 6.4}
{'loss': 2.8947, 'grad_norm': 12.754203796386719, 'learning_rate': 4e-05, 'epoch': 7.11}
{'train_runtime': 115.7784, 'train_samples_per_second': 3.887, 'train_steps_per_second': 0.086, 'train_loss': 5.0305564403533936, 'epoch': 7.11}
[1;34mwandb[0m: üöÄ View run [33mmistral-7b-instruct-v0.3_ZsRE_45[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/67v8283x[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_014105-67v8283x/logs[0m
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:04<00:09,  4.89s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:09<00:04,  4.59s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.28s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.39s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_45/part_1/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_45/part_1/eval
0  to  50
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_45/part_1/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
550  to  600
generating!
600  to  650
generating!
650  to  700
generating!
700  to  750
generating!
750  to  800
generating!
800  to  850
generating!
850  to  900
generating!
900  to  950
generating!
950  to  1000
generating!
1000  to  1050
generating!
1050  to  1100
generating!
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_45/part_1/tokenizer.model
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_45/part_1/tmp_data.jsonl
Skipping file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_45/part_1/log.txt
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_45/part_1/tokenizer_config.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_45/part_1/special_tokens_map.json
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_45/part_1/wandb
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_45/part_1/adapter_config.json
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_45/part_1/checkpoint-10
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_45/part_1/adapter_model.safetensors
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_45/part_1/README.md
Skipping directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_45/part_1/eval
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=1, data_source=ZsRE, data_size=50, random=False
Loading data from ../../data/edit_data/merged_data_part_1.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 50 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_50/part_1/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/mistral-7b-instruct-v0.3
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_50/part_1
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: mistral-7b-instruct-v0.3_ZsRE_50
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:03<00:07,  3.97s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:07<00:03,  3.43s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:09<00:00,  3.12s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:09<00:00,  3.26s/it]
Using custom data configuration default-3ba5a6eb86e11c28
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-3ba5a6eb86e11c28/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2641.25it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 312.68it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-3ba5a6eb86e11c28/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 288.67it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7fb61034dee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 28,311,552 || all params: 7,276,335,104 || trainable%: 0.3890908210705739
0ex [00:00, ?ex/s]1ex [00:00,  8.45ex/s]50ex [00:00, 284.97ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_014919-l4vaowrz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mistral-7b-instruct-v0.3_ZsRE_50
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/l4vaowrz

Example:
<s> What country released Kink FM? South Africa</s>


Example:
<s> When did Joe Van Holsbeeck occur? 1954</s>


Example:
<s> Who was William Boleyn's father? Henry Boleyn</s>


Example:
<s> Who was the mother of John Bowes, 10th Earl of Strathmore and Kinghorne? Elizabeth Bowes, 9th Earl of Strathmore and Kinghorne</s>


Example:
<s> Who fathered Bo Guagua? Xuan Xun</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 50
})
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:11<01:47, 11.99s/it]                                               10%|‚ñà         | 1/10 [00:11<01:47, 11.99s/it] 20%|‚ñà‚ñà        | 2/10 [00:23<01:31, 11.43s/it]                                               20%|‚ñà‚ñà        | 2/10 [00:23<01:31, 11.43s/it] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:34<01:19, 11.30s/it]                                               30%|‚ñà‚ñà‚ñà       | 3/10 [00:34<01:19, 11.30s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:45<01:07, 11.24s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:45<01:07, 11.24s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:56<00:55, 11.18s/it]                                               50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:56<00:55, 11.18s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:07<00:44, 11.16s/it]                                               60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:07<00:44, 11.16s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:18<00:33, 11.16s/it]                                               70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:18<00:33, 11.16s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:29<00:22, 11.13s/it]                                               80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:29<00:22, 11.13s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:40<00:11, 11.13s/it]                                               90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:40<00:11, 11.13s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:51<00:00, 11.12s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:51<00:00, 11.12s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:54<00:00, 11.12s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:54<00:00, 11.41s/it]
{'loss': 5.652, 'grad_norm': 18.514850616455078, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.64}
{'loss': 6.6277, 'grad_norm': 19.340312957763672, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.28}
{'loss': 5.98, 'grad_norm': 18.10269546508789, 'learning_rate': 1.2e-05, 'epoch': 1.92}
{'loss': 5.413, 'grad_norm': 15.528079986572266, 'learning_rate': 1.6000000000000003e-05, 'epoch': 2.56}
{'loss': 5.9313, 'grad_norm': 19.389875411987305, 'learning_rate': 2e-05, 'epoch': 3.2}
{'loss': 5.1239, 'grad_norm': 18.916898727416992, 'learning_rate': 2.4e-05, 'epoch': 3.84}
{'loss': 4.4465, 'grad_norm': 17.493789672851562, 'learning_rate': 2.8000000000000003e-05, 'epoch': 4.48}
{'loss': 4.2596, 'grad_norm': 20.04778480529785, 'learning_rate': 3.2000000000000005e-05, 'epoch': 5.12}
{'loss': 3.227, 'grad_norm': 16.23832893371582, 'learning_rate': 3.6e-05, 'epoch': 5.76}
{'loss': 3.1591, 'grad_norm': 12.697500228881836, 'learning_rate': 4e-05, 'epoch': 6.4}
{'train_runtime': 116.1621, 'train_samples_per_second': 4.304, 'train_steps_per_second': 0.086, 'train_loss': 4.982016611099243, 'epoch': 6.4}
[1;34mwandb[0m: üöÄ View run [33mmistral-7b-instruct-v0.3_ZsRE_50[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/l4vaowrz[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_014919-l4vaowrz/logs[0m
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:04<00:09,  4.97s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:09<00:04,  4.63s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.31s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.43s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_50/part_1/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_50/part_1/eval
0  to  50
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_50/part_1/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
550  to  600
generating!
600  to  650
generating!
650  to  700
generating!
700  to  750
generating!
750  to  800
generating!
800  to  850
generating!
850  to  900
generating!
900  to  950
generating!
950  to  1000
generating!
1000  to  1050
generating!
1050  to  1100
generating!
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_50/part_1/README.md
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_50/part_1/adapter_model.safetensors
Skipping file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_50/part_1/log.txt
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_50/part_1/adapter_config.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_50/part_1/tokenizer_config.json
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_50/part_1/checkpoint-10
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_50/part_1/wandb
Skipping directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_50/part_1/eval
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_50/part_1/tmp_data.jsonl
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_50/part_1/special_tokens_map.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_50/part_1/tokenizer.model
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=1, data_source=ZsRE, data_size=55, random=False
Loading data from ../../data/edit_data/merged_data_part_1.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 55 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_55/part_1/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/mistral-7b-instruct-v0.3
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_55/part_1
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: mistral-7b-instruct-v0.3_ZsRE_55
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:04<00:08,  4.33s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:07<00:03,  3.59s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:10<00:00,  3.21s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:10<00:00,  3.39s/it]
Using custom data configuration default-eba2f088bda9ab27
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-eba2f088bda9ab27/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2681.78it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 336.08it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-eba2f088bda9ab27/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 266.69it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f16685e5ee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 28,311,552 || all params: 7,276,335,104 || trainable%: 0.3890908210705739
0ex [00:00, ?ex/s]1ex [00:00,  7.26ex/s]55ex [00:00, 277.55ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_015736-mjixzgpd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mistral-7b-instruct-v0.3_ZsRE_55
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/mjixzgpd

Example:
<s> Who was the person who directed Bitter Apples? William Beaudine</s>


Example:
<s> Which series is Michael Scott Paper Company apart of? The Good Wife</s>


Example:
<s> What label was responsible for Silence Is Easy? Elektra Records</s>


Example:
<s> What label was responsible for Into the Glorious? Columbia Records</s>


Example:
<s> What is the constellation that is made with NGC 4293? Virgo</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 55
})
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:11<01:47, 11.99s/it]                                               10%|‚ñà         | 1/10 [00:11<01:47, 11.99s/it] 20%|‚ñà‚ñà        | 2/10 [00:22<01:31, 11.39s/it]                                               20%|‚ñà‚ñà        | 2/10 [00:22<01:31, 11.39s/it] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:33<01:18, 11.21s/it]                                               30%|‚ñà‚ñà‚ñà       | 3/10 [00:33<01:18, 11.21s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:45<01:07, 11.17s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:45<01:07, 11.17s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:56<00:55, 11.15s/it]                                               50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:56<00:55, 11.15s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:07<00:44, 11.10s/it]                                               60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:07<00:44, 11.10s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:18<00:33, 11.10s/it]                                               70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:18<00:33, 11.10s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:29<00:22, 11.08s/it]                                               80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:29<00:22, 11.08s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:40<00:11, 11.11s/it]                                               90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:40<00:11, 11.11s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:51<00:00, 11.09s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:51<00:00, 11.09s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:53<00:00, 11.09s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:53<00:00, 11.32s/it]
{'loss': 5.8076, 'grad_norm': 16.120393753051758, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.58}
{'loss': 6.8453, 'grad_norm': 22.197383880615234, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.16}
{'loss': 6.368, 'grad_norm': 18.647357940673828, 'learning_rate': 1.2e-05, 'epoch': 1.75}
{'loss': 5.0989, 'grad_norm': 14.641180038452148, 'learning_rate': 1.6000000000000003e-05, 'epoch': 2.33}
{'loss': 6.075, 'grad_norm': 19.873184204101562, 'learning_rate': 2e-05, 'epoch': 2.91}
{'loss': 5.7967, 'grad_norm': 21.128520965576172, 'learning_rate': 2.4e-05, 'epoch': 3.49}
{'loss': 4.4625, 'grad_norm': 21.400531768798828, 'learning_rate': 2.8000000000000003e-05, 'epoch': 4.07}
{'loss': 4.349, 'grad_norm': 17.830486297607422, 'learning_rate': 3.2000000000000005e-05, 'epoch': 4.65}
{'loss': 3.7779, 'grad_norm': 17.590787887573242, 'learning_rate': 3.6e-05, 'epoch': 5.24}
{'loss': 3.0351, 'grad_norm': 12.998133659362793, 'learning_rate': 4e-05, 'epoch': 5.82}
{'train_runtime': 115.4156, 'train_samples_per_second': 4.765, 'train_steps_per_second': 0.087, 'train_loss': 5.16161081790924, 'epoch': 5.82}
[1;34mwandb[0m: üöÄ View run [33mmistral-7b-instruct-v0.3_ZsRE_55[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/mjixzgpd[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_015736-mjixzgpd/logs[0m
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:04<00:09,  4.55s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:08<00:04,  4.45s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:12<00:00,  4.20s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:12<00:00,  4.28s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_55/part_1/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_55/part_1/eval
0  to  50
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_55/part_1/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
550  to  600
generating!
600  to  650
generating!
650  to  700
generating!
700  to  750
generating!
750  to  800
generating!
800  to  850
generating!
850  to  900
generating!
900  to  950
generating!
950  to  1000
generating!
1000  to  1050
generating!
1050  to  1100
generating!
Skipping file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_55/part_1/log.txt
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_55/part_1/tokenizer.model
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_55/part_1/adapter_config.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_55/part_1/tmp_data.jsonl
Skipping directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_55/part_1/eval
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_55/part_1/adapter_model.safetensors
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_55/part_1/checkpoint-10
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_55/part_1/special_tokens_map.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_55/part_1/tokenizer_config.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_55/part_1/README.md
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_55/part_1/wandb
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=1, data_source=ZsRE, data_size=60, random=False
Loading data from ../../data/edit_data/merged_data_part_1.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 60 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_60/part_1/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/mistral-7b-instruct-v0.3
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_60/part_1
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: mistral-7b-instruct-v0.3_ZsRE_60
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:03<00:06,  3.13s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:06<00:03,  3.09s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:08<00:00,  2.94s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:08<00:00,  2.98s/it]
Using custom data configuration default-3b37cb16cfa6a1ed
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-3b37cb16cfa6a1ed/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2571.61it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 313.03it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-3b37cb16cfa6a1ed/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 289.20it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7fbc902c0ee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 28,311,552 || all params: 7,276,335,104 || trainable%: 0.3890908210705739
0ex [00:00, ?ex/s]60ex [00:00, 603.91ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_020548-u1wz57ay
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mistral-7b-instruct-v0.3_ZsRE_60
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/u1wz57ay

Example:
<s> Who was the person who directed Bitter Apples? William Beaudine</s>


Example:
<s> What is the publisher of Smelly Old History? Harper</s>


Example:
<s> What label was responsible for Silence Is Easy? Elektra Records</s>


Example:
<s> What label was responsible for Into the Glorious? Columbia Records</s>


Example:
<s> What is the constellation that is made with NGC 4293? Virgo</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 60
})
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:11<01:44, 11.57s/it]                                               10%|‚ñà         | 1/10 [00:11<01:44, 11.57s/it] 20%|‚ñà‚ñà        | 2/10 [00:22<01:30, 11.33s/it]                                               20%|‚ñà‚ñà        | 2/10 [00:22<01:30, 11.33s/it] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:33<01:18, 11.27s/it]                                               30%|‚ñà‚ñà‚ñà       | 3/10 [00:33<01:18, 11.27s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:45<01:07, 11.20s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:45<01:07, 11.20s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:56<00:55, 11.18s/it]                                               50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:56<00:55, 11.18s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:07<00:44, 11.15s/it]                                               60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:07<00:44, 11.15s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:18<00:33, 11.13s/it]                                               70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:18<00:33, 11.13s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:29<00:22, 11.10s/it]                                               80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:29<00:22, 11.10s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:40<00:11, 11.10s/it]                                               90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:40<00:11, 11.10s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:51<00:00, 11.13s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:51<00:00, 11.13s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:54<00:00, 11.13s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:54<00:00, 11.41s/it]
{'loss': 6.7753, 'grad_norm': 18.796932220458984, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.53}
{'loss': 5.9573, 'grad_norm': 17.775928497314453, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.07}
{'loss': 6.4582, 'grad_norm': 19.013076782226562, 'learning_rate': 1.2e-05, 'epoch': 1.6}
{'loss': 6.1964, 'grad_norm': 17.736400604248047, 'learning_rate': 1.6000000000000003e-05, 'epoch': 2.13}
{'loss': 5.3364, 'grad_norm': 18.469402313232422, 'learning_rate': 2e-05, 'epoch': 2.67}
{'loss': 5.4591, 'grad_norm': 21.139820098876953, 'learning_rate': 2.4e-05, 'epoch': 3.2}
{'loss': 5.3348, 'grad_norm': 19.600482940673828, 'learning_rate': 2.8000000000000003e-05, 'epoch': 3.73}
{'loss': 4.0549, 'grad_norm': 19.043445587158203, 'learning_rate': 3.2000000000000005e-05, 'epoch': 4.27}
{'loss': 4.0068, 'grad_norm': 18.919063568115234, 'learning_rate': 3.6e-05, 'epoch': 4.8}
{'loss': 3.0299, 'grad_norm': 14.271357536315918, 'learning_rate': 4e-05, 'epoch': 5.33}
{'train_runtime': 116.0787, 'train_samples_per_second': 5.169, 'train_steps_per_second': 0.086, 'train_loss': 5.260901999473572, 'epoch': 5.33}
[1;34mwandb[0m: üöÄ View run [33mmistral-7b-instruct-v0.3_ZsRE_60[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/u1wz57ay[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_020548-u1wz57ay/logs[0m
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:05<00:10,  5.11s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:09<00:04,  4.68s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.33s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.47s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_60/part_1/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_60/part_1/eval
0  to  50
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_60/part_1/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
550  to  600
generating!
600  to  650
generating!
650  to  700
generating!
700  to  750
generating!
750  to  800
generating!
800  to  850
generating!
850  to  900
generating!
900  to  950
generating!
950  to  1000
generating!
1000  to  1050
generating!
1050  to  1100
generating!
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_60/part_1/tokenizer_config.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_60/part_1/tokenizer.model
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_60/part_1/adapter_model.safetensors
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_60/part_1/wandb
Skipping directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_60/part_1/eval
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_60/part_1/adapter_config.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_60/part_1/README.md
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_60/part_1/tmp_data.jsonl
Skipping file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_60/part_1/log.txt
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_60/part_1/checkpoint-10
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_60/part_1/special_tokens_map.json
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=1, data_source=ZsRE, data_size=65, random=False
Loading data from ../../data/edit_data/merged_data_part_1.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 65 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_65/part_1/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/mistral-7b-instruct-v0.3
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_65/part_1
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: mistral-7b-instruct-v0.3_ZsRE_65
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:03<00:06,  3.13s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:06<00:03,  3.09s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:08<00:00,  2.94s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:08<00:00,  2.98s/it]
Using custom data configuration default-8c0e5ac481085d6f
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-8c0e5ac481085d6f/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2522.13it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 291.11it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-8c0e5ac481085d6f/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 261.00it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f4af90a1ee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 28,311,552 || all params: 7,276,335,104 || trainable%: 0.3890908210705739
0ex [00:00, ?ex/s]40ex [00:00, 396.42ex/s]65ex [00:00, 534.61ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_021406-xqkk209m
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mistral-7b-instruct-v0.3_ZsRE_65
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/xqkk209m

Example:
<s> When was the discovery of 503 Evelyn? 17 503</s>


Example:
<s> What was the name of Artemis mother? Athena</s>


Example:
<s> What label was responsible for Silence Is Easy? Elektra Records</s>


Example:
<s> What label was responsible for Into the Glorious? Columbia Records</s>


Example:
<s> What is the constellation that is made with NGC 4293? Virgo</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 65
})
  0%|          | 0/20 [00:00<?, ?it/s]  5%|‚ñå         | 1/20 [00:11<03:40, 11.62s/it]                                                5%|‚ñå         | 1/20 [00:11<03:40, 11.62s/it] 10%|‚ñà         | 2/20 [00:22<03:22, 11.25s/it]                                               10%|‚ñà         | 2/20 [00:22<03:22, 11.25s/it] 15%|‚ñà‚ñå        | 3/20 [00:33<03:10, 11.23s/it]                                               15%|‚ñà‚ñå        | 3/20 [00:33<03:10, 11.23s/it] 20%|‚ñà‚ñà        | 4/20 [00:44<02:58, 11.18s/it]                                               20%|‚ñà‚ñà        | 4/20 [00:44<02:58, 11.18s/it] 25%|‚ñà‚ñà‚ñå       | 5/20 [00:56<02:47, 11.17s/it]                                               25%|‚ñà‚ñà‚ñå       | 5/20 [00:56<02:47, 11.17s/it] 30%|‚ñà‚ñà‚ñà       | 6/20 [01:07<02:36, 11.15s/it]                                               30%|‚ñà‚ñà‚ñà       | 6/20 [01:07<02:36, 11.15s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:18<02:24, 11.10s/it]                                               35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:18<02:24, 11.10s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:29<02:13, 11.14s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:29<02:13, 11.14s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:40<02:02, 11.10s/it]                                               45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:40<02:02, 11.10s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:51<01:51, 11.14s/it]                                                50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:51<01:51, 11.14s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [02:02<01:39, 11.11s/it]                                                55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [02:02<01:39, 11.11s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:13<01:28, 11.11s/it]                                                60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:13<01:28, 11.11s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:25<01:18, 11.17s/it]                                                65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:25<01:18, 11.17s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:36<01:06, 11.13s/it]                                                70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:36<01:06, 11.13s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:47<00:55, 11.13s/it]                                                75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:47<00:55, 11.13s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:58<00:44, 11.10s/it]                                                80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:58<00:44, 11.10s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [03:09<00:33, 11.11s/it]                                                85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [03:09<00:33, 11.11s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:20<00:22, 11.11s/it]                                                90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:20<00:22, 11.11s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:31<00:11, 11.14s/it]                                                95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:31<00:11, 11.14s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:42<00:00, 11.11s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:42<00:00, 11.11s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:45<00:00, 11.11s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:45<00:00, 11.25s/it]
{'loss': 6.6015, 'grad_norm': 17.81758689880371, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.49}
{'loss': 6.1618, 'grad_norm': 18.64164924621582, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.98}
{'loss': 6.0966, 'grad_norm': 17.930217742919922, 'learning_rate': 1.2e-05, 'epoch': 1.48}
{'loss': 6.1899, 'grad_norm': 18.406246185302734, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.97}
{'loss': 5.3283, 'grad_norm': 17.33280372619629, 'learning_rate': 2e-05, 'epoch': 2.46}
{'loss': 5.8803, 'grad_norm': 21.927536010742188, 'learning_rate': 2.4e-05, 'epoch': 2.95}
{'loss': 5.2659, 'grad_norm': 19.780654907226562, 'learning_rate': 2.8000000000000003e-05, 'epoch': 3.45}
{'loss': 4.1104, 'grad_norm': 19.2770938873291, 'learning_rate': 3.2000000000000005e-05, 'epoch': 3.94}
{'loss': 3.5663, 'grad_norm': 16.73967170715332, 'learning_rate': 3.6e-05, 'epoch': 4.43}
{'loss': 3.2559, 'grad_norm': 14.735369682312012, 'learning_rate': 4e-05, 'epoch': 4.92}
{'loss': 2.886, 'grad_norm': 12.045058250427246, 'learning_rate': 4.4000000000000006e-05, 'epoch': 5.42}
{'loss': 2.2521, 'grad_norm': 7.8062520027160645, 'learning_rate': 4.8e-05, 'epoch': 5.91}
{'loss': 1.5598, 'grad_norm': 7.380955219268799, 'learning_rate': 5.2000000000000004e-05, 'epoch': 6.4}
{'loss': 1.9895, 'grad_norm': 7.8525567054748535, 'learning_rate': 5.6000000000000006e-05, 'epoch': 6.89}
{'loss': 1.7311, 'grad_norm': 7.961642265319824, 'learning_rate': 6e-05, 'epoch': 7.38}
{'loss': 1.28, 'grad_norm': 7.618469715118408, 'learning_rate': 6.400000000000001e-05, 'epoch': 7.88}
{'loss': 1.1324, 'grad_norm': 6.867684841156006, 'learning_rate': 6.800000000000001e-05, 'epoch': 8.37}
{'loss': 0.884, 'grad_norm': 6.299886703491211, 'learning_rate': 7.2e-05, 'epoch': 8.86}
{'loss': 0.7923, 'grad_norm': 5.841252326965332, 'learning_rate': 7.6e-05, 'epoch': 9.35}
{'loss': 0.6596, 'grad_norm': 5.431088924407959, 'learning_rate': 8e-05, 'epoch': 9.85}
{'train_runtime': 227.1756, 'train_samples_per_second': 2.861, 'train_steps_per_second': 0.088, 'train_loss': 3.3811911970376967, 'epoch': 9.85}
[1;34mwandb[0m: üöÄ View run [33mmistral-7b-instruct-v0.3_ZsRE_65[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/xqkk209m[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_021406-xqkk209m/logs[0m
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:05<00:10,  5.06s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:09<00:04,  4.66s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.32s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.45s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_65/part_1/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_65/part_1/eval
0  to  50
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_65/part_1/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
550  to  600
generating!
600  to  650
generating!
650  to  700
generating!
700  to  750
generating!
750  to  800
generating!
800  to  850
generating!
850  to  900
generating!
900  to  950
generating!
950  to  1000
generating!
1000  to  1050
generating!
1050  to  1100
generating!
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_65/part_1/adapter_config.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_65/part_1/tokenizer_config.json
Skipping file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_65/part_1/log.txt
Skipping directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_65/part_1/eval
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_65/part_1/tmp_data.jsonl
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_65/part_1/wandb
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_65/part_1/special_tokens_map.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_65/part_1/adapter_model.safetensors
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_65/part_1/tokenizer.model
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_65/part_1/checkpoint-20
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_65/part_1/README.md
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=1, data_source=ZsRE, data_size=70, random=False
Loading data from ../../data/edit_data/merged_data_part_1.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 70 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_70/part_1/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/mistral-7b-instruct-v0.3
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_70/part_1
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: mistral-7b-instruct-v0.3_ZsRE_70
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:04<00:08,  4.16s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:07<00:03,  3.51s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:09<00:00,  3.17s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:09<00:00,  3.33s/it]
Using custom data configuration default-d8ea2c5f58ddb32a
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-d8ea2c5f58ddb32a/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2597.09it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 316.81it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-d8ea2c5f58ddb32a/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 280.63it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7fccd0066ee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 28,311,552 || all params: 7,276,335,104 || trainable%: 0.3890908210705739
0ex [00:00, ?ex/s]17ex [00:00, 168.76ex/s]70ex [00:00, 459.13ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_022406-9ybrle8v
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mistral-7b-instruct-v0.3_ZsRE_70
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/9ybrle8v

Example:
<s> Whom is Siding Spring Survey named after? Joseph Siding</s>


Example:
<s> What label was responsible for Into the Glorious? Columbia Records</s>


Example:
<s> Which was the official year for the approval of JS 7.62? 1966</s>


Example:
<s> Who is Sophie Ward's father? George Ward</s>


Example:
<s> Which family does Dendrolobatus belong to? Carabidae</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 70
})
  0%|          | 0/20 [00:00<?, ?it/s]  5%|‚ñå         | 1/20 [00:11<03:47, 12.00s/it]                                                5%|‚ñå         | 1/20 [00:12<03:47, 12.00s/it] 10%|‚ñà         | 2/20 [00:23<03:26, 11.45s/it]                                               10%|‚ñà         | 2/20 [00:23<03:26, 11.45s/it] 15%|‚ñà‚ñå        | 3/20 [00:34<03:11, 11.25s/it]                                               15%|‚ñà‚ñå        | 3/20 [00:34<03:11, 11.25s/it] 20%|‚ñà‚ñà        | 4/20 [00:45<02:59, 11.23s/it]                                               20%|‚ñà‚ñà        | 4/20 [00:45<02:59, 11.23s/it] 25%|‚ñà‚ñà‚ñå       | 5/20 [00:56<02:47, 11.18s/it]                                               25%|‚ñà‚ñà‚ñå       | 5/20 [00:56<02:47, 11.18s/it] 30%|‚ñà‚ñà‚ñà       | 6/20 [01:07<02:36, 11.16s/it]                                               30%|‚ñà‚ñà‚ñà       | 6/20 [01:07<02:36, 11.16s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:18<02:24, 11.14s/it]                                               35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:18<02:24, 11.14s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:29<02:13, 11.13s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:29<02:13, 11.13s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:40<02:02, 11.12s/it]                                               45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:40<02:02, 11.12s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:51<01:50, 11.09s/it]                                                50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:51<01:50, 11.09s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [02:02<01:39, 11.09s/it]                                                55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [02:02<01:39, 11.09s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:14<01:28, 11.12s/it]                                                60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:14<01:28, 11.12s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:25<01:17, 11.09s/it]                                                65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:25<01:17, 11.09s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:36<01:06, 11.10s/it]                                                70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:36<01:06, 11.10s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:47<00:55, 11.10s/it]                                                75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:47<00:55, 11.10s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:58<00:44, 11.08s/it]                                                80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:58<00:44, 11.08s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [03:09<00:33, 11.08s/it]                                                85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [03:09<00:33, 11.08s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:20<00:22, 11.12s/it]                                                90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:20<00:22, 11.12s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:31<00:11, 11.12s/it]                                                95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:31<00:11, 11.12s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:42<00:00, 11.09s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:42<00:00, 11.09s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:45<00:00, 11.09s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:45<00:00, 11.26s/it]
{'loss': 6.0789, 'grad_norm': 17.32417869567871, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.46}
{'loss': 6.5568, 'grad_norm': 18.868534088134766, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.91}
{'loss': 6.7179, 'grad_norm': 19.935550689697266, 'learning_rate': 1.2e-05, 'epoch': 1.37}
{'loss': 5.5622, 'grad_norm': 16.723169326782227, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.83}
{'loss': 5.9199, 'grad_norm': 19.209672927856445, 'learning_rate': 2e-05, 'epoch': 2.29}
{'loss': 5.7347, 'grad_norm': 19.993778228759766, 'learning_rate': 2.4e-05, 'epoch': 2.74}
{'loss': 4.7853, 'grad_norm': 21.162744522094727, 'learning_rate': 2.8000000000000003e-05, 'epoch': 3.2}
{'loss': 4.5527, 'grad_norm': 19.983057022094727, 'learning_rate': 3.2000000000000005e-05, 'epoch': 3.66}
{'loss': 3.6992, 'grad_norm': 18.231393814086914, 'learning_rate': 3.6e-05, 'epoch': 4.11}
{'loss': 3.4627, 'grad_norm': 16.513591766357422, 'learning_rate': 4e-05, 'epoch': 4.57}
{'loss': 2.8555, 'grad_norm': 11.640230178833008, 'learning_rate': 4.4000000000000006e-05, 'epoch': 5.03}
{'loss': 2.4102, 'grad_norm': 8.705740928649902, 'learning_rate': 4.8e-05, 'epoch': 5.49}
{'loss': 2.2634, 'grad_norm': 8.461387634277344, 'learning_rate': 5.2000000000000004e-05, 'epoch': 5.94}
{'loss': 1.8133, 'grad_norm': 7.71895170211792, 'learning_rate': 5.6000000000000006e-05, 'epoch': 6.4}
{'loss': 1.5764, 'grad_norm': 8.077329635620117, 'learning_rate': 6e-05, 'epoch': 6.86}
{'loss': 1.4638, 'grad_norm': 7.191035747528076, 'learning_rate': 6.400000000000001e-05, 'epoch': 7.31}
{'loss': 1.2312, 'grad_norm': 8.301777839660645, 'learning_rate': 6.800000000000001e-05, 'epoch': 7.77}
{'loss': 1.1367, 'grad_norm': 7.159387588500977, 'learning_rate': 7.2e-05, 'epoch': 8.23}
{'loss': 0.7671, 'grad_norm': 4.990972995758057, 'learning_rate': 7.6e-05, 'epoch': 8.69}
{'loss': 0.7532, 'grad_norm': 7.375429153442383, 'learning_rate': 8e-05, 'epoch': 9.14}
{'train_runtime': 227.4112, 'train_samples_per_second': 3.078, 'train_steps_per_second': 0.088, 'train_loss': 3.46704843044281, 'epoch': 9.14}
[1;34mwandb[0m: üöÄ View run [33mmistral-7b-instruct-v0.3_ZsRE_70[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/9ybrle8v[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_022406-9ybrle8v/logs[0m
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:05<00:10,  5.13s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:09<00:04,  4.69s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.34s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.47s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_70/part_1/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_70/part_1/eval
0  to  50
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_70/part_1/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
550  to  600
generating!
600  to  650
generating!
650  to  700
generating!
700  to  750
generating!
750  to  800
generating!
800  to  850
generating!
850  to  900
generating!
900  to  950
generating!
950  to  1000
generating!
1000  to  1050
generating!
1050  to  1100
generating!
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_70/part_1/adapter_config.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_70/part_1/special_tokens_map.json
Skipping file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_70/part_1/log.txt
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_70/part_1/wandb
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_70/part_1/checkpoint-20
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_70/part_1/tmp_data.jsonl
Skipping directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_70/part_1/eval
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_70/part_1/adapter_model.safetensors
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_70/part_1/tokenizer_config.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_70/part_1/README.md
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_70/part_1/tokenizer.model
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=1, data_source=ZsRE, data_size=75, random=False
Loading data from ../../data/edit_data/merged_data_part_1.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 75 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_75/part_1/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/mistral-7b-instruct-v0.3
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_75/part_1
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: mistral-7b-instruct-v0.3_ZsRE_75
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:04<00:08,  4.13s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:07<00:03,  3.50s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:09<00:00,  3.16s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:09<00:00,  3.32s/it]
Using custom data configuration default-5d49d917084f3e68
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-5d49d917084f3e68/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2668.13it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 329.64it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-5d49d917084f3e68/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 304.71it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7fb0a160eee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 28,311,552 || all params: 7,276,335,104 || trainable%: 0.3890908210705739
0ex [00:00, ?ex/s]49ex [00:00, 486.49ex/s]75ex [00:00, 624.55ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_023404-7inziwwf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mistral-7b-instruct-v0.3_ZsRE_75
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/7inziwwf

Example:
<s> What is the name of Automatic Midnight's record label? Myrrh Records</s>


Example:
<s> When was the discovery of 503 Evelyn? 17 503</s>


Example:
<s> What is Melor's father's name? Merengaria</s>


Example:
<s> What is the position of Andrea Pangrazio? Doge of Venice</s>


Example:
<s> Who designed the Borchardt C-93? Borchardt Firearms</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 75
})
  0%|          | 0/20 [00:00<?, ?it/s]  5%|‚ñå         | 1/20 [00:11<03:42, 11.74s/it]                                                5%|‚ñå         | 1/20 [00:11<03:42, 11.74s/it] 10%|‚ñà         | 2/20 [00:22<03:23, 11.32s/it]                                               10%|‚ñà         | 2/20 [00:22<03:23, 11.32s/it] 15%|‚ñà‚ñå        | 3/20 [00:33<03:10, 11.23s/it]                                               15%|‚ñà‚ñå        | 3/20 [00:33<03:10, 11.23s/it] 20%|‚ñà‚ñà        | 4/20 [00:45<02:59, 11.19s/it]                                               20%|‚ñà‚ñà        | 4/20 [00:45<02:59, 11.19s/it] 25%|‚ñà‚ñà‚ñå       | 5/20 [00:56<02:47, 11.18s/it]                                               25%|‚ñà‚ñà‚ñå       | 5/20 [00:56<02:47, 11.18s/it] 30%|‚ñà‚ñà‚ñà       | 6/20 [01:07<02:35, 11.14s/it]                                               30%|‚ñà‚ñà‚ñà       | 6/20 [01:07<02:35, 11.14s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:18<02:24, 11.14s/it]                                               35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:18<02:24, 11.14s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:29<02:14, 11.17s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:29<02:14, 11.17s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:40<02:02, 11.13s/it]                                               45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:40<02:02, 11.13s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:51<01:51, 11.11s/it]                                                50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:51<01:51, 11.11s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [02:02<01:40, 11.12s/it]                                                55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [02:02<01:40, 11.12s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:13<01:29, 11.13s/it]                                                60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:13<01:29, 11.13s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:25<01:17, 11.13s/it]                                                65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:25<01:17, 11.13s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:36<01:06, 11.14s/it]                                                70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:36<01:06, 11.14s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:47<00:55, 11.14s/it]                                                75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:47<00:55, 11.14s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:58<00:44, 11.11s/it]                                                80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:58<00:44, 11.11s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [03:09<00:33, 11.15s/it]                                                85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [03:09<00:33, 11.15s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:20<00:22, 11.12s/it]                                                90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:20<00:22, 11.12s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:31<00:11, 11.14s/it]                                                95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:31<00:11, 11.14s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:43<00:00, 11.14s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:43<00:00, 11.14s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:45<00:00, 11.14s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:45<00:00, 11.27s/it]
{'loss': 5.7675, 'grad_norm': 17.19223403930664, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.43}
{'loss': 6.4349, 'grad_norm': 19.287092208862305, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.85}
{'loss': 6.3747, 'grad_norm': 19.61986541748047, 'learning_rate': 1.2e-05, 'epoch': 1.28}
{'loss': 6.559, 'grad_norm': 17.62038230895996, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.71}
{'loss': 5.8654, 'grad_norm': 19.286109924316406, 'learning_rate': 2e-05, 'epoch': 2.13}
{'loss': 5.3205, 'grad_norm': 18.21956443786621, 'learning_rate': 2.4e-05, 'epoch': 2.56}
{'loss': 4.967, 'grad_norm': 19.804046630859375, 'learning_rate': 2.8000000000000003e-05, 'epoch': 2.99}
{'loss': 4.3323, 'grad_norm': 18.59792137145996, 'learning_rate': 3.2000000000000005e-05, 'epoch': 3.41}
{'loss': 3.9782, 'grad_norm': 21.144119262695312, 'learning_rate': 3.6e-05, 'epoch': 3.84}
{'loss': 3.1449, 'grad_norm': 12.337306022644043, 'learning_rate': 4e-05, 'epoch': 4.27}
{'loss': 2.7553, 'grad_norm': 11.636224746704102, 'learning_rate': 4.4000000000000006e-05, 'epoch': 4.69}
{'loss': 2.3638, 'grad_norm': 9.744205474853516, 'learning_rate': 4.8e-05, 'epoch': 5.12}
{'loss': 1.9, 'grad_norm': 7.927525997161865, 'learning_rate': 5.2000000000000004e-05, 'epoch': 5.55}
{'loss': 2.1787, 'grad_norm': 7.732694625854492, 'learning_rate': 5.6000000000000006e-05, 'epoch': 5.97}
{'loss': 1.7033, 'grad_norm': 7.974365234375, 'learning_rate': 6e-05, 'epoch': 6.4}
{'loss': 1.4748, 'grad_norm': 6.667853355407715, 'learning_rate': 6.400000000000001e-05, 'epoch': 6.83}
{'loss': 1.1438, 'grad_norm': 7.037996768951416, 'learning_rate': 6.800000000000001e-05, 'epoch': 7.25}
{'loss': 1.0228, 'grad_norm': 6.401759147644043, 'learning_rate': 7.2e-05, 'epoch': 7.68}
{'loss': 1.0007, 'grad_norm': 5.430713653564453, 'learning_rate': 7.6e-05, 'epoch': 8.11}
{'loss': 0.6009, 'grad_norm': 5.195402145385742, 'learning_rate': 8e-05, 'epoch': 8.53}
{'train_runtime': 228.1124, 'train_samples_per_second': 3.288, 'train_steps_per_second': 0.088, 'train_loss': 3.4444248259067534, 'epoch': 8.53}
[1;34mwandb[0m: üöÄ View run [33mmistral-7b-instruct-v0.3_ZsRE_75[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/7inziwwf[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_023404-7inziwwf/logs[0m
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:04<00:09,  4.94s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:09<00:04,  4.62s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.30s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.41s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_75/part_1/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_75/part_1/eval
0  to  50
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_75/part_1/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
550  to  600
generating!
600  to  650
generating!
650  to  700
generating!
700  to  750
generating!
750  to  800
generating!
800  to  850
generating!
850  to  900
generating!
900  to  950
generating!
950  to  1000
generating!
1000  to  1050
generating!
1050  to  1100
generating!
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_75/part_1/special_tokens_map.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_75/part_1/README.md
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_75/part_1/wandb
Skipping file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_75/part_1/log.txt
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_75/part_1/tokenizer.model
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_75/part_1/adapter_config.json
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_75/part_1/checkpoint-20
Skipping directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_75/part_1/eval
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_75/part_1/tmp_data.jsonl
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_75/part_1/tokenizer_config.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_75/part_1/adapter_model.safetensors
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=1, data_source=ZsRE, data_size=80, random=False
Loading data from ../../data/edit_data/merged_data_part_1.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 80 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_80/part_1/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/mistral-7b-instruct-v0.3
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_80/part_1
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: mistral-7b-instruct-v0.3_ZsRE_80
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:03<00:07,  3.95s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:07<00:03,  3.43s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:09<00:00,  3.12s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:09<00:00,  3.26s/it]
Using custom data configuration default-1e4f624dbe15fa26
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-1e4f624dbe15fa26/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2579.52it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 298.25it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-1e4f624dbe15fa26/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 282.50it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f72904e7ee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 28,311,552 || all params: 7,276,335,104 || trainable%: 0.3890908210705739
0ex [00:00, ?ex/s]31ex [00:00, 308.61ex/s]80ex [00:00, 566.80ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_024409-2cj36up5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mistral-7b-instruct-v0.3_ZsRE_80
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/2cj36up5

Example:
<s> What is Melor's father's name? Merengaria</s>


Example:
<s> What work of fiction is Jack Harkness located in? Lost</s>


Example:
<s> Who is Pierre Gorman's father? Richard Gorman</s>


Example:
<s> Who was the person who directed Bitter Apples? William Beaudine</s>


Example:
<s> Which war did Milton F. Pavlic serve in? Korean War</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 80
})
  0%|          | 0/20 [00:00<?, ?it/s]  5%|‚ñå         | 1/20 [00:11<03:42, 11.74s/it]                                                5%|‚ñå         | 1/20 [00:11<03:42, 11.74s/it] 10%|‚ñà         | 2/20 [00:22<03:23, 11.33s/it]                                               10%|‚ñà         | 2/20 [00:22<03:23, 11.33s/it] 15%|‚ñà‚ñå        | 3/20 [00:33<03:10, 11.19s/it]                                               15%|‚ñà‚ñå        | 3/20 [00:33<03:10, 11.19s/it] 20%|‚ñà‚ñà        | 4/20 [00:44<02:59, 11.19s/it]                                               20%|‚ñà‚ñà        | 4/20 [00:45<02:59, 11.19s/it] 25%|‚ñà‚ñà‚ñå       | 5/20 [00:56<02:47, 11.16s/it]                                               25%|‚ñà‚ñà‚ñå       | 5/20 [00:56<02:47, 11.16s/it] 30%|‚ñà‚ñà‚ñà       | 6/20 [01:07<02:36, 11.16s/it]                                               30%|‚ñà‚ñà‚ñà       | 6/20 [01:07<02:36, 11.16s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:18<02:25, 11.16s/it]                                               35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:18<02:25, 11.16s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:29<02:13, 11.12s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:29<02:13, 11.12s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:40<02:02, 11.10s/it]                                               45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:40<02:02, 11.10s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:51<01:51, 11.15s/it]                                                50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:51<01:51, 11.15s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [02:03<01:40, 11.18s/it]                                                55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [02:03<01:40, 11.18s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:14<01:29, 11.14s/it]                                                60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:14<01:29, 11.14s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:25<01:17, 11.12s/it]                                                65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:25<01:17, 11.12s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:36<01:06, 11.16s/it]                                                70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:36<01:06, 11.16s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:47<00:55, 11.12s/it]                                                75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:47<00:55, 11.12s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:58<00:44, 11.13s/it]                                                80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:58<00:44, 11.13s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [03:09<00:33, 11.14s/it]                                                85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [03:09<00:33, 11.14s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:20<00:22, 11.11s/it]                                                90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:20<00:22, 11.11s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:32<00:11, 11.16s/it]                                                95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:32<00:11, 11.16s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:43<00:00, 11.12s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:43<00:00, 11.12s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:44<00:00, 11.12s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:44<00:00, 11.25s/it]
{'loss': 6.1587, 'grad_norm': 16.091928482055664, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.4}
{'loss': 6.0414, 'grad_norm': 19.38678550720215, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.8}
{'loss': 6.0732, 'grad_norm': 19.477109909057617, 'learning_rate': 1.2e-05, 'epoch': 1.2}
{'loss': 6.3251, 'grad_norm': 18.787643432617188, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.6}
{'loss': 5.8423, 'grad_norm': 17.373579025268555, 'learning_rate': 2e-05, 'epoch': 2.0}
{'loss': 5.3348, 'grad_norm': 22.410961151123047, 'learning_rate': 2.4e-05, 'epoch': 2.4}
{'loss': 4.3863, 'grad_norm': 16.847196578979492, 'learning_rate': 2.8000000000000003e-05, 'epoch': 2.8}
{'loss': 4.8161, 'grad_norm': 21.991384506225586, 'learning_rate': 3.2000000000000005e-05, 'epoch': 3.2}
{'loss': 4.2224, 'grad_norm': 17.04354476928711, 'learning_rate': 3.6e-05, 'epoch': 3.6}
{'loss': 2.9965, 'grad_norm': 14.081084251403809, 'learning_rate': 4e-05, 'epoch': 4.0}
{'loss': 2.6932, 'grad_norm': 9.913407325744629, 'learning_rate': 4.4000000000000006e-05, 'epoch': 4.4}
{'loss': 2.4503, 'grad_norm': 9.510456085205078, 'learning_rate': 4.8e-05, 'epoch': 4.8}
{'loss': 2.1308, 'grad_norm': 8.053091049194336, 'learning_rate': 5.2000000000000004e-05, 'epoch': 5.2}
{'loss': 1.767, 'grad_norm': 7.655198574066162, 'learning_rate': 5.6000000000000006e-05, 'epoch': 5.6}
{'loss': 1.8619, 'grad_norm': 8.164006233215332, 'learning_rate': 6e-05, 'epoch': 6.0}
{'loss': 1.5474, 'grad_norm': 6.848607540130615, 'learning_rate': 6.400000000000001e-05, 'epoch': 6.4}
{'loss': 1.2298, 'grad_norm': 7.058786392211914, 'learning_rate': 6.800000000000001e-05, 'epoch': 6.8}
{'loss': 0.9675, 'grad_norm': 5.971179962158203, 'learning_rate': 7.2e-05, 'epoch': 7.2}
{'loss': 0.9984, 'grad_norm': 6.00392484664917, 'learning_rate': 7.6e-05, 'epoch': 7.6}
{'loss': 0.8298, 'grad_norm': 5.822839736938477, 'learning_rate': 8e-05, 'epoch': 8.0}
{'train_runtime': 227.223, 'train_samples_per_second': 3.521, 'train_steps_per_second': 0.088, 'train_loss': 3.433650103211403, 'epoch': 8.0}
[1;34mwandb[0m: üöÄ View run [33mmistral-7b-instruct-v0.3_ZsRE_80[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/2cj36up5[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_024409-2cj36up5/logs[0m
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:04<00:09,  4.84s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:09<00:04,  4.57s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.26s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.37s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_80/part_1/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_80/part_1/eval
0  to  50
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_80/part_1/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
550  to  600
generating!
600  to  650
generating!
650  to  700
generating!
700  to  750
generating!
750  to  800
generating!
800  to  850
generating!
850  to  900
generating!
900  to  950
generating!
950  to  1000
generating!
1000  to  1050
generating!
1050  to  1100
generating!
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_80/part_1/tmp_data.jsonl
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_80/part_1/tokenizer.model
Skipping directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_80/part_1/eval
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_80/part_1/README.md
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_80/part_1/wandb
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_80/part_1/adapter_config.json
Skipping file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_80/part_1/log.txt
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_80/part_1/adapter_model.safetensors
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_80/part_1/checkpoint-20
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_80/part_1/special_tokens_map.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_80/part_1/tokenizer_config.json
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=1, data_source=ZsRE, data_size=85, random=False
Loading data from ../../data/edit_data/merged_data_part_1.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 85 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_85/part_1/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/mistral-7b-instruct-v0.3
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_85/part_1
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: mistral-7b-instruct-v0.3_ZsRE_85
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:03<00:06,  3.13s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:06<00:03,  3.10s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:08<00:00,  2.96s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:08<00:00,  3.00s/it]
Using custom data configuration default-bd37fbe4fad58717
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-bd37fbe4fad58717/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 7752.87it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 541.62it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-bd37fbe4fad58717/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 274.77it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f5429370ee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 28,311,552 || all params: 7,276,335,104 || trainable%: 0.3890908210705739
0ex [00:00, ?ex/s]37ex [00:00, 369.09ex/s]85ex [00:00, 622.88ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_025406-ek4vdu1j
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mistral-7b-instruct-v0.3_ZsRE_85
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/ek4vdu1j

Example:
<s> Which family does Dendrolobatus belong to? Carabidae</s>


Example:
<s> What is the name of Oomalama's record label? Sony Music Entertainment</s>


Example:
<s> The mother of Princess Sophie of the Netherlands is whom? Sophie of the Netherlands</s>


Example:
<s> What original network is Una Maid en Manhattan on? Rede Globo</s>


Example:
<s> What is the constellation that is made with NGC 6072? Hydra</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 85
})
  0%|          | 0/20 [00:00<?, ?it/s]  5%|‚ñå         | 1/20 [00:11<03:47, 11.96s/it]                                                5%|‚ñå         | 1/20 [00:11<03:47, 11.96s/it] 10%|‚ñà         | 2/20 [00:23<03:26, 11.47s/it]                                               10%|‚ñà         | 2/20 [00:23<03:26, 11.47s/it] 15%|‚ñà‚ñå        | 3/20 [00:34<03:11, 11.27s/it]                                               15%|‚ñà‚ñå        | 3/20 [00:34<03:11, 11.27s/it] 20%|‚ñà‚ñà        | 4/20 [00:45<03:00, 11.25s/it]                                               20%|‚ñà‚ñà        | 4/20 [00:45<03:00, 11.25s/it] 25%|‚ñà‚ñà‚ñå       | 5/20 [00:56<02:47, 11.19s/it]                                               25%|‚ñà‚ñà‚ñå       | 5/20 [00:56<02:47, 11.19s/it] 30%|‚ñà‚ñà‚ñà       | 6/20 [01:07<02:36, 11.14s/it]                                               30%|‚ñà‚ñà‚ñà       | 6/20 [01:07<02:36, 11.14s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:18<02:24, 11.11s/it]                                               35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:18<02:24, 11.11s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:29<02:13, 11.15s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:29<02:13, 11.15s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:40<02:02, 11.15s/it]                                               45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:40<02:02, 11.15s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:52<01:51, 11.14s/it]                                                50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:52<01:51, 11.14s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [02:03<01:40, 11.12s/it]                                                55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [02:03<01:40, 11.12s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:14<01:28, 11.12s/it]                                                60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:14<01:28, 11.12s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:25<01:17, 11.13s/it]                                                65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:25<01:17, 11.13s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:36<01:06, 11.13s/it]                                                70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:36<01:06, 11.13s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:47<00:55, 11.11s/it]                                                75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:47<00:55, 11.11s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:58<00:44, 11.12s/it]                                                80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:58<00:44, 11.12s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [03:09<00:33, 11.10s/it]                                                85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [03:09<00:33, 11.10s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:20<00:22, 11.14s/it]                                                90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:20<00:22, 11.14s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:32<00:11, 11.12s/it]                                                95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:32<00:11, 11.12s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:43<00:00, 11.13s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:43<00:00, 11.13s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:45<00:00, 11.13s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:45<00:00, 11.28s/it]
{'loss': 6.1752, 'grad_norm': 18.25988006591797, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.38}
{'loss': 6.503, 'grad_norm': 19.24062728881836, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.75}
{'loss': 6.3373, 'grad_norm': 16.34905433654785, 'learning_rate': 1.2e-05, 'epoch': 1.13}
{'loss': 5.9164, 'grad_norm': 17.93963623046875, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.51}
{'loss': 5.2221, 'grad_norm': 19.111400604248047, 'learning_rate': 2e-05, 'epoch': 1.88}
{'loss': 5.1711, 'grad_norm': 20.161521911621094, 'learning_rate': 2.4e-05, 'epoch': 2.26}
{'loss': 5.0178, 'grad_norm': 21.064233779907227, 'learning_rate': 2.8000000000000003e-05, 'epoch': 2.64}
{'loss': 4.3745, 'grad_norm': 17.994342803955078, 'learning_rate': 3.2000000000000005e-05, 'epoch': 3.01}
{'loss': 3.7148, 'grad_norm': 17.403295516967773, 'learning_rate': 3.6e-05, 'epoch': 3.39}
{'loss': 3.3371, 'grad_norm': 14.143097877502441, 'learning_rate': 4e-05, 'epoch': 3.76}
{'loss': 2.6073, 'grad_norm': 14.708138465881348, 'learning_rate': 4.4000000000000006e-05, 'epoch': 4.14}
{'loss': 2.4678, 'grad_norm': 9.27443790435791, 'learning_rate': 4.8e-05, 'epoch': 4.52}
{'loss': 2.2803, 'grad_norm': 8.14724349975586, 'learning_rate': 5.2000000000000004e-05, 'epoch': 4.89}
{'loss': 1.8708, 'grad_norm': 7.605013370513916, 'learning_rate': 5.6000000000000006e-05, 'epoch': 5.27}
{'loss': 1.6694, 'grad_norm': 6.461538314819336, 'learning_rate': 6e-05, 'epoch': 5.65}
{'loss': 1.6396, 'grad_norm': 7.556030750274658, 'learning_rate': 6.400000000000001e-05, 'epoch': 6.02}
{'loss': 1.267, 'grad_norm': 5.948193550109863, 'learning_rate': 6.800000000000001e-05, 'epoch': 6.4}
{'loss': 1.2042, 'grad_norm': 6.800625324249268, 'learning_rate': 7.2e-05, 'epoch': 6.78}
{'loss': 1.007, 'grad_norm': 6.6437602043151855, 'learning_rate': 7.6e-05, 'epoch': 7.15}
{'loss': 0.7873, 'grad_norm': 5.244040489196777, 'learning_rate': 8e-05, 'epoch': 7.53}
{'train_runtime': 227.8764, 'train_samples_per_second': 3.73, 'train_steps_per_second': 0.088, 'train_loss': 3.4285022526979447, 'epoch': 7.53}
[1;34mwandb[0m: üöÄ View run [33mmistral-7b-instruct-v0.3_ZsRE_85[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/ek4vdu1j[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_025406-ek4vdu1j/logs[0m
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:05<00:10,  5.06s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:09<00:04,  4.66s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.32s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.45s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_85/part_1/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_85/part_1/eval
0  to  50
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_85/part_1/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
550  to  600
generating!
600  to  650
generating!
650  to  700
generating!
700  to  750
generating!
750  to  800
generating!
800  to  850
generating!
850  to  900
generating!
900  to  950
generating!
950  to  1000
generating!
1000  to  1050
generating!
1050  to  1100
generating!
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_85/part_1/adapter_model.safetensors
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_85/part_1/README.md
Skipping directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_85/part_1/eval
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_85/part_1/tokenizer.model
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_85/part_1/tokenizer_config.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_85/part_1/tmp_data.jsonl
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_85/part_1/wandb
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_85/part_1/special_tokens_map.json
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_85/part_1/checkpoint-20
Skipping file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_85/part_1/log.txt
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_85/part_1/adapter_config.json
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=1, data_source=ZsRE, data_size=90, random=False
Loading data from ../../data/edit_data/merged_data_part_1.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 90 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_90/part_1/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/mistral-7b-instruct-v0.3
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_90/part_1
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: mistral-7b-instruct-v0.3_ZsRE_90
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:03<00:06,  3.13s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:06<00:03,  3.09s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:08<00:00,  2.94s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:08<00:00,  2.99s/it]
Using custom data configuration default-c5bfeed348069685
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-c5bfeed348069685/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2628.01it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 313.94it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-c5bfeed348069685/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 292.49it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f349276bee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 28,311,552 || all params: 7,276,335,104 || trainable%: 0.3890908210705739
0ex [00:00, ?ex/s]1ex [00:00,  8.44ex/s]90ex [00:00, 443.10ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_030350-ljv6ea1n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mistral-7b-instruct-v0.3_ZsRE_90
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/ljv6ea1n

Example:
<s> What is the musical instrument Ariadne musica was intended for? orchestra</s>


Example:
<s> What is the publisher of Smelly Old History? Harper</s>


Example:
<s> The person that is the mother of Infanta Adelgundes, Duchess of Guimar√£es is who? Princess Joaquina of Bourbon-arma</s>


Example:
<s> The mother of Princess Sophie of the Netherlands is whom? Sophie of the Netherlands</s>


Example:
<s> Which company is known as the manufacturer of Euroduplex? Hitachi</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 90
})
  0%|          | 0/20 [00:00<?, ?it/s]  5%|‚ñå         | 1/20 [00:11<03:47, 11.99s/it]                                                5%|‚ñå         | 1/20 [00:11<03:47, 11.99s/it] 10%|‚ñà         | 2/20 [00:23<03:26, 11.48s/it]                                               10%|‚ñà         | 2/20 [00:23<03:26, 11.48s/it] 15%|‚ñà‚ñå        | 3/20 [00:34<03:12, 11.32s/it]                                               15%|‚ñà‚ñå        | 3/20 [00:34<03:12, 11.32s/it] 20%|‚ñà‚ñà        | 4/20 [00:45<03:00, 11.26s/it]                                               20%|‚ñà‚ñà        | 4/20 [00:45<03:00, 11.26s/it] 25%|‚ñà‚ñà‚ñå       | 5/20 [00:56<02:47, 11.18s/it]                                               25%|‚ñà‚ñà‚ñå       | 5/20 [00:56<02:47, 11.18s/it] 30%|‚ñà‚ñà‚ñà       | 6/20 [01:07<02:36, 11.20s/it]                                               30%|‚ñà‚ñà‚ñà       | 6/20 [01:07<02:36, 11.20s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:19<02:26, 11.25s/it]                                               35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:19<02:26, 11.25s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:30<02:14, 11.19s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:30<02:14, 11.19s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:41<02:02, 11.15s/it]                                               45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:41<02:02, 11.15s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:52<01:51, 11.12s/it]                                                50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:52<01:51, 11.12s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [02:03<01:40, 11.16s/it]                                                55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [02:03<01:40, 11.16s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:14<01:29, 11.19s/it]                                                60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:14<01:29, 11.19s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:25<01:18, 11.18s/it]                                                65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:25<01:18, 11.18s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:37<01:06, 11.17s/it]                                                70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:37<01:06, 11.17s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:48<00:55, 11.13s/it]                                                75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:48<00:55, 11.13s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:59<00:44, 11.13s/it]                                                80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:59<00:44, 11.13s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [03:10<00:33, 11.21s/it]                                                85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [03:10<00:33, 11.21s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:21<00:22, 11.19s/it]                                                90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:21<00:22, 11.19s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:32<00:11, 11.15s/it]                                                95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:32<00:11, 11.15s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:43<00:00, 11.15s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:43<00:00, 11.15s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:45<00:00, 11.15s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:45<00:00, 11.30s/it]
{'loss': 6.1144, 'grad_norm': 16.57735824584961, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.36}
{'loss': 6.267, 'grad_norm': 19.846595764160156, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.71}
{'loss': 6.1141, 'grad_norm': 18.127626419067383, 'learning_rate': 1.2e-05, 'epoch': 1.07}
{'loss': 5.8456, 'grad_norm': 16.085840225219727, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.42}
{'loss': 5.5848, 'grad_norm': 19.793594360351562, 'learning_rate': 2e-05, 'epoch': 1.78}
{'loss': 5.1568, 'grad_norm': 19.74955940246582, 'learning_rate': 2.4e-05, 'epoch': 2.13}
{'loss': 4.3866, 'grad_norm': 18.514310836791992, 'learning_rate': 2.8000000000000003e-05, 'epoch': 2.49}
{'loss': 4.6436, 'grad_norm': 19.634428024291992, 'learning_rate': 3.2000000000000005e-05, 'epoch': 2.84}
{'loss': 3.6657, 'grad_norm': 18.887571334838867, 'learning_rate': 3.6e-05, 'epoch': 3.2}
{'loss': 3.1292, 'grad_norm': 14.416157722473145, 'learning_rate': 4e-05, 'epoch': 3.56}
{'loss': 2.8675, 'grad_norm': 11.202651023864746, 'learning_rate': 4.4000000000000006e-05, 'epoch': 3.91}
{'loss': 2.3337, 'grad_norm': 9.02829647064209, 'learning_rate': 4.8e-05, 'epoch': 4.27}
{'loss': 1.9412, 'grad_norm': 7.323978424072266, 'learning_rate': 5.2000000000000004e-05, 'epoch': 4.62}
{'loss': 2.3464, 'grad_norm': 8.042810440063477, 'learning_rate': 5.6000000000000006e-05, 'epoch': 4.98}
{'loss': 1.2778, 'grad_norm': 6.0164031982421875, 'learning_rate': 6e-05, 'epoch': 5.33}
{'loss': 1.7169, 'grad_norm': 7.7637858390808105, 'learning_rate': 6.400000000000001e-05, 'epoch': 5.69}
{'loss': 1.6443, 'grad_norm': 6.277969837188721, 'learning_rate': 6.800000000000001e-05, 'epoch': 6.04}
{'loss': 1.3853, 'grad_norm': 7.294130325317383, 'learning_rate': 7.2e-05, 'epoch': 6.4}
{'loss': 0.9789, 'grad_norm': 5.184551239013672, 'learning_rate': 7.6e-05, 'epoch': 6.76}
{'loss': 1.0682, 'grad_norm': 6.46294641494751, 'learning_rate': 8e-05, 'epoch': 7.11}
{'train_runtime': 228.0291, 'train_samples_per_second': 3.947, 'train_steps_per_second': 0.088, 'train_loss': 3.4234015882015227, 'epoch': 7.11}
[1;34mwandb[0m: üöÄ View run [33mmistral-7b-instruct-v0.3_ZsRE_90[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/ljv6ea1n[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_030350-ljv6ea1n/logs[0m
mv: cannot stat './tmp_data.jsonl': No such file or directory
mv: cannot stat './wandb': No such file or directory
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:05<00:10,  5.08s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:09<00:04,  4.68s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.32s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.46s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_90/part_1/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_90/part_1/eval
0  to  50
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_90/part_1/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
550  to  600
generating!
600  to  650
generating!
650  to  700
generating!
700  to  750
generating!
750  to  800
generating!
800  to  850
generating!
850  to  900
generating!
900  to  950
generating!
950  to  1000
generating!
1000  to  1050
generating!
1050  to  1100
generating!
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_90/part_1/tokenizer.model
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_90/part_1/checkpoint-20
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_90/part_1/adapter_config.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_90/part_1/adapter_model.safetensors
Skipping directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_90/part_1/eval
Skipping file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_90/part_1/log.txt
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_90/part_1/tokenizer_config.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_90/part_1/README.md
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_90/part_1/special_tokens_map.json
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=1, data_source=ZsRE, data_size=95, random=False
Loading data from ../../data/edit_data/merged_data_part_1.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 95 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_95/part_1/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/mistral-7b-instruct-v0.3
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_95/part_1
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: mistral-7b-instruct-v0.3_ZsRE_95
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:03<00:06,  3.14s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:06<00:03,  3.11s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:08<00:00,  2.95s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:08<00:00,  3.00s/it]
Using custom data configuration default-fe8185be72a7f05b
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-fe8185be72a7f05b/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2647.92it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 287.01it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-fe8185be72a7f05b/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 259.68it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f2d6801fee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 28,311,552 || all params: 7,276,335,104 || trainable%: 0.3890908210705739
0ex [00:00, ?ex/s]69ex [00:00, 685.80ex/s]95ex [00:00, 819.29ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_031339-wviuycje
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mistral-7b-instruct-v0.3_ZsRE_95
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/wviuycje

Example:
<s> What work of fiction is Jack Harkness located in? Lost</s>


Example:
<s> Who was Princess Frederica Amalia of Denmark's mother? Caroline Amalia of Hesse-Kassel</s>


Example:
<s> Who fathered Bo Guagua? Xuan Xun</s>


Example:
<s> Who desigened Mark 19 torpedo? United States Marine Corps</s>


Example:
<s> What production company or companies created Rio 2? FremantleMedia</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 95
})
  0%|          | 0/20 [00:00<?, ?it/s]  5%|‚ñå         | 1/20 [00:11<03:40, 11.61s/it]                                                5%|‚ñå         | 1/20 [00:11<03:40, 11.61s/it] 10%|‚ñà         | 2/20 [00:22<03:24, 11.36s/it]                                               10%|‚ñà         | 2/20 [00:22<03:24, 11.36s/it] 15%|‚ñà‚ñå        | 3/20 [00:33<03:11, 11.24s/it]                                               15%|‚ñà‚ñå        | 3/20 [00:33<03:11, 11.24s/it] 20%|‚ñà‚ñà        | 4/20 [00:44<02:58, 11.15s/it]                                               20%|‚ñà‚ñà        | 4/20 [00:44<02:58, 11.15s/it] 25%|‚ñà‚ñà‚ñå       | 5/20 [00:56<02:47, 11.14s/it]                                               25%|‚ñà‚ñà‚ñå       | 5/20 [00:56<02:47, 11.14s/it] 30%|‚ñà‚ñà‚ñà       | 6/20 [01:07<02:36, 11.18s/it]                                               30%|‚ñà‚ñà‚ñà       | 6/20 [01:07<02:36, 11.18s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:18<02:25, 11.16s/it]                                               35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:18<02:25, 11.16s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:29<02:14, 11.17s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:29<02:14, 11.17s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:40<02:02, 11.16s/it]                                               45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:40<02:02, 11.16s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:51<01:51, 11.15s/it]                                                50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:51<01:51, 11.15s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [02:02<01:40, 11.12s/it]                                                55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [02:02<01:40, 11.12s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:14<01:28, 11.12s/it]                                                60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:14<01:28, 11.12s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:25<01:18, 11.15s/it]                                                65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:25<01:18, 11.15s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:36<01:06, 11.14s/it]                                                70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:36<01:06, 11.14s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:47<00:55, 11.12s/it]                                                75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:47<00:55, 11.12s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:58<00:44, 11.09s/it]                                                80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:58<00:44, 11.09s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [03:09<00:33, 11.08s/it]                                                85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [03:09<00:33, 11.08s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:20<00:22, 11.17s/it]                                                90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:20<00:22, 11.17s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:32<00:11, 11.19s/it]                                                95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:32<00:11, 11.19s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:43<00:00, 11.14s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:43<00:00, 11.14s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:44<00:00, 11.14s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:44<00:00, 11.25s/it]
{'loss': 6.218, 'grad_norm': 17.94854736328125, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.34}
{'loss': 5.8531, 'grad_norm': 17.04420280456543, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.67}
{'loss': 6.0715, 'grad_norm': 19.147409439086914, 'learning_rate': 1.2e-05, 'epoch': 1.01}
{'loss': 6.5073, 'grad_norm': 18.781219482421875, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.35}
{'loss': 5.4623, 'grad_norm': 17.561443328857422, 'learning_rate': 2e-05, 'epoch': 1.68}
{'loss': 4.7637, 'grad_norm': 17.056560516357422, 'learning_rate': 2.4e-05, 'epoch': 2.02}
{'loss': 5.0307, 'grad_norm': 20.921968460083008, 'learning_rate': 2.8000000000000003e-05, 'epoch': 2.36}
{'loss': 4.2222, 'grad_norm': 18.948078155517578, 'learning_rate': 3.2000000000000005e-05, 'epoch': 2.69}
{'loss': 3.4455, 'grad_norm': 15.604986190795898, 'learning_rate': 3.6e-05, 'epoch': 3.03}
{'loss': 3.1648, 'grad_norm': 12.814423561096191, 'learning_rate': 4e-05, 'epoch': 3.37}
{'loss': 2.4702, 'grad_norm': 11.380480766296387, 'learning_rate': 4.4000000000000006e-05, 'epoch': 3.71}
{'loss': 2.7805, 'grad_norm': 9.555587768554688, 'learning_rate': 4.8e-05, 'epoch': 4.04}
{'loss': 2.3423, 'grad_norm': 8.560863494873047, 'learning_rate': 5.2000000000000004e-05, 'epoch': 4.38}
{'loss': 1.8676, 'grad_norm': 6.771290302276611, 'learning_rate': 5.6000000000000006e-05, 'epoch': 4.72}
{'loss': 1.5838, 'grad_norm': 6.8101701736450195, 'learning_rate': 6e-05, 'epoch': 5.05}
{'loss': 1.4783, 'grad_norm': 7.347458362579346, 'learning_rate': 6.400000000000001e-05, 'epoch': 5.39}
{'loss': 1.5273, 'grad_norm': 6.288821697235107, 'learning_rate': 6.800000000000001e-05, 'epoch': 5.73}
{'loss': 1.1888, 'grad_norm': 6.703863143920898, 'learning_rate': 7.2e-05, 'epoch': 6.06}
{'loss': 1.028, 'grad_norm': 6.082444667816162, 'learning_rate': 7.6e-05, 'epoch': 6.4}
{'loss': 0.9352, 'grad_norm': 4.510766506195068, 'learning_rate': 8e-05, 'epoch': 6.74}
{'train_runtime': 226.9575, 'train_samples_per_second': 4.186, 'train_steps_per_second': 0.088, 'train_loss': 3.397064560651779, 'epoch': 6.74}
[1;34mwandb[0m: üöÄ View run [33mmistral-7b-instruct-v0.3_ZsRE_95[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/wviuycje[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_031339-wviuycje/logs[0m
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:05<00:10,  5.04s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:09<00:04,  4.67s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.32s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.45s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_95/part_1/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_95/part_1/eval
0  to  50
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_95/part_1/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
550  to  600
generating!
600  to  650
generating!
650  to  700
generating!
700  to  750
generating!
750  to  800
generating!
800  to  850
generating!
850  to  900
generating!
900  to  950
generating!
950  to  1000
generating!
1000  to  1050
generating!
1050  to  1100
generating!
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_95/part_1/wandb
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_95/part_1/adapter_model.safetensors
Skipping file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_95/part_1/log.txt
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_95/part_1/tmp_data.jsonl
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_95/part_1/README.md
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_95/part_1/adapter_config.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_95/part_1/tokenizer_config.json
Skipping directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_95/part_1/eval
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_95/part_1/special_tokens_map.json
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_95/part_1/checkpoint-20
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_95/part_1/tokenizer.model
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=2, data_source=ZsRE, data_size=1, random=False
Loading data from ../../data/edit_data/merged_data_part_2.json
Loaded 1167 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 435 entries.
Selected 1 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_1/part_2/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/mistral-7b-instruct-v0.3
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_1/part_2
batch_size: 1
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: mistral-7b-instruct-v0.3_ZsRE_1
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:03<00:07,  3.85s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:06<00:03,  3.39s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:09<00:00,  3.11s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:09<00:00,  3.23s/it]
Using custom data configuration default-a5172016cf9237b4
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-a5172016cf9237b4/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2542.00it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 304.16it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-a5172016cf9237b4/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 289.56it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f4bf0352ee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 28,311,552 || all params: 7,276,335,104 || trainable%: 0.3890908210705739
0ex [00:00, ?ex/s]1ex [00:00, 25.64ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_032334-yz6v0sai
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mistral-7b-instruct-v0.3_ZsRE_1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/yz6v0sai

Example:
<s> In what fictional work would you find a character named San Theodoros? The Adventures of Sherlock Holmes</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1
})
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:00<00:08,  1.08it/s]                                               10%|‚ñà         | 1/10 [00:00<00:08,  1.08it/s] 20%|‚ñà‚ñà        | 2/10 [00:01<00:04,  1.68it/s]                                               20%|‚ñà‚ñà        | 2/10 [00:01<00:04,  1.68it/s] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:01<00:03,  2.06it/s]                                               30%|‚ñà‚ñà‚ñà       | 3/10 [00:01<00:03,  2.06it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:02<00:02,  2.29it/s]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:02<00:02,  2.29it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:02<00:02,  2.45it/s]                                               50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:02<00:02,  2.45it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:02<00:01,  2.56it/s]                                               60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:02<00:01,  2.56it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [00:03<00:01,  2.63it/s]                                               70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [00:03<00:01,  2.63it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [00:03<00:00,  2.68it/s]                                               80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [00:03<00:00,  2.68it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [00:03<00:00,  2.71it/s]                                               90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [00:03<00:00,  2.71it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:04<00:00,  2.73it/s]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:04<00:00,  2.73it/s]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:06<00:00,  2.73it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:06<00:00,  1.66it/s]
{'loss': 2.3289, 'grad_norm': 38.77854919433594, 'learning_rate': 4.000000000000001e-06, 'epoch': 1.0}
{'loss': 2.3289, 'grad_norm': 38.89750671386719, 'learning_rate': 8.000000000000001e-06, 'epoch': 2.0}
{'loss': 2.0493, 'grad_norm': 35.37394332885742, 'learning_rate': 1.2e-05, 'epoch': 3.0}
{'loss': 1.5719, 'grad_norm': 28.121814727783203, 'learning_rate': 1.6000000000000003e-05, 'epoch': 4.0}
{'loss': 1.0847, 'grad_norm': 20.522817611694336, 'learning_rate': 2e-05, 'epoch': 5.0}
{'loss': 0.6795, 'grad_norm': 13.984277725219727, 'learning_rate': 2.4e-05, 'epoch': 6.0}
{'loss': 0.3228, 'grad_norm': 13.351356506347656, 'learning_rate': 2.8000000000000003e-05, 'epoch': 7.0}
{'loss': 0.1322, 'grad_norm': 7.741158485412598, 'learning_rate': 3.2000000000000005e-05, 'epoch': 8.0}
{'loss': 0.027, 'grad_norm': 2.14032244682312, 'learning_rate': 3.6e-05, 'epoch': 9.0}
{'loss': 0.0027, 'grad_norm': 0.25489890575408936, 'learning_rate': 4e-05, 'epoch': 10.0}
{'train_runtime': 7.9569, 'train_samples_per_second': 1.257, 'train_steps_per_second': 1.257, 'train_loss': 1.052792066684924, 'epoch': 10.0}
[1;34mwandb[0m: üöÄ View run [33mmistral-7b-instruct-v0.3_ZsRE_1[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/yz6v0sai[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_032334-yz6v0sai/logs[0m
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:04<00:09,  4.54s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:08<00:04,  4.47s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:12<00:00,  4.22s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:12<00:00,  4.29s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_1/part_2/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_1/part_2/eval
0  to  50
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_1/part_2/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
550  to  600
generating!
600  to  650
generating!
650  to  700
generating!
700  to  750
generating!
750  to  800
generating!
800  to  850
generating!
850  to  900
generating!
900  to  950
generating!
950  to  1000
generating!
1000  to  1050
generating!
1050  to  1100
generating!
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_1/part_2/adapter_config.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_1/part_2/README.md
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_1/part_2/special_tokens_map.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_1/part_2/tmp_data.jsonl
Skipping file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_1/part_2/log.txt
Skipping directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_1/part_2/eval
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_1/part_2/wandb
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_1/part_2/tokenizer_config.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_1/part_2/adapter_model.safetensors
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_1/part_2/checkpoint-10
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_1/part_2/tokenizer.model
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=2, data_source=ZsRE, data_size=5, random=False
Loading data from ../../data/edit_data/merged_data_part_2.json
Loaded 1167 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 435 entries.
Selected 5 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_5/part_2/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/mistral-7b-instruct-v0.3
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_5/part_2
batch_size: 5
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: mistral-7b-instruct-v0.3_ZsRE_5
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:03<00:06,  3.12s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:06<00:03,  3.09s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:08<00:00,  2.94s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:08<00:00,  2.99s/it]
Using custom data configuration default-0060ff2c1d6a586a
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-0060ff2c1d6a586a/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2680.07it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 330.62it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-0060ff2c1d6a586a/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 285.70it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f666354bee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 28,311,552 || all params: 7,276,335,104 || trainable%: 0.3890908210705739
0ex [00:00, ?ex/s]5ex [00:00, 91.37ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_032956-53r8dv37
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mistral-7b-instruct-v0.3_ZsRE_5
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/53r8dv37

Example:
<s> What is Mus√©e Bourdelle named after? Pierre Bourdelle</s>


Example:
<s> What is the status of Hyloxalus parcus? vulnerable</s>


Example:
<s> What is the fictional universe that √âowyn appears in? Known Space</s>


Example:
<s> Which constellation is Messier 68 a part of? Cygnus</s>


Example:
<s> In what fictional work would you find a character named San Theodoros? The Adventures of Sherlock Holmes</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 5
})
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:02<00:20,  2.29s/it]                                               10%|‚ñà         | 1/10 [00:02<00:20,  2.29s/it] 20%|‚ñà‚ñà        | 2/10 [00:04<00:15,  1.96s/it]                                               20%|‚ñà‚ñà        | 2/10 [00:04<00:15,  1.96s/it] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:05<00:12,  1.85s/it]                                               30%|‚ñà‚ñà‚ñà       | 3/10 [00:05<00:12,  1.85s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:07<00:10,  1.81s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:07<00:10,  1.81s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:09<00:08,  1.78s/it]                                               50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:09<00:08,  1.78s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:10<00:07,  1.76s/it]                                               60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:10<00:07,  1.76s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [00:12<00:05,  1.75s/it]                                               70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [00:12<00:05,  1.75s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [00:14<00:03,  1.75s/it]                                               80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [00:14<00:03,  1.75s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [00:16<00:01,  1.74s/it]                                               90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [00:16<00:01,  1.74s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:17<00:00,  1.74s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:17<00:00,  1.74s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:19<00:00,  1.74s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:19<00:00,  1.97s/it]
{'loss': 7.1338, 'grad_norm': 33.68798065185547, 'learning_rate': 4.000000000000001e-06, 'epoch': 1.0}
{'loss': 7.1338, 'grad_norm': 32.19382095336914, 'learning_rate': 8.000000000000001e-06, 'epoch': 2.0}
{'loss': 6.8743, 'grad_norm': 32.771419525146484, 'learning_rate': 1.2e-05, 'epoch': 3.0}
{'loss': 6.3374, 'grad_norm': 34.893001556396484, 'learning_rate': 1.6000000000000003e-05, 'epoch': 4.0}
{'loss': 5.5952, 'grad_norm': 29.12666130065918, 'learning_rate': 2e-05, 'epoch': 5.0}
{'loss': 4.9, 'grad_norm': 30.300071716308594, 'learning_rate': 2.4e-05, 'epoch': 6.0}
{'loss': 4.1456, 'grad_norm': 28.5105037689209, 'learning_rate': 2.8000000000000003e-05, 'epoch': 7.0}
{'loss': 3.2593, 'grad_norm': 28.739479064941406, 'learning_rate': 3.2000000000000005e-05, 'epoch': 8.0}
{'loss': 2.2576, 'grad_norm': 26.657148361206055, 'learning_rate': 3.6e-05, 'epoch': 9.0}
{'loss': 1.3546, 'grad_norm': 24.169029235839844, 'learning_rate': 4e-05, 'epoch': 10.0}
{'train_runtime': 21.623, 'train_samples_per_second': 2.312, 'train_steps_per_second': 0.462, 'train_loss': 4.899152445793152, 'epoch': 10.0}
[1;34mwandb[0m: üöÄ View run [33mmistral-7b-instruct-v0.3_ZsRE_5[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/53r8dv37[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_032956-53r8dv37/logs[0m
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:04<00:09,  4.80s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:09<00:04,  4.57s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.27s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.37s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_5/part_2/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_5/part_2/eval
0  to  50
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_5/part_2/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
550  to  600
generating!
600  to  650
generating!
650  to  700
generating!
700  to  750
generating!
750  to  800
generating!
800  to  850
generating!
850  to  900
generating!
900  to  950
generating!
950  to  1000
generating!
1000  to  1050
generating!
1050  to  1100
generating!
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_5/part_2/adapter_model.safetensors
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_5/part_2/adapter_config.json
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_5/part_2/wandb
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_5/part_2/tmp_data.jsonl
Skipping file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_5/part_2/log.txt
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_5/part_2/tokenizer.model
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_5/part_2/checkpoint-10
Skipping directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_5/part_2/eval
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_5/part_2/tokenizer_config.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_5/part_2/special_tokens_map.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_5/part_2/README.md
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=2, data_source=ZsRE, data_size=10, random=False
Loading data from ../../data/edit_data/merged_data_part_2.json
Loaded 1167 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 435 entries.
Selected 10 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_10/part_2/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/mistral-7b-instruct-v0.3
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_10/part_2
batch_size: 10
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: mistral-7b-instruct-v0.3_ZsRE_10
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:03<00:06,  3.13s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:06<00:03,  3.10s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:08<00:00,  2.94s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:08<00:00,  2.99s/it]
Using custom data configuration default-b469adbb052d4095
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-b469adbb052d4095/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2718.28it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 331.28it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-b469adbb052d4095/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 745.65it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f56200beee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 28,311,552 || all params: 7,276,335,104 || trainable%: 0.3890908210705739
0ex [00:00, ?ex/s]10ex [00:00, 189.40ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_033632-sdhyvshm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mistral-7b-instruct-v0.3_ZsRE_10
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/sdhyvshm

Example:
<s> What university did John Mortvedt attend? University of Copenhagen</s>


Example:
<s> What is St√©phan Perrot's country of citizenship? Belgium</s>


Example:
<s> In what fictional work would you find a character named San Theodoros? The Adventures of Sherlock Holmes</s>


Example:
<s> Who was Roshan Kumari's mother? Ratna Pathak</s>


Example:
<s> What is the fictional universe that √âowyn appears in? Known Space</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 10
})
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:04<00:36,  4.01s/it]                                               10%|‚ñà         | 1/10 [00:04<00:36,  4.01s/it] 20%|‚ñà‚ñà        | 2/10 [00:07<00:29,  3.68s/it]                                               20%|‚ñà‚ñà        | 2/10 [00:07<00:29,  3.68s/it] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:10<00:24,  3.57s/it]                                               30%|‚ñà‚ñà‚ñà       | 3/10 [00:10<00:24,  3.57s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:14<00:21,  3.52s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:14<00:21,  3.52s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:17<00:17,  3.49s/it]                                               50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:17<00:17,  3.49s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:21<00:13,  3.48s/it]                                               60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:21<00:13,  3.48s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [00:24<00:10,  3.47s/it]                                               70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [00:24<00:10,  3.47s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [00:28<00:06,  3.47s/it]                                               80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [00:28<00:06,  3.47s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [00:31<00:03,  3.46s/it]                                               90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [00:31<00:03,  3.46s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:35<00:00,  3.46s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:35<00:00,  3.46s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:37<00:00,  3.46s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:37<00:00,  3.70s/it]
{'loss': 6.3742, 'grad_norm': 23.883983612060547, 'learning_rate': 4.000000000000001e-06, 'epoch': 1.0}
{'loss': 6.3742, 'grad_norm': 23.86846160888672, 'learning_rate': 8.000000000000001e-06, 'epoch': 2.0}
{'loss': 6.1821, 'grad_norm': 24.763864517211914, 'learning_rate': 1.2e-05, 'epoch': 3.0}
{'loss': 5.797, 'grad_norm': 24.73303985595703, 'learning_rate': 1.6000000000000003e-05, 'epoch': 4.0}
{'loss': 5.2279, 'grad_norm': 23.46023178100586, 'learning_rate': 2e-05, 'epoch': 5.0}
{'loss': 4.5528, 'grad_norm': 22.6961612701416, 'learning_rate': 2.4e-05, 'epoch': 6.0}
{'loss': 3.8466, 'grad_norm': 22.18294906616211, 'learning_rate': 2.8000000000000003e-05, 'epoch': 7.0}
{'loss': 3.0883, 'grad_norm': 20.76176643371582, 'learning_rate': 3.2000000000000005e-05, 'epoch': 8.0}
{'loss': 2.2776, 'grad_norm': 19.72593879699707, 'learning_rate': 3.6e-05, 'epoch': 9.0}
{'loss': 1.4903, 'grad_norm': 16.140405654907227, 'learning_rate': 4e-05, 'epoch': 10.0}
{'train_runtime': 39.1115, 'train_samples_per_second': 2.557, 'train_steps_per_second': 0.256, 'train_loss': 4.5210843801498415, 'epoch': 10.0}
[1;34mwandb[0m: üöÄ View run [33mmistral-7b-instruct-v0.3_ZsRE_10[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/sdhyvshm[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_033632-sdhyvshm/logs[0m
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:04<00:09,  4.54s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:08<00:04,  4.47s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:12<00:00,  4.22s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:12<00:00,  4.30s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_10/part_2/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_10/part_2/eval
0  to  50
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_10/part_2/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
550  to  600
generating!
600  to  650
generating!
650  to  700
generating!
700  to  750
generating!
750  to  800
generating!
800  to  850
generating!
850  to  900
generating!
900  to  950
generating!
950  to  1000
generating!
1000  to  1050
generating!
1050  to  1100
generating!
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_10/part_2/tokenizer.model
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_10/part_2/tokenizer_config.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_10/part_2/tmp_data.jsonl
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_10/part_2/adapter_config.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_10/part_2/adapter_model.safetensors
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_10/part_2/checkpoint-10
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_10/part_2/README.md
Skipping file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_10/part_2/log.txt
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_10/part_2/wandb
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_10/part_2/special_tokens_map.json
Skipping directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_10/part_2/eval
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=2, data_source=ZsRE, data_size=15, random=False
Loading data from ../../data/edit_data/merged_data_part_2.json
Loaded 1167 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 435 entries.
Selected 15 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_15/part_2/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/mistral-7b-instruct-v0.3
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_15/part_2
batch_size: 15
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: mistral-7b-instruct-v0.3_ZsRE_15
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:03<00:06,  3.14s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:06<00:03,  3.10s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:08<00:00,  2.95s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:08<00:00,  2.99s/it]
Using custom data configuration default-37f88d648f0ad688
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-37f88d648f0ad688/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2752.17it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 324.13it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-37f88d648f0ad688/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 277.35it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f5b2c201ee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 28,311,552 || all params: 7,276,335,104 || trainable%: 0.3890908210705739
0ex [00:00, ?ex/s]15ex [00:00, 165.03ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_034325-krzmb9jl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mistral-7b-instruct-v0.3_ZsRE_15
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/krzmb9jl

Example:
<s> What is St√©phan Perrot's country of citizenship? Belgium</s>


Example:
<s> Who was Roshan Kumari's mother? Ratna Pathak</s>


Example:
<s> When was Welsh Proms launched? 1999</s>


Example:
<s> What is the fictional universe that √âowyn appears in? Known Space</s>


Example:
<s> In what fictional work would you find a character named San Theodoros? The Adventures of Sherlock Holmes</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 15
})
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:05<00:51,  5.73s/it]                                               10%|‚ñà         | 1/10 [00:05<00:51,  5.73s/it] 20%|‚ñà‚ñà        | 2/10 [00:10<00:43,  5.40s/it]                                               20%|‚ñà‚ñà        | 2/10 [00:10<00:43,  5.40s/it] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:16<00:37,  5.29s/it]                                               30%|‚ñà‚ñà‚ñà       | 3/10 [00:16<00:37,  5.29s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:21<00:31,  5.25s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:21<00:31,  5.25s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:26<00:26,  5.23s/it]                                               50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:26<00:26,  5.23s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:31<00:20,  5.21s/it]                                               60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:31<00:20,  5.21s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [00:36<00:15,  5.20s/it]                                               70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [00:36<00:15,  5.20s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [00:41<00:10,  5.20s/it]                                               80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [00:41<00:10,  5.20s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [00:47<00:05,  5.21s/it]                                               90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [00:47<00:05,  5.21s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:52<00:00,  5.21s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:52<00:00,  5.21s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:56<00:00,  5.21s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:56<00:00,  5.62s/it]
{'loss': 5.8296, 'grad_norm': 19.997684478759766, 'learning_rate': 4.000000000000001e-06, 'epoch': 1.0}
{'loss': 5.8296, 'grad_norm': 21.480236053466797, 'learning_rate': 8.000000000000001e-06, 'epoch': 2.0}
{'loss': 5.6655, 'grad_norm': 21.38779640197754, 'learning_rate': 1.2e-05, 'epoch': 3.0}
{'loss': 5.3412, 'grad_norm': 20.630634307861328, 'learning_rate': 1.6000000000000003e-05, 'epoch': 4.0}
{'loss': 4.8698, 'grad_norm': 20.97027587890625, 'learning_rate': 2e-05, 'epoch': 5.0}
{'loss': 4.2825, 'grad_norm': 20.340375900268555, 'learning_rate': 2.4e-05, 'epoch': 6.0}
{'loss': 3.625, 'grad_norm': 19.976642608642578, 'learning_rate': 2.8000000000000003e-05, 'epoch': 7.0}
{'loss': 2.9104, 'grad_norm': 18.419116973876953, 'learning_rate': 3.2000000000000005e-05, 'epoch': 8.0}
{'loss': 2.167, 'grad_norm': 15.93930721282959, 'learning_rate': 3.6e-05, 'epoch': 9.0}
{'loss': 1.4481, 'grad_norm': 13.246126174926758, 'learning_rate': 4e-05, 'epoch': 10.0}
{'train_runtime': 58.3514, 'train_samples_per_second': 2.571, 'train_steps_per_second': 0.171, 'train_loss': 4.196866464614868, 'epoch': 10.0}
[1;34mwandb[0m: üöÄ View run [33mmistral-7b-instruct-v0.3_ZsRE_15[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/krzmb9jl[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_034325-krzmb9jl/logs[0m
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:05<00:10,  5.13s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:09<00:04,  4.70s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.35s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.49s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_15/part_2/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_15/part_2/eval
0  to  50
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_15/part_2/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
550  to  600
generating!
600  to  650
generating!
650  to  700
generating!
700  to  750
generating!
750  to  800
generating!
800  to  850
generating!
850  to  900
generating!
900  to  950
generating!
950  to  1000
generating!
1000  to  1050
generating!
1050  to  1100
generating!
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_15/part_2/tmp_data.jsonl
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_15/part_2/tokenizer.model
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_15/part_2/tokenizer_config.json
Skipping file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_15/part_2/log.txt
Skipping directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_15/part_2/eval
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_15/part_2/wandb
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_15/part_2/README.md
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_15/part_2/special_tokens_map.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_15/part_2/adapter_config.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_15/part_2/adapter_model.safetensors
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_15/part_2/checkpoint-10
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=2, data_source=ZsRE, data_size=20, random=False
Loading data from ../../data/edit_data/merged_data_part_2.json
Loaded 1167 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 435 entries.
Selected 20 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_20/part_2/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/mistral-7b-instruct-v0.3
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_20/part_2
batch_size: 20
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: mistral-7b-instruct-v0.3_ZsRE_20
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:03<00:07,  3.89s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:06<00:03,  3.41s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:09<00:00,  3.12s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:09<00:00,  3.25s/it]
Using custom data configuration default-8a74d1f72f630d13
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-8a74d1f72f630d13/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2626.36it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 303.25it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-8a74d1f72f630d13/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 290.08it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f6001e8fee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 28,311,552 || all params: 7,276,335,104 || trainable%: 0.3890908210705739
0ex [00:00, ?ex/s]20ex [00:00, 291.80ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_035044-ax11qnir
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mistral-7b-instruct-v0.3_ZsRE_20
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/ax11qnir

Example:
<s> What is the name of the constellation which HD 175740 belongs? Vela</s>


Example:
<s> When was Welsh Proms launched? 1999</s>


Example:
<s> In which war did Attilio Imolesi participate? Spanish Civil War</s>


Example:
<s> Who was Roshan Kumari's mother? Ratna Pathak</s>


Example:
<s> Which is the basis of PL/pgSQL? Scala</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 20
})
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:07<01:06,  7.44s/it]                                               10%|‚ñà         | 1/10 [00:07<01:06,  7.44s/it] 20%|‚ñà‚ñà        | 2/10 [00:14<00:57,  7.13s/it]                                               20%|‚ñà‚ñà        | 2/10 [00:14<00:57,  7.13s/it] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:21<00:49,  7.04s/it]                                               30%|‚ñà‚ñà‚ñà       | 3/10 [00:21<00:49,  7.04s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:28<00:41,  7.00s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:28<00:41,  7.00s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:35<00:34,  6.97s/it]                                               50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:35<00:34,  6.97s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:42<00:27,  6.96s/it]                                               60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:42<00:27,  6.96s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [00:49<00:20,  6.95s/it]                                               70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [00:49<00:20,  6.95s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [00:55<00:13,  6.95s/it]                                               80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [00:55<00:13,  6.95s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:02<00:06,  6.95s/it]                                               90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:02<00:06,  6.95s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:09<00:00,  6.94s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:09<00:00,  6.94s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:11<00:00,  6.94s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:11<00:00,  7.17s/it]
{'loss': 5.9589, 'grad_norm': 23.205045700073242, 'learning_rate': 4.000000000000001e-06, 'epoch': 1.0}
{'loss': 5.9589, 'grad_norm': 25.14175796508789, 'learning_rate': 8.000000000000001e-06, 'epoch': 2.0}
{'loss': 5.7788, 'grad_norm': 24.476423263549805, 'learning_rate': 1.2e-05, 'epoch': 3.0}
{'loss': 5.4215, 'grad_norm': 24.545236587524414, 'learning_rate': 1.6000000000000003e-05, 'epoch': 4.0}
{'loss': 4.9052, 'grad_norm': 23.557878494262695, 'learning_rate': 2e-05, 'epoch': 5.0}
{'loss': 4.3076, 'grad_norm': 20.133209228515625, 'learning_rate': 2.4e-05, 'epoch': 6.0}
{'loss': 3.655, 'grad_norm': 18.6254940032959, 'learning_rate': 2.8000000000000003e-05, 'epoch': 7.0}
{'loss': 2.9591, 'grad_norm': 17.41423225402832, 'learning_rate': 3.2000000000000005e-05, 'epoch': 8.0}
{'loss': 2.2684, 'grad_norm': 15.057929992675781, 'learning_rate': 3.6e-05, 'epoch': 9.0}
{'loss': 1.6443, 'grad_norm': 13.120943069458008, 'learning_rate': 4e-05, 'epoch': 10.0}
{'train_runtime': 73.8306, 'train_samples_per_second': 2.709, 'train_steps_per_second': 0.135, 'train_loss': 4.285764050483704, 'epoch': 10.0}
[1;34mwandb[0m: üöÄ View run [33mmistral-7b-instruct-v0.3_ZsRE_20[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/ax11qnir[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_035044-ax11qnir/logs[0m
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:04<00:09,  4.59s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:08<00:04,  4.48s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:12<00:00,  4.22s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:12<00:00,  4.30s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_20/part_2/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_20/part_2/eval
0  to  50
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_20/part_2/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
550  to  600
generating!
600  to  650
generating!
650  to  700
generating!
700  to  750
generating!
750  to  800
generating!
800  to  850
generating!
850  to  900
generating!
900  to  950
generating!
950  to  1000
generating!
1000  to  1050
generating!
1050  to  1100
generating!
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_20/part_2/tokenizer.model
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_20/part_2/adapter_config.json
Skipping directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_20/part_2/eval
Skipping file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_20/part_2/log.txt
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_20/part_2/wandb
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_20/part_2/README.md
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_20/part_2/adapter_model.safetensors
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_20/part_2/tokenizer_config.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_20/part_2/tmp_data.jsonl
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_20/part_2/special_tokens_map.json
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_20/part_2/checkpoint-10
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=2, data_source=ZsRE, data_size=25, random=False
Loading data from ../../data/edit_data/merged_data_part_2.json
Loaded 1167 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 435 entries.
Selected 25 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_25/part_2/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/mistral-7b-instruct-v0.3
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_25/part_2
batch_size: 25
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: mistral-7b-instruct-v0.3_ZsRE_25
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:03<00:07,  3.92s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:06<00:03,  3.42s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:09<00:00,  3.12s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:09<00:00,  3.25s/it]
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Traceback (most recent call last):
  File "/home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/train_lora_wo_tmp.py", line 496, in <module>
    fire.Fire(train)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/fire/core.py", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/fire/core.py", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "/home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/train_lora_wo_tmp.py", line 373, in train
    data = load_dataset("json", data_files=data_path)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/load.py", line 1675, in load_dataset
    builder_instance = load_dataset_builder(
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/load.py", line 1512, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/load.py", line 1117, in dataset_module_factory
    return PackagedDatasetModuleFactory(
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/load.py", line 774, in get_module
    data_files = DataFilesDict.from_local_or_remote(patterns, use_auth_token=self.downnload_config.use_auth_token)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/data_files.py", line 578, in from_local_or_remote
    DataFilesList.from_local_or_remote(
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/data_files.py", line 546, in from_local_or_remote
    data_files = resolve_patterns_locally_or_by_urls(base_path, patterns, allowed_extensions)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/data_files.py", line 196, in resolve_patterns_locally_or_by_urls
    for path in _resolve_single_pattern_locally(base_path, pattern, allowed_extensions):
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/data_files.py", line 146, in _resolve_single_pattern_locally
    raise FileNotFoundError(error_msg)
FileNotFoundError: Unable to find '/home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/./tmp_data.jsonl' at /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora
mv: cannot stat './tmp_data.jsonl': No such file or directory
mv: cannot stat './wandb': No such file or directory
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:04<00:09,  4.56s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:08<00:04,  4.47s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:12<00:00,  4.21s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:12<00:00,  4.29s/it]
We are now merging the lora adapter to base model
Traceback (most recent call last):
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/peft/config.py", line 184, in _get_peft_type
    config_file = hf_hub_download(
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py", line 101, in inner_f
    return f(*args, **kwargs)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_25/part_2'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/lora_eval.py", line 36, in <module>
    lora_model = PeftModel.from_pretrained(model, args.lora_path)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/peft/peft_model.py", line 324, in from_pretrained
    PeftConfig._get_peft_type(
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/peft/config.py", line 190, in _get_peft_type
    raise ValueError(f"Can't find '{CONFIG_NAME}' at '{model_id}'")
ValueError: Can't find 'adapter_config.json' at '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_25/part_2'
Skipping file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_25/part_2/log.txt
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=2, data_source=ZsRE, data_size=30, random=False
Loading data from ../../data/edit_data/merged_data_part_2.json
Loaded 1167 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 435 entries.
Selected 30 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_30/part_2/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/mistral-7b-instruct-v0.3
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_30/part_2
batch_size: 30
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: mistral-7b-instruct-v0.3_ZsRE_30
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:03<00:06,  3.15s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:06<00:03,  3.11s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:09<00:00,  2.96s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:09<00:00,  3.00s/it]
Using custom data configuration default-6025f717450f4cc5
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-6025f717450f4cc5/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2661.36it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 298.63it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-6025f717450f4cc5/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 587.27it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7fe7f26adee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 28,311,552 || all params: 7,276,335,104 || trainable%: 0.3890908210705739
0ex [00:00, ?ex/s]30ex [00:00, 328.29ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_035847-ucsthysg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mistral-7b-instruct-v0.3_ZsRE_30
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/ucsthysg

Example:
<s> Who had the role of director in Pidakkozhi Koovunna Noottandu? J Sasikumar</s>


Example:
<s> Which lady gave birth to James Hemings? Charlotte Hemings</s>


Example:
<s> Who was Roshan Kumari's mother? Ratna Pathak</s>


Example:
<s> Who acted in Mangalam Veettil Manaseswari Gupta? Mukesh</s>


Example:
<s> The movie Toy Story is by whom? Burt Lancaster</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 30
})
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:11<01:40, 11.21s/it]                                               10%|‚ñà         | 1/10 [00:11<01:40, 11.21s/it] 20%|‚ñà‚ñà        | 2/10 [00:21<01:25, 10.69s/it]                                               20%|‚ñà‚ñà        | 2/10 [00:21<01:25, 10.69s/it] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:31<01:13, 10.53s/it]                                               30%|‚ñà‚ñà‚ñà       | 3/10 [00:31<01:13, 10.53s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:42<01:02, 10.46s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:42<01:02, 10.46s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:52<00:52, 10.43s/it]                                               50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:52<00:52, 10.43s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:02<00:41, 10.40s/it]                                               60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:02<00:41, 10.40s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:13<00:31, 10.39s/it]                                               70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:13<00:31, 10.39s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:23<00:20, 10.38s/it]                                               80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:23<00:20, 10.38s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:34<00:10, 10.38s/it]                                               90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:34<00:10, 10.38s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:44<00:00, 10.37s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:44<00:00, 10.37s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:46<00:00, 10.37s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:46<00:00, 10.64s/it]
{'loss': 6.2827, 'grad_norm': 21.620311737060547, 'learning_rate': 4.000000000000001e-06, 'epoch': 1.0}
{'loss': 6.2827, 'grad_norm': 22.065448760986328, 'learning_rate': 8.000000000000001e-06, 'epoch': 2.0}
{'loss': 6.1251, 'grad_norm': 22.699182510375977, 'learning_rate': 1.2e-05, 'epoch': 3.0}
{'loss': 5.8133, 'grad_norm': 21.01140022277832, 'learning_rate': 1.6000000000000003e-05, 'epoch': 4.0}
{'loss': 5.3632, 'grad_norm': 20.90547752380371, 'learning_rate': 2e-05, 'epoch': 5.0}
{'loss': 4.8068, 'grad_norm': 20.127544403076172, 'learning_rate': 2.4e-05, 'epoch': 6.0}
{'loss': 4.1887, 'grad_norm': 18.46307373046875, 'learning_rate': 2.8000000000000003e-05, 'epoch': 7.0}
{'loss': 3.4989, 'grad_norm': 17.640094757080078, 'learning_rate': 3.2000000000000005e-05, 'epoch': 8.0}
{'loss': 2.7939, 'grad_norm': 15.130318641662598, 'learning_rate': 3.6e-05, 'epoch': 9.0}
{'loss': 2.1448, 'grad_norm': 12.244735717773438, 'learning_rate': 4e-05, 'epoch': 10.0}
{'train_runtime': 108.5304, 'train_samples_per_second': 2.764, 'train_steps_per_second': 0.092, 'train_loss': 4.730013084411621, 'epoch': 10.0}
[1;34mwandb[0m: üöÄ View run [33mmistral-7b-instruct-v0.3_ZsRE_30[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/ucsthysg[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_035847-ucsthysg/logs[0m
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:05<00:10,  5.10s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:09<00:04,  4.69s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.34s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.48s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_30/part_2/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_30/part_2/eval
0  to  50
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_30/part_2/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
550  to  600
generating!
600  to  650
generating!
650  to  700
generating!
700  to  750
generating!
750  to  800
generating!
800  to  850
generating!
850  to  900
generating!
900  to  950
generating!
950  to  1000
generating!
1000  to  1050
generating!
1050  to  1100
generating!
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_30/part_2/adapter_model.safetensors
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_30/part_2/tokenizer.model
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_30/part_2/README.md
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_30/part_2/adapter_config.json
Skipping directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_30/part_2/eval
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_30/part_2/tokenizer_config.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_30/part_2/tmp_data.jsonl
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_30/part_2/special_tokens_map.json
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_30/part_2/checkpoint-10
Skipping file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_30/part_2/log.txt
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_30/part_2/wandb
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=2, data_source=ZsRE, data_size=35, random=False
Loading data from ../../data/edit_data/merged_data_part_2.json
Loaded 1167 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 435 entries.
Selected 35 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_35/part_2/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/mistral-7b-instruct-v0.3
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_35/part_2
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: mistral-7b-instruct-v0.3_ZsRE_35
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:03<00:06,  3.14s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:06<00:03,  3.11s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:09<00:00,  2.96s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:09<00:00,  3.00s/it]
Using custom data configuration default-50af6ef4c2d52d64
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-50af6ef4c2d52d64/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2706.00it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 338.61it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-50af6ef4c2d52d64/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 616.54it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f4938eaeee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 28,311,552 || all params: 7,276,335,104 || trainable%: 0.3890908210705739
0ex [00:00, ?ex/s]12ex [00:00, 118.68ex/s]35ex [00:00, 270.23ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_040652-7shapah1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mistral-7b-instruct-v0.3_ZsRE_35
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/7shapah1

Example:
<s> Who acted in Mangalam Veettil Manaseswari Gupta? Mukesh</s>


Example:
<s> Which lady gave birth to James Hemings? Charlotte Hemings</s>


Example:
<s> The movie Toy Story is by whom? Burt Lancaster</s>


Example:
<s> What university did John Mortvedt attend? University of Copenhagen</s>


Example:
<s> Which is the date of death for Guido Nicheli? 1921</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 35
})
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:11<01:44, 11.56s/it]                                               10%|‚ñà         | 1/10 [00:11<01:44, 11.56s/it] 20%|‚ñà‚ñà        | 2/10 [00:22<01:29, 11.24s/it]                                               20%|‚ñà‚ñà        | 2/10 [00:22<01:29, 11.24s/it] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:33<01:18, 11.15s/it]                                               30%|‚ñà‚ñà‚ñà       | 3/10 [00:33<01:18, 11.15s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:44<01:06, 11.10s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:44<01:06, 11.10s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:55<00:55, 11.09s/it]                                               50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:55<00:55, 11.09s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:06<00:44, 11.08s/it]                                               60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:06<00:44, 11.08s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:17<00:33, 11.06s/it]                                               70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:17<00:33, 11.06s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:28<00:22, 11.06s/it]                                               80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:28<00:22, 11.06s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:39<00:11, 11.06s/it]                                               90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:39<00:11, 11.06s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:50<00:00, 11.06s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:50<00:00, 11.06s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:52<00:00, 11.06s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:52<00:00, 11.28s/it]
{'loss': 6.3252, 'grad_norm': 20.377206802368164, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.91}
{'loss': 6.0165, 'grad_norm': 21.654891967773438, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.83}
{'loss': 6.2274, 'grad_norm': 21.21990966796875, 'learning_rate': 1.2e-05, 'epoch': 2.74}
{'loss': 6.3683, 'grad_norm': 24.372777938842773, 'learning_rate': 1.6000000000000003e-05, 'epoch': 3.66}
{'loss': 4.9174, 'grad_norm': 22.009899139404297, 'learning_rate': 2e-05, 'epoch': 4.57}
{'loss': 5.2476, 'grad_norm': 21.312559127807617, 'learning_rate': 2.4e-05, 'epoch': 5.49}
{'loss': 4.1573, 'grad_norm': 20.282155990600586, 'learning_rate': 2.8000000000000003e-05, 'epoch': 6.4}
{'loss': 4.1085, 'grad_norm': 20.856473922729492, 'learning_rate': 3.2000000000000005e-05, 'epoch': 7.31}
{'loss': 3.433, 'grad_norm': 17.402647018432617, 'learning_rate': 3.6e-05, 'epoch': 8.23}
{'loss': 2.1898, 'grad_norm': 12.10358715057373, 'learning_rate': 4e-05, 'epoch': 9.14}
{'train_runtime': 114.8076, 'train_samples_per_second': 3.049, 'train_steps_per_second': 0.087, 'train_loss': 4.899097442626953, 'epoch': 9.14}
[1;34mwandb[0m: üöÄ View run [33mmistral-7b-instruct-v0.3_ZsRE_35[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/7shapah1[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_040652-7shapah1/logs[0m
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:05<00:10,  5.03s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:09<00:04,  4.67s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.33s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.46s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_35/part_2/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_35/part_2/eval
0  to  50
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_35/part_2/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
550  to  600
generating!
600  to  650
generating!
650  to  700
generating!
700  to  750
generating!
750  to  800
generating!
800  to  850
generating!
850  to  900
generating!
900  to  950
generating!
950  to  1000
generating!
1000  to  1050
generating!
1050  to  1100
generating!
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_35/part_2/adapter_model.safetensors
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_35/part_2/special_tokens_map.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_35/part_2/tokenizer.model
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_35/part_2/tmp_data.jsonl
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_35/part_2/checkpoint-10
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_35/part_2/README.md
Skipping directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_35/part_2/eval
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_35/part_2/tokenizer_config.json
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_35/part_2/wandb
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_35/part_2/adapter_config.json
Skipping file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_35/part_2/log.txt
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=2, data_source=ZsRE, data_size=40, random=False
Loading data from ../../data/edit_data/merged_data_part_2.json
Loaded 1167 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 435 entries.
Selected 40 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_40/part_2/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/mistral-7b-instruct-v0.3
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_40/part_2
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: mistral-7b-instruct-v0.3_ZsRE_40
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:03<00:07,  3.51s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:06<00:03,  3.26s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:09<00:00,  3.04s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:09<00:00,  3.13s/it]
Using custom data configuration default-519ab017d592698a
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-519ab017d592698a/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2610.02it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 319.64it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-519ab017d592698a/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 266.15it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f2e4435fee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 28,311,552 || all params: 7,276,335,104 || trainable%: 0.3890908210705739
0ex [00:00, ?ex/s]40ex [00:00, 449.38ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_041505-a9ded3cz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mistral-7b-instruct-v0.3_ZsRE_40
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/a9ded3cz

Example:
<s> Who was Laimbu's father? Qaimbu</s>


Example:
<s> Who was Roshan Kumari's mother? Ratna Pathak</s>


Example:
<s> What is the name of Last Stop Suburbia's record label? Def Jam Recordings</s>


Example:
<s> What is the name of the constellation where 11 Aquarii belongs? Aquarius</s>


Example:
<s> What is Mus√©e Bourdelle named after? Pierre Bourdelle</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 40
})
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:11<01:44, 11.57s/it]                                               10%|‚ñà         | 1/10 [00:11<01:44, 11.57s/it] 20%|‚ñà‚ñà        | 2/10 [00:22<01:30, 11.28s/it]                                               20%|‚ñà‚ñà        | 2/10 [00:22<01:30, 11.28s/it] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:33<01:18, 11.18s/it]                                               30%|‚ñà‚ñà‚ñà       | 3/10 [00:33<01:18, 11.18s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:44<01:06, 11.14s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:44<01:06, 11.14s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:55<00:55, 11.12s/it]                                               50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:55<00:55, 11.12s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:06<00:44, 11.10s/it]                                               60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:06<00:44, 11.10s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:18<00:33, 11.09s/it]                                               70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:18<00:33, 11.09s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:29<00:22, 11.09s/it]                                               80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:29<00:22, 11.09s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:40<00:11, 11.08s/it]                                               90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:40<00:11, 11.08s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:51<00:00, 11.08s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:51<00:00, 11.08s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:53<00:00, 11.08s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:53<00:00, 11.34s/it]
{'loss': 6.5376, 'grad_norm': 20.599031448364258, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.8}
{'loss': 6.1512, 'grad_norm': 18.779001235961914, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.6}
{'loss': 6.3186, 'grad_norm': 23.026334762573242, 'learning_rate': 1.2e-05, 'epoch': 2.4}
{'loss': 6.2415, 'grad_norm': 21.125822067260742, 'learning_rate': 1.6000000000000003e-05, 'epoch': 3.2}
{'loss': 5.3625, 'grad_norm': 20.166851043701172, 'learning_rate': 2e-05, 'epoch': 4.0}
{'loss': 5.2532, 'grad_norm': 20.375864028930664, 'learning_rate': 2.4e-05, 'epoch': 4.8}
{'loss': 4.6049, 'grad_norm': 19.948537826538086, 'learning_rate': 2.8000000000000003e-05, 'epoch': 5.6}
{'loss': 3.852, 'grad_norm': 19.42131233215332, 'learning_rate': 3.2000000000000005e-05, 'epoch': 6.4}
{'loss': 3.1365, 'grad_norm': 17.028575897216797, 'learning_rate': 3.6e-05, 'epoch': 7.2}
{'loss': 2.7669, 'grad_norm': 12.953426361083984, 'learning_rate': 4e-05, 'epoch': 8.0}
{'train_runtime': 115.3245, 'train_samples_per_second': 3.468, 'train_steps_per_second': 0.087, 'train_loss': 5.022490739822388, 'epoch': 8.0}
[1;34mwandb[0m: üöÄ View run [33mmistral-7b-instruct-v0.3_ZsRE_40[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/a9ded3cz[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_041505-a9ded3cz/logs[0m
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:04<00:09,  4.77s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:09<00:04,  4.60s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.31s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.40s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_40/part_2/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_40/part_2/eval
0  to  50
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_40/part_2/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
550  to  600
generating!
600  to  650
generating!
650  to  700
generating!
700  to  750
generating!
750  to  800
generating!
800  to  850
generating!
850  to  900
generating!
900  to  950
generating!
950  to  1000
generating!
1000  to  1050
generating!
1050  to  1100
generating!
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_40/part_2/wandb
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_40/part_2/adapter_model.safetensors
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_40/part_2/tmp_data.jsonl
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_40/part_2/adapter_config.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_40/part_2/README.md
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_40/part_2/tokenizer.model
Skipping file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_40/part_2/log.txt
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_40/part_2/special_tokens_map.json
Skipping directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_40/part_2/eval
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_40/part_2/tokenizer_config.json
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_40/part_2/checkpoint-10
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=2, data_source=ZsRE, data_size=45, random=False
Loading data from ../../data/edit_data/merged_data_part_2.json
Loaded 1167 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 435 entries.
Selected 45 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_45/part_2/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/mistral-7b-instruct-v0.3
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_45/part_2
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: mistral-7b-instruct-v0.3_ZsRE_45
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:03<00:07,  3.98s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:07<00:03,  3.47s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:09<00:00,  3.17s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:09<00:00,  3.30s/it]
Using custom data configuration default-99a7464de582e821
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-99a7464de582e821/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 7516.67it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 618.90it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-99a7464de582e821/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 626.76it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f412374eee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 28,311,552 || all params: 7,276,335,104 || trainable%: 0.3890908210705739
0ex [00:00, ?ex/s]45ex [00:00, 1490.98ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_042346-z4zaat2k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mistral-7b-instruct-v0.3_ZsRE_45
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/z4zaat2k

Example:
<s> Who had the role of director in Pidakkozhi Koovunna Noottandu? J Sasikumar</s>


Example:
<s> Which sports team is Ali Sadiki playing for? Al-Oruba SC</s>


Example:
<s> The mother of Yolanda of Poland is whom? El≈ºbieta Barszcza</s>


Example:
<s> Which director helmed the movie Man on Ground? D W Griffith</s>


Example:
<s> Who is Tippity Witchet's father? Hippie Witchet</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 45
})
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:11<01:47, 11.92s/it]                                               10%|‚ñà         | 1/10 [00:11<01:47, 11.92s/it] 20%|‚ñà‚ñà        | 2/10 [00:22<01:31, 11.40s/it]                                               20%|‚ñà‚ñà        | 2/10 [00:22<01:31, 11.40s/it] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:34<01:18, 11.25s/it]                                               30%|‚ñà‚ñà‚ñà       | 3/10 [00:34<01:18, 11.25s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:45<01:07, 11.18s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:45<01:07, 11.18s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:56<00:55, 11.14s/it]                                               50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:56<00:55, 11.14s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:07<00:44, 11.11s/it]                                               60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:07<00:44, 11.11s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:18<00:33, 11.10s/it]                                               70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:18<00:33, 11.10s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:29<00:22, 11.09s/it]                                               80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:29<00:22, 11.09s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:40<00:11, 11.09s/it]                                               90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:40<00:11, 11.09s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:51<00:00, 11.08s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:51<00:00, 11.08s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:53<00:00, 11.08s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:53<00:00, 11.33s/it]
{'loss': 6.3496, 'grad_norm': 20.12040138244629, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.71}
{'loss': 6.4646, 'grad_norm': 20.418771743774414, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.42}
{'loss': 5.9976, 'grad_norm': 24.203744888305664, 'learning_rate': 1.2e-05, 'epoch': 2.13}
{'loss': 5.9663, 'grad_norm': 19.619781494140625, 'learning_rate': 1.6000000000000003e-05, 'epoch': 2.84}
{'loss': 5.7133, 'grad_norm': 25.042896270751953, 'learning_rate': 2e-05, 'epoch': 3.56}
{'loss': 5.0774, 'grad_norm': 24.385547637939453, 'learning_rate': 2.4e-05, 'epoch': 4.27}
{'loss': 4.4296, 'grad_norm': 16.591854095458984, 'learning_rate': 2.8000000000000003e-05, 'epoch': 4.98}
{'loss': 4.1001, 'grad_norm': 20.12274169921875, 'learning_rate': 3.2000000000000005e-05, 'epoch': 5.69}
{'loss': 3.079, 'grad_norm': 15.732053756713867, 'learning_rate': 3.6e-05, 'epoch': 6.4}
{'loss': 2.876, 'grad_norm': 13.368446350097656, 'learning_rate': 4e-05, 'epoch': 7.11}
{'train_runtime': 115.4075, 'train_samples_per_second': 3.899, 'train_steps_per_second': 0.087, 'train_loss': 5.00534086227417, 'epoch': 7.11}
[1;34mwandb[0m: üöÄ View run [33mmistral-7b-instruct-v0.3_ZsRE_45[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/z4zaat2k[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_042346-z4zaat2k/logs[0m
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:04<00:09,  4.88s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:09<00:04,  4.64s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.31s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.42s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_45/part_2/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_45/part_2/eval
0  to  50
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_45/part_2/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
550  to  600
generating!
600  to  650
generating!
650  to  700
generating!
700  to  750
generating!
750  to  800
generating!
800  to  850
generating!
850  to  900
generating!
900  to  950
generating!
950  to  1000
generating!
1000  to  1050
generating!
1050  to  1100
generating!
Skipping file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_45/part_2/log.txt
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_45/part_2/tmp_data.jsonl
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_45/part_2/checkpoint-10
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_45/part_2/tokenizer_config.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_45/part_2/adapter_model.safetensors
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_45/part_2/adapter_config.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_45/part_2/special_tokens_map.json
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_45/part_2/wandb
Skipping directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_45/part_2/eval
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_45/part_2/tokenizer.model
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_45/part_2/README.md
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=2, data_source=ZsRE, data_size=50, random=False
Loading data from ../../data/edit_data/merged_data_part_2.json
Loaded 1167 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 435 entries.
Selected 50 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_50/part_2/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/mistral-7b-instruct-v0.3
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_50/part_2
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: mistral-7b-instruct-v0.3_ZsRE_50
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:03<00:06,  3.18s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:06<00:03,  3.14s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:09<00:00,  2.99s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:09<00:00,  3.03s/it]
Using custom data configuration default-93e79affd10df485
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-93e79affd10df485/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 7244.05it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 581.49it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-93e79affd10df485/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 284.90it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f0c7013eee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 28,311,552 || all params: 7,276,335,104 || trainable%: 0.3890908210705739
0ex [00:00, ?ex/s]50ex [00:00, 775.38ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_043158-k31jrwxh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mistral-7b-instruct-v0.3_ZsRE_50
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/k31jrwxh

Example:
<s> What university did John Mortvedt attend? University of Copenhagen</s>


Example:
<s> Who acted in Mangalam Veettil Manaseswari Gupta? Mukesh</s>


Example:
<s> What company produced Atlantis, the Lost Continent? Columbia Records</s>


Example:
<s> Who is listed as Jenny Erpenbeck father? Erpenbeck, Jr</s>


Example:
<s> What is the name of Last Stop Suburbia's record label? Def Jam Recordings</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 50
})
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:11<01:44, 11.62s/it]                                               10%|‚ñà         | 1/10 [00:11<01:44, 11.62s/it] 20%|‚ñà‚ñà        | 2/10 [00:22<01:30, 11.27s/it]                                               20%|‚ñà‚ñà        | 2/10 [00:22<01:30, 11.27s/it] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:33<01:18, 11.20s/it]                                               30%|‚ñà‚ñà‚ñà       | 3/10 [00:33<01:18, 11.20s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:44<01:06, 11.13s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:44<01:06, 11.13s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:55<00:55, 11.13s/it]                                               50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:55<00:55, 11.13s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:07<00:44, 11.13s/it]                                               60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:07<00:44, 11.13s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:18<00:33, 11.13s/it]                                               70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:18<00:33, 11.13s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:29<00:22, 11.10s/it]                                               80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:29<00:22, 11.10s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:40<00:11, 11.07s/it]                                               90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:40<00:11, 11.07s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:51<00:00, 11.10s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:51<00:00, 11.10s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:53<00:00, 11.10s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:53<00:00, 11.32s/it]
{'loss': 6.6549, 'grad_norm': 20.441791534423828, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.64}
{'loss': 5.7971, 'grad_norm': 18.8343563079834, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.28}
{'loss': 6.1003, 'grad_norm': 22.65193748474121, 'learning_rate': 1.2e-05, 'epoch': 1.92}
{'loss': 5.6209, 'grad_norm': 19.520246505737305, 'learning_rate': 1.6000000000000003e-05, 'epoch': 2.56}
{'loss': 5.7109, 'grad_norm': 20.061763763427734, 'learning_rate': 2e-05, 'epoch': 3.2}
{'loss': 4.843, 'grad_norm': 21.65786361694336, 'learning_rate': 2.4e-05, 'epoch': 3.84}
{'loss': 4.7088, 'grad_norm': 18.664491653442383, 'learning_rate': 2.8000000000000003e-05, 'epoch': 4.48}
{'loss': 3.8792, 'grad_norm': 20.128652572631836, 'learning_rate': 3.2000000000000005e-05, 'epoch': 5.12}
{'loss': 3.2676, 'grad_norm': 16.63161277770996, 'learning_rate': 3.6e-05, 'epoch': 5.76}
{'loss': 2.9091, 'grad_norm': 13.76824951171875, 'learning_rate': 4e-05, 'epoch': 6.4}
{'train_runtime': 115.2043, 'train_samples_per_second': 4.34, 'train_steps_per_second': 0.087, 'train_loss': 4.949169445037842, 'epoch': 6.4}
[1;34mwandb[0m: üöÄ View run [33mmistral-7b-instruct-v0.3_ZsRE_50[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/k31jrwxh[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_043158-k31jrwxh/logs[0m
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:05<00:10,  5.08s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:09<00:04,  4.71s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.35s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.49s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_50/part_2/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_50/part_2/eval
0  to  50
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_50/part_2/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
550  to  600
generating!
600  to  650
generating!
650  to  700
generating!
700  to  750
generating!
750  to  800
generating!
800  to  850
generating!
850  to  900
generating!
900  to  950
generating!
950  to  1000
generating!
1000  to  1050
generating!
1050  to  1100
generating!
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_50/part_2/README.md
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_50/part_2/adapter_model.safetensors
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_50/part_2/checkpoint-10
Skipping directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_50/part_2/eval
Skipping file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_50/part_2/log.txt
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_50/part_2/tmp_data.jsonl
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_50/part_2/tokenizer.model
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_50/part_2/adapter_config.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_50/part_2/special_tokens_map.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_50/part_2/tokenizer_config.json
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_50/part_2/wandb
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=2, data_source=ZsRE, data_size=55, random=False
Loading data from ../../data/edit_data/merged_data_part_2.json
Loaded 1167 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 435 entries.
Selected 55 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_55/part_2/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/mistral-7b-instruct-v0.3
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_55/part_2
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: mistral-7b-instruct-v0.3_ZsRE_55
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:03<00:06,  3.40s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:06<00:03,  3.23s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:09<00:00,  3.04s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:09<00:00,  3.11s/it]
Using custom data configuration default-8c3acfc3f2a6e1d3
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-8c3acfc3f2a6e1d3/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 7169.75it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 518.46it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-8c3acfc3f2a6e1d3/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 598.42it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7fb1e0f60ee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 28,311,552 || all params: 7,276,335,104 || trainable%: 0.3890908210705739
0ex [00:00, ?ex/s]55ex [00:00, 1143.98ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_044012-dwjpu14q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mistral-7b-instruct-v0.3_ZsRE_55
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/dwjpu14q

Example:
<s> In which year Saint Petersburg Governorate ceased to exist? 1817</s>


Example:
<s> Of which constellation is HD 220105 a part? Pegasus</s>


Example:
<s> What is the name of the constellation where 11 Aquarii belongs? Aquarius</s>


Example:
<s> The movie Toy Story is by whom? Burt Lancaster</s>


Example:
<s> Who was Roshan Kumari's mother? Ratna Pathak</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 55
})
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:11<01:44, 11.66s/it]                                               10%|‚ñà         | 1/10 [00:11<01:44, 11.66s/it] 20%|‚ñà‚ñà        | 2/10 [00:22<01:30, 11.30s/it]                                               20%|‚ñà‚ñà        | 2/10 [00:22<01:30, 11.30s/it] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:33<01:18, 11.23s/it]                                               30%|‚ñà‚ñà‚ñà       | 3/10 [00:33<01:18, 11.23s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:44<01:07, 11.18s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:44<01:07, 11.18s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:56<00:55, 11.16s/it]                                               50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:56<00:55, 11.16s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:07<00:44, 11.13s/it]                                               60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:07<00:44, 11.13s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:18<00:33, 11.14s/it]                                               70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:18<00:33, 11.14s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:29<00:22, 11.14s/it]                                               80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:29<00:22, 11.14s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:40<00:11, 11.14s/it]                                               90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:40<00:11, 11.14s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:51<00:00, 11.13s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:51<00:00, 11.13s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:53<00:00, 11.13s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:53<00:00, 11.34s/it]
{'loss': 6.3696, 'grad_norm': 20.919694900512695, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.58}
{'loss': 5.6306, 'grad_norm': 18.53401756286621, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.16}
{'loss': 6.2824, 'grad_norm': 22.15435791015625, 'learning_rate': 1.2e-05, 'epoch': 1.75}
{'loss': 5.6988, 'grad_norm': 21.804378509521484, 'learning_rate': 1.6000000000000003e-05, 'epoch': 2.33}
{'loss': 5.6113, 'grad_norm': 21.05946922302246, 'learning_rate': 2e-05, 'epoch': 2.91}
{'loss': 5.4103, 'grad_norm': 21.64242172241211, 'learning_rate': 2.4e-05, 'epoch': 3.49}
{'loss': 4.2294, 'grad_norm': 19.183584213256836, 'learning_rate': 2.8000000000000003e-05, 'epoch': 4.07}
{'loss': 3.9635, 'grad_norm': 18.87125587463379, 'learning_rate': 3.2000000000000005e-05, 'epoch': 4.65}
{'loss': 3.2564, 'grad_norm': 16.269033432006836, 'learning_rate': 3.6e-05, 'epoch': 5.24}
{'loss': 2.9325, 'grad_norm': 12.7332181930542, 'learning_rate': 4e-05, 'epoch': 5.82}
{'train_runtime': 115.3292, 'train_samples_per_second': 4.769, 'train_steps_per_second': 0.087, 'train_loss': 4.938483572006225, 'epoch': 5.82}
[1;34mwandb[0m: üöÄ View run [33mmistral-7b-instruct-v0.3_ZsRE_55[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/dwjpu14q[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_044012-dwjpu14q/logs[0m
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:05<00:10,  5.15s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:09<00:04,  4.73s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.36s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.50s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_55/part_2/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_55/part_2/eval
0  to  50
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_55/part_2/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
550  to  600
generating!
600  to  650
generating!
650  to  700
generating!
700  to  750
generating!
750  to  800
generating!
800  to  850
generating!
850  to  900
generating!
900  to  950
generating!
950  to  1000
generating!
1000  to  1050
generating!
1050  to  1100
generating!
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_55/part_2/tmp_data.jsonl
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_55/part_2/adapter_model.safetensors
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_55/part_2/special_tokens_map.json
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_55/part_2/checkpoint-10
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_55/part_2/tokenizer_config.json
Skipping file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_55/part_2/log.txt
Skipping directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_55/part_2/eval
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_55/part_2/README.md
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_55/part_2/adapter_config.json
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_55/part_2/wandb
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_55/part_2/tokenizer.model
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=2, data_source=ZsRE, data_size=60, random=False
Loading data from ../../data/edit_data/merged_data_part_2.json
Loaded 1167 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 435 entries.
Selected 60 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_60/part_2/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/mistral-7b-instruct-v0.3
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_60/part_2
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: mistral-7b-instruct-v0.3_ZsRE_60
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:03<00:06,  3.18s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:06<00:03,  3.14s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:09<00:00,  2.99s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:09<00:00,  3.03s/it]
Using custom data configuration default-3c6a1305cf999092
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-3c6a1305cf999092/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2702.52it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 317.27it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-3c6a1305cf999092/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 285.04it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f7a64081ee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 28,311,552 || all params: 7,276,335,104 || trainable%: 0.3890908210705739
0ex [00:00, ?ex/s]32ex [00:00, 318.14ex/s]60ex [00:00, 473.50ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_044825-57t7n7py
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mistral-7b-instruct-v0.3_ZsRE_60
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/57t7n7py

Example:
<s> In which year Saint Petersburg Governorate ceased to exist? 1817</s>


Example:
<s> What voice type is Licia Albanese? mezzo soprano</s>


Example:
<s> What is the name of the constellation where 11 Aquarii belongs? Aquarius</s>


Example:
<s> The movie Toy Story is by whom? Burt Lancaster</s>


Example:
<s> Who was Roshan Kumari's mother? Ratna Pathak</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 60
})
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:11<01:44, 11.64s/it]                                               10%|‚ñà         | 1/10 [00:11<01:44, 11.64s/it] 20%|‚ñà‚ñà        | 2/10 [00:22<01:30, 11.26s/it]                                               20%|‚ñà‚ñà        | 2/10 [00:22<01:30, 11.26s/it] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:33<01:18, 11.21s/it]                                               30%|‚ñà‚ñà‚ñà       | 3/10 [00:33<01:18, 11.21s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:44<01:07, 11.18s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:44<01:07, 11.18s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:55<00:55, 11.12s/it]                                               50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:55<00:55, 11.12s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:06<00:44, 11.10s/it]                                               60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:06<00:44, 11.10s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:18<00:33, 11.10s/it]                                               70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:18<00:33, 11.10s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:29<00:22, 11.08s/it]                                               80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:29<00:22, 11.08s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:40<00:11, 11.09s/it]                                               90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:40<00:11, 11.09s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:51<00:00, 11.10s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:51<00:00, 11.10s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:53<00:00, 11.10s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:53<00:00, 11.34s/it]
{'loss': 5.7703, 'grad_norm': 19.105623245239258, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.53}
{'loss': 6.7209, 'grad_norm': 25.598257064819336, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.07}
{'loss': 5.4419, 'grad_norm': 21.54950714111328, 'learning_rate': 1.2e-05, 'epoch': 1.6}
{'loss': 6.3904, 'grad_norm': 19.244600296020508, 'learning_rate': 1.6000000000000003e-05, 'epoch': 2.13}
{'loss': 5.2943, 'grad_norm': 20.352214813232422, 'learning_rate': 2e-05, 'epoch': 2.67}
{'loss': 5.1034, 'grad_norm': 23.3676700592041, 'learning_rate': 2.4e-05, 'epoch': 3.2}
{'loss': 4.9808, 'grad_norm': 20.57158851623535, 'learning_rate': 2.8000000000000003e-05, 'epoch': 3.73}
{'loss': 3.7289, 'grad_norm': 17.8475341796875, 'learning_rate': 3.2000000000000005e-05, 'epoch': 4.27}
{'loss': 3.3684, 'grad_norm': 15.97416877746582, 'learning_rate': 3.6e-05, 'epoch': 4.8}
{'loss': 2.633, 'grad_norm': 13.70605754852295, 'learning_rate': 4e-05, 'epoch': 5.33}
{'train_runtime': 115.3939, 'train_samples_per_second': 5.2, 'train_steps_per_second': 0.087, 'train_loss': 4.9432188987731935, 'epoch': 5.33}
[1;34mwandb[0m: üöÄ View run [33mmistral-7b-instruct-v0.3_ZsRE_60[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/57t7n7py[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_044825-57t7n7py/logs[0m
mv: cannot stat './tmp_data.jsonl': No such file or directory
mv: cannot stat './wandb': No such file or directory
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:04<00:09,  4.99s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:09<00:04,  4.67s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.33s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.46s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_60/part_2/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_60/part_2/eval
0  to  50
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_60/part_2/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
550  to  600
generating!
600  to  650
generating!
650  to  700
generating!
700  to  750
generating!
750  to  800
generating!
800  to  850
generating!
850  to  900
generating!
900  to  950
generating!
950  to  1000
generating!
1000  to  1050
generating!
1050  to  1100
generating!
Skipping directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_60/part_2/eval
Skipping file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_60/part_2/log.txt
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_60/part_2/special_tokens_map.json
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_60/part_2/checkpoint-10
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_60/part_2/adapter_model.safetensors
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_60/part_2/tokenizer_config.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_60/part_2/adapter_config.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_60/part_2/tokenizer.model
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_60/part_2/README.md
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=2, data_source=ZsRE, data_size=65, random=False
Loading data from ../../data/edit_data/merged_data_part_2.json
Loaded 1167 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 435 entries.
Selected 65 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_65/part_2/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/mistral-7b-instruct-v0.3
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_65/part_2
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: mistral-7b-instruct-v0.3_ZsRE_65
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:03<00:06,  3.18s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:06<00:03,  3.15s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:09<00:00,  2.99s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:09<00:00,  3.04s/it]
Using custom data configuration default-f37aef88e1939046
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-f37aef88e1939046/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 7410.43it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 581.57it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-f37aef88e1939046/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 398.21it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f6a200feee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 28,311,552 || all params: 7,276,335,104 || trainable%: 0.3890908210705739
0ex [00:00, ?ex/s]53ex [00:00, 529.26ex/s]65ex [00:00, 596.83ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_045635-lcke9kzn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mistral-7b-instruct-v0.3_ZsRE_65
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/lcke9kzn

Example:
<s> Who had the role of director in Pidakkozhi Koovunna Noottandu? J Sasikumar</s>


Example:
<s> Who was Rolf Thommessen's father? Gunnar Thommessen</s>


Example:
<s> What is the name of the constellation where 11 Aquarii belongs? Aquarius</s>


Example:
<s> The movie Toy Story is by whom? Burt Lancaster</s>


Example:
<s> Who was Roshan Kumari's mother? Ratna Pathak</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 65
})
  0%|          | 0/20 [00:00<?, ?it/s]  5%|‚ñå         | 1/20 [00:11<03:41, 11.64s/it]                                                5%|‚ñå         | 1/20 [00:11<03:41, 11.64s/it] 10%|‚ñà         | 2/20 [00:22<03:22, 11.25s/it]                                               10%|‚ñà         | 2/20 [00:22<03:22, 11.25s/it] 15%|‚ñà‚ñå        | 3/20 [00:33<03:09, 11.15s/it]                                               15%|‚ñà‚ñå        | 3/20 [00:33<03:09, 11.15s/it] 20%|‚ñà‚ñà        | 4/20 [00:44<02:58, 11.13s/it]                                               20%|‚ñà‚ñà        | 4/20 [00:44<02:58, 11.13s/it] 25%|‚ñà‚ñà‚ñå       | 5/20 [00:55<02:46, 11.10s/it]                                               25%|‚ñà‚ñà‚ñå       | 5/20 [00:55<02:46, 11.10s/it] 30%|‚ñà‚ñà‚ñà       | 6/20 [01:06<02:35, 11.11s/it]                                               30%|‚ñà‚ñà‚ñà       | 6/20 [01:06<02:35, 11.11s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:17<02:24, 11.08s/it]                                               35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:17<02:24, 11.08s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:29<02:13, 11.10s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:29<02:13, 11.10s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:40<02:02, 11.10s/it]                                               45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:40<02:02, 11.10s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:51<01:50, 11.08s/it]                                                50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:51<01:50, 11.08s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [02:02<01:39, 11.10s/it]                                                55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [02:02<01:39, 11.10s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:13<01:28, 11.08s/it]                                                60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:13<01:28, 11.08s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:24<01:17, 11.06s/it]                                                65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:24<01:17, 11.06s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:35<01:06, 11.08s/it]                                                70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:35<01:06, 11.08s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:46<00:55, 11.07s/it]                                                75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:46<00:55, 11.07s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:57<00:44, 11.06s/it]                                                80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:57<00:44, 11.06s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [03:08<00:33, 11.10s/it]                                                85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [03:08<00:33, 11.10s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:19<00:22, 11.08s/it]                                                90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:19<00:22, 11.08s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:30<00:11, 11.07s/it]                                                95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:30<00:11, 11.07s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:42<00:00, 11.09s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:42<00:00, 11.09s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:43<00:00, 11.09s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:43<00:00, 11.20s/it]
{'loss': 5.8608, 'grad_norm': 18.922752380371094, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.49}
{'loss': 6.492, 'grad_norm': 21.375757217407227, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.98}
{'loss': 6.0631, 'grad_norm': 22.699417114257812, 'learning_rate': 1.2e-05, 'epoch': 1.48}
{'loss': 6.1312, 'grad_norm': 20.69173812866211, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.97}
{'loss': 5.3671, 'grad_norm': 21.148069381713867, 'learning_rate': 2e-05, 'epoch': 2.46}
{'loss': 5.2456, 'grad_norm': 20.73600196838379, 'learning_rate': 2.4e-05, 'epoch': 2.95}
{'loss': 4.5785, 'grad_norm': 19.515663146972656, 'learning_rate': 2.8000000000000003e-05, 'epoch': 3.45}
{'loss': 4.1009, 'grad_norm': 19.291515350341797, 'learning_rate': 3.2000000000000005e-05, 'epoch': 3.94}
{'loss': 3.3763, 'grad_norm': 16.86880111694336, 'learning_rate': 3.6e-05, 'epoch': 4.43}
{'loss': 2.9511, 'grad_norm': 12.319901466369629, 'learning_rate': 4e-05, 'epoch': 4.92}
{'loss': 2.2166, 'grad_norm': 10.40030288696289, 'learning_rate': 4.4000000000000006e-05, 'epoch': 5.42}
{'loss': 2.1675, 'grad_norm': 8.747917175292969, 'learning_rate': 4.8e-05, 'epoch': 5.91}
{'loss': 1.825, 'grad_norm': 7.437482833862305, 'learning_rate': 5.2000000000000004e-05, 'epoch': 6.4}
{'loss': 1.4427, 'grad_norm': 6.70883846282959, 'learning_rate': 5.6000000000000006e-05, 'epoch': 6.89}
{'loss': 1.5088, 'grad_norm': 7.744499683380127, 'learning_rate': 6e-05, 'epoch': 7.38}
{'loss': 0.9731, 'grad_norm': 6.534130573272705, 'learning_rate': 6.400000000000001e-05, 'epoch': 7.88}
{'loss': 1.074, 'grad_norm': 6.903711318969727, 'learning_rate': 6.800000000000001e-05, 'epoch': 8.37}
{'loss': 0.8039, 'grad_norm': 4.915553092956543, 'learning_rate': 7.2e-05, 'epoch': 8.86}
{'loss': 0.7331, 'grad_norm': 5.067290782928467, 'learning_rate': 7.6e-05, 'epoch': 9.35}
{'loss': 0.4873, 'grad_norm': 4.420919895172119, 'learning_rate': 8e-05, 'epoch': 9.85}
{'train_runtime': 226.0474, 'train_samples_per_second': 2.876, 'train_steps_per_second': 0.088, 'train_loss': 3.169931690394878, 'epoch': 9.85}
[1;34mwandb[0m: üöÄ View run [33mmistral-7b-instruct-v0.3_ZsRE_65[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/lcke9kzn[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_045635-lcke9kzn/logs[0m
mv: cannot stat './tmp_data.jsonl': No such file or directory
mv: cannot stat './wandb': No such file or directory
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:04<00:09,  4.78s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:09<00:04,  4.61s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.30s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.40s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_65/part_2/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_65/part_2/eval
0  to  50
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_65/part_2/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
550  to  600
generating!
600  to  650
generating!
650  to  700
generating!
700  to  750
generating!
750  to  800
generating!
800  to  850
generating!
850  to  900
generating!
900  to  950
generating!
950  to  1000
generating!
1000  to  1050
generating!
1050  to  1100
generating!
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_65/part_2/adapter_model.safetensors
Skipping directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_65/part_2/eval
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_65/part_2/tokenizer.model
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_65/part_2/checkpoint-20
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_65/part_2/special_tokens_map.json
Skipping file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_65/part_2/log.txt
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_65/part_2/README.md
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_65/part_2/tokenizer_config.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_65/part_2/adapter_config.json
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=2, data_source=ZsRE, data_size=70, random=False
Loading data from ../../data/edit_data/merged_data_part_2.json
Loaded 1167 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 435 entries.
Selected 70 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_70/part_2/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/mistral-7b-instruct-v0.3
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_70/part_2
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: mistral-7b-instruct-v0.3_ZsRE_70
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:03<00:06,  3.18s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:06<00:03,  3.14s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:09<00:00,  2.99s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:09<00:00,  3.03s/it]
Using custom data configuration default-3701509149ac225e
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-3701509149ac225e/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2649.59it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 323.56it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-3701509149ac225e/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 232.58it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7fbf403ceee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 28,311,552 || all params: 7,276,335,104 || trainable%: 0.3890908210705739
0ex [00:00, ?ex/s]1ex [00:00,  8.63ex/s]70ex [00:00, 372.66ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_050627-oynjr96z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mistral-7b-instruct-v0.3_ZsRE_70
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/oynjr96z

Example:
<s> What caused Terry Giddy's death? Parkinson's disease</s>


Example:
<s> The movie Toy Story is by whom? Burt Lancaster</s>


Example:
<s> Who was Laimbu's father? Qaimbu</s>


Example:
<s> What country is Shmavon Shmavonyan from? Yemen</s>


Example:
<s> What is Mus√©e Bourdelle named after? Pierre Bourdelle</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 70
})
  0%|          | 0/20 [00:00<?, ?it/s]  5%|‚ñå         | 1/20 [00:11<03:42, 11.69s/it]                                                5%|‚ñå         | 1/20 [00:11<03:42, 11.69s/it] 10%|‚ñà         | 2/20 [00:22<03:23, 11.28s/it]                                               10%|‚ñà         | 2/20 [00:22<03:23, 11.28s/it] 15%|‚ñà‚ñå        | 3/20 [00:33<03:10, 11.22s/it]                                               15%|‚ñà‚ñå        | 3/20 [00:33<03:10, 11.22s/it] 20%|‚ñà‚ñà        | 4/20 [00:44<02:58, 11.14s/it]                                               20%|‚ñà‚ñà        | 4/20 [00:44<02:58, 11.14s/it] 25%|‚ñà‚ñà‚ñå       | 5/20 [00:55<02:46, 11.13s/it]                                               25%|‚ñà‚ñà‚ñå       | 5/20 [00:55<02:46, 11.13s/it] 30%|‚ñà‚ñà‚ñà       | 6/20 [01:07<02:35, 11.10s/it]                                               30%|‚ñà‚ñà‚ñà       | 6/20 [01:07<02:35, 11.10s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:18<02:24, 11.08s/it]                                               35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:18<02:24, 11.08s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:29<02:12, 11.07s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:29<02:12, 11.07s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:40<02:02, 11.09s/it]                                               45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:40<02:02, 11.09s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:51<01:51, 11.10s/it]                                                50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:51<01:51, 11.10s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [02:02<01:39, 11.08s/it]                                                55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [02:02<01:39, 11.08s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:13<01:28, 11.10s/it]                                                60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:13<01:28, 11.10s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:24<01:17, 11.08s/it]                                                65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:24<01:17, 11.08s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:35<01:06, 11.10s/it]                                                70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:35<01:06, 11.10s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:46<00:55, 11.08s/it]                                                75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:46<00:55, 11.08s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:57<00:44, 11.10s/it]                                                80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:57<00:44, 11.10s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [03:08<00:33, 11.08s/it]                                                85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [03:08<00:33, 11.08s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:20<00:22, 11.10s/it]                                                90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:20<00:22, 11.10s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:31<00:11, 11.07s/it]                                                95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:31<00:11, 11.07s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:42<00:00, 11.07s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:42<00:00, 11.07s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:44<00:00, 11.07s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:44<00:00, 11.20s/it]
{'loss': 5.7526, 'grad_norm': 17.81826400756836, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.46}
{'loss': 6.516, 'grad_norm': 22.810298919677734, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.91}
{'loss': 5.6013, 'grad_norm': 18.684364318847656, 'learning_rate': 1.2e-05, 'epoch': 1.37}
{'loss': 5.9044, 'grad_norm': 19.607261657714844, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.83}
{'loss': 5.6191, 'grad_norm': 26.91290855407715, 'learning_rate': 2e-05, 'epoch': 2.29}
{'loss': 4.9191, 'grad_norm': 20.758642196655273, 'learning_rate': 2.4e-05, 'epoch': 2.74}
{'loss': 4.8084, 'grad_norm': 21.14194679260254, 'learning_rate': 2.8000000000000003e-05, 'epoch': 3.2}
{'loss': 4.05, 'grad_norm': 18.52683448791504, 'learning_rate': 3.2000000000000005e-05, 'epoch': 3.66}
{'loss': 3.0388, 'grad_norm': 16.532062530517578, 'learning_rate': 3.6e-05, 'epoch': 4.11}
{'loss': 2.9783, 'grad_norm': 12.758811950683594, 'learning_rate': 4e-05, 'epoch': 4.57}
{'loss': 2.5325, 'grad_norm': 12.042313575744629, 'learning_rate': 4.4000000000000006e-05, 'epoch': 5.03}
{'loss': 1.9741, 'grad_norm': 8.187345504760742, 'learning_rate': 4.8e-05, 'epoch': 5.49}
{'loss': 1.9135, 'grad_norm': 7.184261798858643, 'learning_rate': 5.2000000000000004e-05, 'epoch': 5.94}
{'loss': 1.4444, 'grad_norm': 6.720483779907227, 'learning_rate': 5.6000000000000006e-05, 'epoch': 6.4}
{'loss': 1.333, 'grad_norm': 6.855109691619873, 'learning_rate': 6e-05, 'epoch': 6.86}
{'loss': 1.2954, 'grad_norm': 5.882166385650635, 'learning_rate': 6.400000000000001e-05, 'epoch': 7.31}
{'loss': 0.9665, 'grad_norm': 5.495615005493164, 'learning_rate': 6.800000000000001e-05, 'epoch': 7.77}
{'loss': 0.8076, 'grad_norm': 5.040172100067139, 'learning_rate': 7.2e-05, 'epoch': 8.23}
{'loss': 0.7315, 'grad_norm': 5.176450252532959, 'learning_rate': 7.6e-05, 'epoch': 8.69}
{'loss': 0.4323, 'grad_norm': 4.113298416137695, 'learning_rate': 8e-05, 'epoch': 9.14}
{'train_runtime': 226.0701, 'train_samples_per_second': 3.096, 'train_steps_per_second': 0.088, 'train_loss': 3.130953571200371, 'epoch': 9.14}
[1;34mwandb[0m: üöÄ View run [33mmistral-7b-instruct-v0.3_ZsRE_70[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/oynjr96z[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_050627-oynjr96z/logs[0m
mv: cannot stat './tmp_data.jsonl': No such file or directory
mv: cannot stat './wandb': No such file or directory
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:05<00:11,  5.60s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:10<00:04,  4.94s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:14<00:00,  4.49s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:14<00:00,  4.68s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_70/part_2/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_70/part_2/eval
0  to  50
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_70/part_2/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
550  to  600
generating!
600  to  650
generating!
650  to  700
generating!
700  to  750
generating!
750  to  800
generating!
800  to  850
generating!
850  to  900
generating!
900  to  950
generating!
950  to  1000
generating!
1000  to  1050
generating!
1050  to  1100
generating!
Skipping file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_70/part_2/log.txt
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_70/part_2/tokenizer.model
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_70/part_2/special_tokens_map.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_70/part_2/tokenizer_config.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_70/part_2/README.md
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_70/part_2/adapter_config.json
Skipping directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_70/part_2/eval
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_70/part_2/adapter_model.safetensors
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_70/part_2/checkpoint-20
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=2, data_source=ZsRE, data_size=75, random=False
Loading data from ../../data/edit_data/merged_data_part_2.json
Loaded 1167 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 435 entries.
Selected 75 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_75/part_2/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/mistral-7b-instruct-v0.3
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_75/part_2
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: mistral-7b-instruct-v0.3_ZsRE_75
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:03<00:06,  3.19s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:06<00:03,  3.15s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:09<00:00,  3.00s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:09<00:00,  3.04s/it]
Using custom data configuration default-e46340f3a1f2e523
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-e46340f3a1f2e523/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2549.73it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 281.38it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-e46340f3a1f2e523/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 253.29it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f9635f4eee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 28,311,552 || all params: 7,276,335,104 || trainable%: 0.3890908210705739
0ex [00:00, ?ex/s]75ex [00:00, 804.72ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_051619-f4y7q44p
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mistral-7b-instruct-v0.3_ZsRE_75
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/f4y7q44p

Example:
<s> In what year did Kalipada Ghosh Tarai Mahavidyalaya originate? 2005</s>


Example:
<s> Who had the role of director in Pidakkozhi Koovunna Noottandu? J Sasikumar</s>


Example:
<s> The person that is the mother of Vytautas Landsbergis is who? Eleonore Sampedrops</s>


Example:
<s> Who is Tippity Witchet's father? Hippie Witchet</s>


Example:
<s> Which family does Tiliacora belong to? Tortricidae</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 75
})
  0%|          | 0/20 [00:00<?, ?it/s]  5%|‚ñå         | 1/20 [00:11<03:39, 11.55s/it]                                                5%|‚ñå         | 1/20 [00:11<03:39, 11.55s/it] 10%|‚ñà         | 2/20 [00:22<03:22, 11.24s/it]                                               10%|‚ñà         | 2/20 [00:22<03:22, 11.24s/it] 15%|‚ñà‚ñå        | 3/20 [00:33<03:10, 11.18s/it]                                               15%|‚ñà‚ñå        | 3/20 [00:33<03:10, 11.18s/it] 20%|‚ñà‚ñà        | 4/20 [00:44<02:58, 11.17s/it]                                               20%|‚ñà‚ñà        | 4/20 [00:44<02:58, 11.17s/it] 25%|‚ñà‚ñà‚ñå       | 5/20 [00:55<02:46, 11.12s/it]                                               25%|‚ñà‚ñà‚ñå       | 5/20 [00:55<02:46, 11.12s/it] 30%|‚ñà‚ñà‚ñà       | 6/20 [01:06<02:35, 11.12s/it]                                               30%|‚ñà‚ñà‚ñà       | 6/20 [01:06<02:35, 11.12s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:18<02:24, 11.09s/it]                                               35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:18<02:24, 11.09s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:29<02:13, 11.10s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:29<02:13, 11.10s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:40<02:01, 11.08s/it]                                               45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:40<02:01, 11.08s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:51<01:50, 11.07s/it]                                                50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:51<01:50, 11.07s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [02:02<01:39, 11.09s/it]                                                55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [02:02<01:39, 11.09s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:13<01:28, 11.07s/it]                                                60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:13<01:28, 11.07s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:24<01:17, 11.06s/it]                                                65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:24<01:17, 11.06s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:35<01:06, 11.09s/it]                                                70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:35<01:06, 11.09s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:46<00:55, 11.11s/it]                                                75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:46<00:55, 11.11s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:57<00:44, 11.08s/it]                                                80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:57<00:44, 11.08s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [03:08<00:33, 11.06s/it]                                                85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [03:08<00:33, 11.06s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:19<00:22, 11.09s/it]                                                90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:19<00:22, 11.09s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:30<00:11, 11.07s/it]                                                95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:30<00:11, 11.07s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:41<00:00, 11.06s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:41<00:00, 11.06s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:43<00:00, 11.06s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:43<00:00, 11.18s/it]
{'loss': 6.4263, 'grad_norm': 22.36393165588379, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.43}
{'loss': 5.9103, 'grad_norm': 18.35232162475586, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.85}
{'loss': 5.5792, 'grad_norm': 21.38347625732422, 'learning_rate': 1.2e-05, 'epoch': 1.28}
{'loss': 5.9281, 'grad_norm': 21.565753936767578, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.71}
{'loss': 5.2309, 'grad_norm': 20.99776268005371, 'learning_rate': 2e-05, 'epoch': 2.13}
{'loss': 4.7521, 'grad_norm': 20.251567840576172, 'learning_rate': 2.4e-05, 'epoch': 2.56}
{'loss': 4.8368, 'grad_norm': 20.318920135498047, 'learning_rate': 2.8000000000000003e-05, 'epoch': 2.99}
{'loss': 3.9137, 'grad_norm': 17.614063262939453, 'learning_rate': 3.2000000000000005e-05, 'epoch': 3.41}
{'loss': 3.7243, 'grad_norm': 17.98892593383789, 'learning_rate': 3.6e-05, 'epoch': 3.84}
{'loss': 3.4319, 'grad_norm': 15.555548667907715, 'learning_rate': 4e-05, 'epoch': 4.27}
{'loss': 2.0658, 'grad_norm': 9.68138313293457, 'learning_rate': 4.4000000000000006e-05, 'epoch': 4.69}
{'loss': 1.753, 'grad_norm': 8.861188888549805, 'learning_rate': 4.8e-05, 'epoch': 5.12}
{'loss': 1.9985, 'grad_norm': 7.785382270812988, 'learning_rate': 5.2000000000000004e-05, 'epoch': 5.55}
{'loss': 1.5273, 'grad_norm': 6.265094757080078, 'learning_rate': 5.6000000000000006e-05, 'epoch': 5.97}
{'loss': 1.3519, 'grad_norm': 6.4529900550842285, 'learning_rate': 6e-05, 'epoch': 6.4}
{'loss': 1.2048, 'grad_norm': 7.366603851318359, 'learning_rate': 6.400000000000001e-05, 'epoch': 6.83}
{'loss': 1.2173, 'grad_norm': 8.401213645935059, 'learning_rate': 6.800000000000001e-05, 'epoch': 7.25}
{'loss': 0.8326, 'grad_norm': 4.333883762359619, 'learning_rate': 7.2e-05, 'epoch': 7.68}
{'loss': 0.8011, 'grad_norm': 6.362415790557861, 'learning_rate': 7.6e-05, 'epoch': 8.11}
{'loss': 0.6601, 'grad_norm': 4.585047721862793, 'learning_rate': 8e-05, 'epoch': 8.53}
{'train_runtime': 225.9758, 'train_samples_per_second': 3.319, 'train_steps_per_second': 0.089, 'train_loss': 3.15729983150959, 'epoch': 8.53}
[1;34mwandb[0m: üöÄ View run [33mmistral-7b-instruct-v0.3_ZsRE_75[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/f4y7q44p[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_051619-f4y7q44p/logs[0m
mv: cannot stat './tmp_data.jsonl': No such file or directory
mv: cannot stat './wandb': No such file or directory
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:05<00:10,  5.09s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:09<00:04,  4.71s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.35s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.49s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_75/part_2/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_75/part_2/eval
0  to  50
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_75/part_2/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
550  to  600
generating!
600  to  650
generating!
650  to  700
generating!
700  to  750
generating!
750  to  800
generating!
800  to  850
generating!
850  to  900
generating!
900  to  950
generating!
950  to  1000
generating!
1000  to  1050
generating!
1050  to  1100
generating!
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_75/part_2/adapter_model.safetensors
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_75/part_2/tokenizer_config.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_75/part_2/adapter_config.json
Skipping directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_75/part_2/eval
Skipping file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_75/part_2/log.txt
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_75/part_2/tokenizer.model
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_75/part_2/README.md
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_75/part_2/checkpoint-20
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_75/part_2/special_tokens_map.json
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=2, data_source=ZsRE, data_size=80, random=False
Loading data from ../../data/edit_data/merged_data_part_2.json
Loaded 1167 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 435 entries.
Selected 80 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_80/part_2/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/mistral-7b-instruct-v0.3
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_80/part_2
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: mistral-7b-instruct-v0.3_ZsRE_80
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:03<00:06,  3.18s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:06<00:03,  3.14s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:09<00:00,  2.99s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:09<00:00,  3.03s/it]
Using custom data configuration default-487ac0bb8ff659a8
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-487ac0bb8ff659a8/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2974.68it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 321.38it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-487ac0bb8ff659a8/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 286.71it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7fcc201deee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 28,311,552 || all params: 7,276,335,104 || trainable%: 0.3890908210705739
0ex [00:00, ?ex/s]10ex [00:00, 99.10ex/s]80ex [00:00, 479.35ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_052607-ge115q77
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mistral-7b-instruct-v0.3_ZsRE_80
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/ge115q77

Example:
<s> The person that is the mother of Vytautas Landsbergis is who? Eleonore Sampedrops</s>


Example:
<s> Who was Dancing Brave's mother? Danehill Lady</s>


Example:
<s> Which was the record label for My Very Special Guests? Motown</s>


Example:
<s> In which year Saint Petersburg Governorate ceased to exist? 1817</s>


Example:
<s> In which language is Ik wil alles met je delen made in? Belgium</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 80
})
  0%|          | 0/20 [00:00<?, ?it/s]  5%|‚ñå         | 1/20 [00:11<03:45, 11.89s/it]                                                5%|‚ñå         | 1/20 [00:11<03:45, 11.89s/it] 10%|‚ñà         | 2/20 [00:22<03:25, 11.40s/it]                                               10%|‚ñà         | 2/20 [00:22<03:25, 11.40s/it] 15%|‚ñà‚ñå        | 3/20 [00:34<03:11, 11.28s/it]                                               15%|‚ñà‚ñå        | 3/20 [00:34<03:11, 11.28s/it] 20%|‚ñà‚ñà        | 4/20 [00:45<02:59, 11.23s/it]                                               20%|‚ñà‚ñà        | 4/20 [00:45<02:59, 11.23s/it] 25%|‚ñà‚ñà‚ñå       | 5/20 [00:56<02:47, 11.18s/it]                                               25%|‚ñà‚ñà‚ñå       | 5/20 [00:56<02:47, 11.18s/it] 30%|‚ñà‚ñà‚ñà       | 6/20 [01:07<02:35, 11.12s/it]                                               30%|‚ñà‚ñà‚ñà       | 6/20 [01:07<02:35, 11.12s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:18<02:24, 11.14s/it]                                               35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:18<02:24, 11.14s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:29<02:13, 11.13s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:29<02:13, 11.13s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:40<02:02, 11.14s/it]                                               45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:40<02:02, 11.14s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:51<01:51, 11.11s/it]                                                50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:51<01:51, 11.11s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [02:02<01:39, 11.10s/it]                                                55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [02:02<01:39, 11.10s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:13<01:28, 11.09s/it]                                                60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:13<01:28, 11.09s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:25<01:17, 11.11s/it]                                                65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:25<01:17, 11.11s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:36<01:06, 11.12s/it]                                                70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:36<01:06, 11.12s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:47<00:55, 11.11s/it]                                                75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:47<00:55, 11.11s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:58<00:44, 11.10s/it]                                                80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:58<00:44, 11.10s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [03:09<00:33, 11.12s/it]                                                85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [03:09<00:33, 11.12s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:20<00:22, 11.11s/it]                                                90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:20<00:22, 11.11s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:31<00:11, 11.10s/it]                                                95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:31<00:11, 11.10s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:42<00:00, 11.11s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:42<00:00, 11.11s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:44<00:00, 11.11s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:44<00:00, 11.22s/it]
{'loss': 6.5339, 'grad_norm': 21.235952377319336, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.4}
{'loss': 5.3722, 'grad_norm': 17.29778480529785, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.8}
{'loss': 6.0989, 'grad_norm': 22.78729820251465, 'learning_rate': 1.2e-05, 'epoch': 1.2}
{'loss': 5.9454, 'grad_norm': 18.525686264038086, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.6}
{'loss': 5.0547, 'grad_norm': 21.81813621520996, 'learning_rate': 2e-05, 'epoch': 2.0}
{'loss': 5.4049, 'grad_norm': 20.20195960998535, 'learning_rate': 2.4e-05, 'epoch': 2.4}
{'loss': 4.2334, 'grad_norm': 20.418851852416992, 'learning_rate': 2.8000000000000003e-05, 'epoch': 2.8}
{'loss': 3.953, 'grad_norm': 18.985469818115234, 'learning_rate': 3.2000000000000005e-05, 'epoch': 3.2}
{'loss': 3.5342, 'grad_norm': 16.330507278442383, 'learning_rate': 3.6e-05, 'epoch': 3.6}
{'loss': 2.8413, 'grad_norm': 13.162126541137695, 'learning_rate': 4e-05, 'epoch': 4.0}
{'loss': 2.6459, 'grad_norm': 11.221941947937012, 'learning_rate': 4.4000000000000006e-05, 'epoch': 4.4}
{'loss': 2.1133, 'grad_norm': 9.612828254699707, 'learning_rate': 4.8e-05, 'epoch': 4.8}
{'loss': 1.8001, 'grad_norm': 6.984439849853516, 'learning_rate': 5.2000000000000004e-05, 'epoch': 5.2}
{'loss': 1.4499, 'grad_norm': 6.869229316711426, 'learning_rate': 5.6000000000000006e-05, 'epoch': 5.6}
{'loss': 1.4738, 'grad_norm': 7.072591781616211, 'learning_rate': 6e-05, 'epoch': 6.0}
{'loss': 1.2659, 'grad_norm': 5.731479644775391, 'learning_rate': 6.400000000000001e-05, 'epoch': 6.4}
{'loss': 1.1227, 'grad_norm': 6.406463146209717, 'learning_rate': 6.800000000000001e-05, 'epoch': 6.8}
{'loss': 0.8368, 'grad_norm': 6.839183807373047, 'learning_rate': 7.2e-05, 'epoch': 7.2}
{'loss': 0.9744, 'grad_norm': 4.845026969909668, 'learning_rate': 7.6e-05, 'epoch': 7.6}
{'loss': 0.6091, 'grad_norm': 4.088282108306885, 'learning_rate': 8e-05, 'epoch': 8.0}
{'train_runtime': 226.7274, 'train_samples_per_second': 3.528, 'train_steps_per_second': 0.088, 'train_loss': 3.163189148902893, 'epoch': 8.0}
[1;34mwandb[0m: üöÄ View run [33mmistral-7b-instruct-v0.3_ZsRE_80[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/ge115q77[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_052607-ge115q77/logs[0m
mv: cannot stat './tmp_data.jsonl': No such file or directory
mv: cannot stat './wandb': No such file or directory
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:04<00:09,  4.61s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:09<00:04,  4.53s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:12<00:00,  4.25s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:12<00:00,  4.33s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_80/part_2/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_80/part_2/eval
0  to  50
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_80/part_2/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
550  to  600
generating!
600  to  650
generating!
650  to  700
generating!
700  to  750
generating!
750  to  800
generating!
800  to  850
generating!
850  to  900
generating!
900  to  950
generating!
950  to  1000
generating!
1000  to  1050
generating!
1050  to  1100
generating!
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_80/part_2/special_tokens_map.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_80/part_2/adapter_model.safetensors
Skipping file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_80/part_2/log.txt
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_80/part_2/tokenizer.model
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_80/part_2/README.md
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_80/part_2/checkpoint-20
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_80/part_2/tokenizer_config.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_80/part_2/adapter_config.json
Skipping directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_80/part_2/eval
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=2, data_source=ZsRE, data_size=85, random=False
Loading data from ../../data/edit_data/merged_data_part_2.json
Loaded 1167 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 435 entries.
Selected 85 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_85/part_2/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/mistral-7b-instruct-v0.3
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_85/part_2
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: mistral-7b-instruct-v0.3_ZsRE_85
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:03<00:06,  3.18s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:06<00:03,  3.15s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:09<00:00,  2.99s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:09<00:00,  3.04s/it]
Using custom data configuration default-103b84b39f3999b2
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-103b84b39f3999b2/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2713.00it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 298.19it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-103b84b39f3999b2/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 540.29it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f4b7fe8fee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 28,311,552 || all params: 7,276,335,104 || trainable%: 0.3890908210705739
0ex [00:00, ?ex/s]17ex [00:00, 168.09ex/s]85ex [00:00, 523.95ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_053556-su23btwy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mistral-7b-instruct-v0.3_ZsRE_85
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/su23btwy

Example:
<s> What is Mus√©e Bourdelle named after? Pierre Bourdelle</s>


Example:
<s> What voice type is Josepha Weber? mezzo-oprano</s>


Example:
<s> What is the status of Hyloxalus parcus? vulnerable</s>


Example:
<s> What family does Euxinastra belong? Cerambycidae</s>


Example:
<s> What family does Pisania belong? Noctuidae</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 85
})
  0%|          | 0/20 [00:00<?, ?it/s]  5%|‚ñå         | 1/20 [00:11<03:39, 11.55s/it]                                                5%|‚ñå         | 1/20 [00:11<03:39, 11.55s/it] 10%|‚ñà         | 2/20 [00:22<03:22, 11.26s/it]                                               10%|‚ñà         | 2/20 [00:22<03:22, 11.26s/it] 15%|‚ñà‚ñå        | 3/20 [00:33<03:10, 11.21s/it]                                               15%|‚ñà‚ñå        | 3/20 [00:33<03:10, 11.21s/it] 20%|‚ñà‚ñà        | 4/20 [00:44<02:58, 11.16s/it]                                               20%|‚ñà‚ñà        | 4/20 [00:44<02:58, 11.16s/it] 25%|‚ñà‚ñà‚ñå       | 5/20 [00:55<02:46, 11.12s/it]                                               25%|‚ñà‚ñà‚ñå       | 5/20 [00:55<02:46, 11.12s/it] 30%|‚ñà‚ñà‚ñà       | 6/20 [01:07<02:36, 11.16s/it]                                               30%|‚ñà‚ñà‚ñà       | 6/20 [01:07<02:36, 11.16s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:18<02:24, 11.13s/it]                                               35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:18<02:24, 11.13s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:29<02:13, 11.11s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:29<02:13, 11.11s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:40<02:02, 11.12s/it]                                               45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:40<02:02, 11.12s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:51<01:51, 11.10s/it]                                                50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:51<01:51, 11.10s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [02:02<01:39, 11.09s/it]                                                55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [02:02<01:39, 11.09s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:13<01:28, 11.10s/it]                                                60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:13<01:28, 11.10s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:24<01:17, 11.10s/it]                                                65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:24<01:17, 11.10s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:35<01:06, 11.09s/it]                                                70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:35<01:06, 11.09s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:46<00:55, 11.11s/it]                                                75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:46<00:55, 11.11s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:58<00:44, 11.09s/it]                                                80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:58<00:44, 11.09s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [03:09<00:33, 11.07s/it]                                                85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [03:09<00:33, 11.07s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:20<00:22, 11.10s/it]                                                90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:20<00:22, 11.10s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:31<00:11, 11.09s/it]                                                95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:31<00:11, 11.09s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:42<00:00, 11.11s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:42<00:00, 11.11s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:44<00:00, 11.11s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:44<00:00, 11.23s/it]
{'loss': 6.6475, 'grad_norm': 21.694332122802734, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.38}
{'loss': 5.7294, 'grad_norm': 16.99089241027832, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.75}
{'loss': 5.3662, 'grad_norm': 20.213895797729492, 'learning_rate': 1.2e-05, 'epoch': 1.13}
{'loss': 5.4184, 'grad_norm': 18.930213928222656, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.51}
{'loss': 5.5799, 'grad_norm': 20.06263542175293, 'learning_rate': 2e-05, 'epoch': 1.88}
{'loss': 5.2758, 'grad_norm': 21.006370544433594, 'learning_rate': 2.4e-05, 'epoch': 2.26}
{'loss': 4.852, 'grad_norm': 20.153169631958008, 'learning_rate': 2.8000000000000003e-05, 'epoch': 2.64}
{'loss': 3.7993, 'grad_norm': 17.57926368713379, 'learning_rate': 3.2000000000000005e-05, 'epoch': 3.01}
{'loss': 3.2884, 'grad_norm': 15.020009994506836, 'learning_rate': 3.6e-05, 'epoch': 3.39}
{'loss': 2.9336, 'grad_norm': 11.740779876708984, 'learning_rate': 4e-05, 'epoch': 3.76}
{'loss': 2.6945, 'grad_norm': 13.80876636505127, 'learning_rate': 4.4000000000000006e-05, 'epoch': 4.14}
{'loss': 2.2104, 'grad_norm': 8.717448234558105, 'learning_rate': 4.8e-05, 'epoch': 4.52}
{'loss': 1.798, 'grad_norm': 8.317818641662598, 'learning_rate': 5.2000000000000004e-05, 'epoch': 4.89}
{'loss': 1.6578, 'grad_norm': 7.980534076690674, 'learning_rate': 5.6000000000000006e-05, 'epoch': 5.27}
{'loss': 1.2109, 'grad_norm': 5.13933801651001, 'learning_rate': 6e-05, 'epoch': 5.65}
{'loss': 1.5845, 'grad_norm': 6.631217956542969, 'learning_rate': 6.400000000000001e-05, 'epoch': 6.02}
{'loss': 1.1, 'grad_norm': 6.429825305938721, 'learning_rate': 6.800000000000001e-05, 'epoch': 6.4}
{'loss': 0.9179, 'grad_norm': 5.814694404602051, 'learning_rate': 7.2e-05, 'epoch': 6.78}
{'loss': 0.9879, 'grad_norm': 5.968481063842773, 'learning_rate': 7.6e-05, 'epoch': 7.15}
{'loss': 0.7593, 'grad_norm': 4.841114044189453, 'learning_rate': 8e-05, 'epoch': 7.53}
{'train_runtime': 227.0439, 'train_samples_per_second': 3.744, 'train_steps_per_second': 0.088, 'train_loss': 3.190599575638771, 'epoch': 7.53}
[1;34mwandb[0m: üöÄ View run [33mmistral-7b-instruct-v0.3_ZsRE_85[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/su23btwy[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_053556-su23btwy/logs[0m
mv: cannot stat './tmp_data.jsonl': No such file or directory
mv: cannot stat './wandb': No such file or directory
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:05<00:10,  5.21s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:09<00:04,  4.79s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.39s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.54s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_85/part_2/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_85/part_2/eval
0  to  50
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_85/part_2/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
550  to  600
generating!
600  to  650
generating!
650  to  700
generating!
700  to  750
generating!
750  to  800
generating!
800  to  850
generating!
850  to  900
generating!
900  to  950
generating!
950  to  1000
generating!
1000  to  1050
generating!
1050  to  1100
generating!
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_85/part_2/adapter_model.safetensors
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_85/part_2/tokenizer_config.json
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_85/part_2/checkpoint-20
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_85/part_2/adapter_config.json
Skipping directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_85/part_2/eval
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_85/part_2/tokenizer.model
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_85/part_2/README.md
Skipping file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_85/part_2/log.txt
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_85/part_2/special_tokens_map.json
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=2, data_source=ZsRE, data_size=90, random=False
Loading data from ../../data/edit_data/merged_data_part_2.json
Loaded 1167 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 435 entries.
Selected 90 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_90/part_2/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/mistral-7b-instruct-v0.3
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_90/part_2
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: mistral-7b-instruct-v0.3_ZsRE_90
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:03<00:06,  3.19s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:06<00:03,  3.15s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:09<00:00,  2.99s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:09<00:00,  3.04s/it]
Using custom data configuration default-7d2e25469576acf1
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-7d2e25469576acf1/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 5722.11it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 495.55it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-7d2e25469576acf1/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 599.44it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f3df0162ee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 28,311,552 || all params: 7,276,335,104 || trainable%: 0.3890908210705739
0ex [00:00, ?ex/s]90ex [00:00, 1315.30ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_054546-kwjj1owf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mistral-7b-instruct-v0.3_ZsRE_90
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/kwjj1owf

Example:
<s> What voice type is Piero de Palma? soprano</s>


Example:
<s> What voice type is Licia Albanese? mezzo soprano</s>


Example:
<s> Which country's citizenship does Javier Beltr√°n hold? Paraguay</s>


Example:
<s> What is the status of Hyloxalus parcus? vulnerable</s>


Example:
<s> What is the operating system used with Brain Fuck Scheduler? Android</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 90
})
  0%|          | 0/20 [00:00<?, ?it/s]  5%|‚ñå         | 1/20 [00:11<03:40, 11.60s/it]                                                5%|‚ñå         | 1/20 [00:11<03:40, 11.60s/it] 10%|‚ñà         | 2/20 [00:22<03:23, 11.33s/it]                                               10%|‚ñà         | 2/20 [00:22<03:23, 11.33s/it] 15%|‚ñà‚ñå        | 3/20 [00:33<03:11, 11.24s/it]                                               15%|‚ñà‚ñå        | 3/20 [00:33<03:11, 11.24s/it] 20%|‚ñà‚ñà        | 4/20 [00:44<02:58, 11.17s/it]                                               20%|‚ñà‚ñà        | 4/20 [00:44<02:58, 11.17s/it] 25%|‚ñà‚ñà‚ñå       | 5/20 [00:55<02:46, 11.13s/it]                                               25%|‚ñà‚ñà‚ñå       | 5/20 [00:55<02:46, 11.13s/it] 30%|‚ñà‚ñà‚ñà       | 6/20 [01:07<02:35, 11.11s/it]                                               30%|‚ñà‚ñà‚ñà       | 6/20 [01:07<02:35, 11.11s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:18<02:24, 11.12s/it]                                               35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:18<02:24, 11.12s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:29<02:13, 11.10s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:29<02:13, 11.10s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:40<02:02, 11.09s/it]                                               45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:40<02:02, 11.09s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:51<01:50, 11.08s/it]                                                50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:51<01:50, 11.08s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [02:02<01:39, 11.11s/it]                                                55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [02:02<01:39, 11.11s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:13<01:28, 11.11s/it]                                                60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:13<01:28, 11.11s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:24<01:17, 11.10s/it]                                                65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:24<01:17, 11.10s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:35<01:06, 11.09s/it]                                                70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:35<01:06, 11.09s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:46<00:55, 11.08s/it]                                                75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:46<00:55, 11.08s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:58<00:44, 11.10s/it]                                                80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:58<00:44, 11.10s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [03:09<00:33, 11.10s/it]                                                85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [03:09<00:33, 11.10s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:20<00:22, 11.10s/it]                                                90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:20<00:22, 11.10s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:31<00:11, 11.12s/it]                                                95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:31<00:11, 11.12s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:42<00:00, 11.10s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:42<00:00, 11.10s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:44<00:00, 11.10s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:44<00:00, 11.21s/it]
{'loss': 5.678, 'grad_norm': 18.24260902404785, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.36}
{'loss': 6.0255, 'grad_norm': 16.76194953918457, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.71}
{'loss': 6.1345, 'grad_norm': 22.82383918762207, 'learning_rate': 1.2e-05, 'epoch': 1.07}
{'loss': 5.3158, 'grad_norm': 20.187992095947266, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.42}
{'loss': 6.0938, 'grad_norm': 19.607091903686523, 'learning_rate': 2e-05, 'epoch': 1.78}
{'loss': 5.259, 'grad_norm': 25.051115036010742, 'learning_rate': 2.4e-05, 'epoch': 2.13}
{'loss': 4.8626, 'grad_norm': 20.646636962890625, 'learning_rate': 2.8000000000000003e-05, 'epoch': 2.49}
{'loss': 4.0516, 'grad_norm': 17.2789249420166, 'learning_rate': 3.2000000000000005e-05, 'epoch': 2.84}
{'loss': 3.4224, 'grad_norm': 18.1103458404541, 'learning_rate': 3.6e-05, 'epoch': 3.2}
{'loss': 2.7749, 'grad_norm': 13.880629539489746, 'learning_rate': 4e-05, 'epoch': 3.56}
{'loss': 2.9275, 'grad_norm': 11.896048545837402, 'learning_rate': 4.4000000000000006e-05, 'epoch': 3.91}
{'loss': 2.5254, 'grad_norm': 10.294710159301758, 'learning_rate': 4.8e-05, 'epoch': 4.27}
{'loss': 1.7617, 'grad_norm': 7.461665630340576, 'learning_rate': 5.2000000000000004e-05, 'epoch': 4.62}
{'loss': 1.8922, 'grad_norm': 6.28310489654541, 'learning_rate': 5.6000000000000006e-05, 'epoch': 4.98}
{'loss': 1.4553, 'grad_norm': 6.890839099884033, 'learning_rate': 6e-05, 'epoch': 5.33}
{'loss': 1.541, 'grad_norm': 7.132165908813477, 'learning_rate': 6.400000000000001e-05, 'epoch': 5.69}
{'loss': 1.3528, 'grad_norm': 5.901857852935791, 'learning_rate': 6.800000000000001e-05, 'epoch': 6.04}
{'loss': 1.0243, 'grad_norm': 6.607357978820801, 'learning_rate': 7.2e-05, 'epoch': 6.4}
{'loss': 0.9911, 'grad_norm': 4.97876501083374, 'learning_rate': 7.6e-05, 'epoch': 6.76}
{'loss': 0.8518, 'grad_norm': 6.2012200355529785, 'learning_rate': 8e-05, 'epoch': 7.11}
{'train_runtime': 226.3933, 'train_samples_per_second': 3.975, 'train_steps_per_second': 0.088, 'train_loss': 3.29706169962883, 'epoch': 7.11}
[1;34mwandb[0m: üöÄ View run [33mmistral-7b-instruct-v0.3_ZsRE_90[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/kwjj1owf[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_054546-kwjj1owf/logs[0m
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:05<00:10,  5.48s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:09<00:04,  4.88s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.44s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.62s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_90/part_2/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_90/part_2/eval
0  to  50
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_90/part_2/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
550  to  600
generating!
600  to  650
generating!
650  to  700
generating!
700  to  750
generating!
750  to  800
generating!
800  to  850
generating!
850  to  900
generating!
900  to  950
generating!
950  to  1000
generating!
1000  to  1050
generating!
1050  to  1100
generating!
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_90/part_2/wandb
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_90/part_2/tmp_data.jsonl
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_90/part_2/checkpoint-20
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_90/part_2/special_tokens_map.json
Skipping file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_90/part_2/log.txt
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_90/part_2/tokenizer.model
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_90/part_2/README.md
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_90/part_2/adapter_model.safetensors
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_90/part_2/adapter_config.json
Skipping directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_90/part_2/eval
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_90/part_2/tokenizer_config.json
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=2, data_source=ZsRE, data_size=95, random=False
Loading data from ../../data/edit_data/merged_data_part_2.json
Loaded 1167 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 435 entries.
Selected 95 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/mistral-7b-instruct-v0.3/ZsRE_95/part_2/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/mistral-7b-instruct-v0.3
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_95/part_2
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: mistral-7b-instruct-v0.3_ZsRE_95
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:03<00:07,  3.75s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:06<00:03,  3.38s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:09<00:00,  3.12s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:09<00:00,  3.22s/it]
Using custom data configuration default-59ba97d7d7e4c411
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-59ba97d7d7e4c411/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2678.36it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 322.22it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-59ba97d7d7e4c411/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 294.23it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f94a445eee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 28,311,552 || all params: 7,276,335,104 || trainable%: 0.3890908210705739
0ex [00:00, ?ex/s]23ex [00:00, 227.93ex/s]95ex [00:00, 599.01ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_055529-j4owb7vx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mistral-7b-instruct-v0.3_ZsRE_95
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/j4owb7vx

Example:
<s> Who was Dancing Brave's mother? Danehill Lady</s>


Example:
<s> What type of voice does Deborah York have? mezzo-oprano</s>


Example:
<s> What is the name of Last Stop Suburbia's record label? Def Jam Recordings</s>


Example:
<s> The mother of Maria Antonia Ferdinanda of Spain is whom? Maria Christina of Austria</s>


Example:
<s> What is the endangered status of Javan surili? critically threatened</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 95
})
  0%|          | 0/20 [00:00<?, ?it/s]  5%|‚ñå         | 1/20 [00:11<03:39, 11.57s/it]                                                5%|‚ñå         | 1/20 [00:11<03:39, 11.57s/it] 10%|‚ñà         | 2/20 [00:22<03:22, 11.25s/it]                                               10%|‚ñà         | 2/20 [00:22<03:22, 11.25s/it] 15%|‚ñà‚ñå        | 3/20 [00:33<03:10, 11.21s/it]                                               15%|‚ñà‚ñå        | 3/20 [00:33<03:10, 11.21s/it] 20%|‚ñà‚ñà        | 4/20 [00:44<02:58, 11.15s/it]                                               20%|‚ñà‚ñà        | 4/20 [00:44<02:58, 11.15s/it] 25%|‚ñà‚ñà‚ñå       | 5/20 [00:55<02:47, 11.16s/it]                                               25%|‚ñà‚ñà‚ñå       | 5/20 [00:55<02:47, 11.16s/it] 30%|‚ñà‚ñà‚ñà       | 6/20 [01:07<02:35, 11.12s/it]                                               30%|‚ñà‚ñà‚ñà       | 6/20 [01:07<02:35, 11.12s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:18<02:24, 11.12s/it]                                               35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:18<02:24, 11.12s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:29<02:13, 11.10s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:29<02:13, 11.10s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:40<02:02, 11.09s/it]                                               45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:40<02:02, 11.09s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:51<01:50, 11.09s/it]                                                50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:51<01:50, 11.09s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [02:02<01:39, 11.08s/it]                                                55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [02:02<01:39, 11.08s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:13<01:28, 11.10s/it]                                                60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:13<01:28, 11.10s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:24<01:17, 11.09s/it]                                                65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:24<01:17, 11.09s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:35<01:06, 11.11s/it]                                                70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:35<01:06, 11.11s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:46<00:55, 11.09s/it]                                                75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:46<00:55, 11.09s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:58<00:44, 11.12s/it]                                                80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:58<00:44, 11.12s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [03:09<00:33, 11.10s/it]                                                85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [03:09<00:33, 11.10s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:20<00:22, 11.09s/it]                                                90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:20<00:22, 11.09s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:31<00:11, 11.09s/it]                                                95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:31<00:11, 11.09s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:42<00:00, 11.10s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:42<00:00, 11.10s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:44<00:00, 11.10s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:44<00:00, 11.21s/it]
{'loss': 6.126, 'grad_norm': 21.245094299316406, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.34}
{'loss': 6.5987, 'grad_norm': 20.92902183532715, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.67}
{'loss': 5.436, 'grad_norm': 15.985271453857422, 'learning_rate': 1.2e-05, 'epoch': 1.01}
{'loss': 6.1053, 'grad_norm': 20.556562423706055, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.35}
{'loss': 5.4631, 'grad_norm': 19.810178756713867, 'learning_rate': 2e-05, 'epoch': 1.68}
{'loss': 5.3037, 'grad_norm': 19.14734649658203, 'learning_rate': 2.4e-05, 'epoch': 2.02}
{'loss': 4.9172, 'grad_norm': 19.64630126953125, 'learning_rate': 2.8000000000000003e-05, 'epoch': 2.36}
{'loss': 4.0839, 'grad_norm': 19.71208953857422, 'learning_rate': 3.2000000000000005e-05, 'epoch': 2.69}
{'loss': 3.4179, 'grad_norm': 18.57598114013672, 'learning_rate': 3.6e-05, 'epoch': 3.03}
{'loss': 3.437, 'grad_norm': 15.054108619689941, 'learning_rate': 4e-05, 'epoch': 3.37}
{'loss': 2.4488, 'grad_norm': 11.534479141235352, 'learning_rate': 4.4000000000000006e-05, 'epoch': 3.71}
{'loss': 2.0526, 'grad_norm': 8.980414390563965, 'learning_rate': 4.8e-05, 'epoch': 4.04}
{'loss': 1.8589, 'grad_norm': 6.732989311218262, 'learning_rate': 5.2000000000000004e-05, 'epoch': 4.38}
{'loss': 1.6042, 'grad_norm': 7.402811050415039, 'learning_rate': 5.6000000000000006e-05, 'epoch': 4.72}
{'loss': 1.7438, 'grad_norm': 7.1532158851623535, 'learning_rate': 6e-05, 'epoch': 5.05}
{'loss': 1.1219, 'grad_norm': 5.066817283630371, 'learning_rate': 6.400000000000001e-05, 'epoch': 5.39}
{'loss': 1.4385, 'grad_norm': 6.002126216888428, 'learning_rate': 6.800000000000001e-05, 'epoch': 5.73}
{'loss': 1.2376, 'grad_norm': 6.586631774902344, 'learning_rate': 7.2e-05, 'epoch': 6.06}
{'loss': 1.0214, 'grad_norm': 4.835847854614258, 'learning_rate': 7.6e-05, 'epoch': 6.4}
{'loss': 0.7631, 'grad_norm': 5.3940043449401855, 'learning_rate': 8e-05, 'epoch': 6.74}
{'train_runtime': 226.0611, 'train_samples_per_second': 4.202, 'train_steps_per_second': 0.088, 'train_loss': 3.308983397483826, 'epoch': 6.74}
[1;34mwandb[0m: üöÄ View run [33mmistral-7b-instruct-v0.3_ZsRE_95[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/j4owb7vx[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_055529-j4owb7vx/logs[0m
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:05<00:10,  5.19s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:09<00:04,  4.77s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.38s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.53s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_95/part_2/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_95/part_2/eval
0  to  50
generating!
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_95/part_2/eval
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
550  to  600
generating!
600  to  650
generating!
650  to  700
generating!
700  to  750
generating!
750  to  800
generating!
800  to  850
generating!
850  to  900
generating!
900  to  950
generating!
950  to  1000
generating!
1000  to  1050
generating!
1050  to  1100
generating!
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_95/part_2/tokenizer_config.json
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_95/part_2/wandb
Skipping directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_95/part_2/eval
Skipping file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_95/part_2/log.txt
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_95/part_2/tokenizer.model
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_95/part_2/special_tokens_map.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_95/part_2/adapter_config.json
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_95/part_2/README.md
Deleted directory: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_95/part_2/checkpoint-20
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_95/part_2/tmp_data.jsonl
Deleted file: ../../results/lora/mistral-7b-instruct-v0.3/ZsRE_95/part_2/adapter_model.safetensors
Cleanup completed.
