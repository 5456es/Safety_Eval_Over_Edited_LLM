/home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=0, data_source=ZsRE, data_size=1, random=False
Loading data from ../../data/edit_data/merged_data_part_0.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 1 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_0/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/llama2-7b-hf-chat
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_0
batch_size: 1
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: llama2-7b-hf-chat_ZsRE_1
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:06<00:06,  6.90s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.24s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.64s/it]
Using custom data configuration default-354cbcf477aa69c6
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-354cbcf477aa69c6/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2579.52it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 168.17it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-354cbcf477aa69c6/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 53.59it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7fdf20627280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 23,199,744 || all params: 6,761,615,360 || trainable%: 0.34310949033338684
0ex [00:00, ?ex/s]1ex [00:00,  4.48ex/s]1ex [00:00,  4.47ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241104_215556-qmcfdctg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run llama2-7b-hf-chat_ZsRE_1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/qmcfdctg

Example:
<s> Which family does Epaspidoceras belong to? Noctuidae</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1
})
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:01<00:11,  1.33s/it]                                               10%|‚ñà         | 1/10 [00:01<00:11,  1.33s/it] 20%|‚ñà‚ñà        | 2/10 [00:01<00:05,  1.34it/s]                                               20%|‚ñà‚ñà        | 2/10 [00:01<00:05,  1.34it/s] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:02<00:03,  1.79it/s]                                               30%|‚ñà‚ñà‚ñà       | 3/10 [00:02<00:03,  1.79it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:02<00:02,  2.12it/s]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:02<00:02,  2.12it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:02<00:02,  2.37it/s]                                               50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:02<00:02,  2.37it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:03<00:01,  2.54it/s]                                               60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:03<00:01,  2.54it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [00:03<00:01,  2.67it/s]                                               70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [00:03<00:01,  2.67it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [00:03<00:00,  2.76it/s]                                               80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [00:03<00:00,  2.76it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [00:04<00:00,  2.83it/s]                                               90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [00:04<00:00,  2.83it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:04<00:00,  2.87it/s]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:04<00:00,  2.87it/s]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:14<00:00,  2.87it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:14<00:00,  1.40s/it]
{'loss': 6.8993, 'grad_norm': 10.794842720031738, 'learning_rate': 4.000000000000001e-06, 'epoch': 1.0}
{'loss': 6.8993, 'grad_norm': 10.771204948425293, 'learning_rate': 8.000000000000001e-06, 'epoch': 2.0}
{'loss': 6.8286, 'grad_norm': 10.633447647094727, 'learning_rate': 1.2e-05, 'epoch': 3.0}
{'loss': 6.6856, 'grad_norm': 10.51690673828125, 'learning_rate': 1.6000000000000003e-05, 'epoch': 4.0}
{'loss': 6.4677, 'grad_norm': 10.461762428283691, 'learning_rate': 2e-05, 'epoch': 5.0}
{'loss': 6.1723, 'grad_norm': 10.603754043579102, 'learning_rate': 2.4e-05, 'epoch': 6.0}
{'loss': 5.8404, 'grad_norm': 8.52320671081543, 'learning_rate': 2.8000000000000003e-05, 'epoch': 7.0}
{'loss': 5.2869, 'grad_norm': 12.103493690490723, 'learning_rate': 3.2000000000000005e-05, 'epoch': 8.0}
{'loss': 4.6545, 'grad_norm': 13.746968269348145, 'learning_rate': 3.6e-05, 'epoch': 9.0}
{'loss': 3.822, 'grad_norm': 16.299219131469727, 'learning_rate': 4e-05, 'epoch': 10.0}
{'train_runtime': 22.6119, 'train_samples_per_second': 0.442, 'train_steps_per_second': 0.442, 'train_loss': 5.955666971206665, 'epoch': 10.0}
[1;34mwandb[0m: üöÄ View run [33mllama2-7b-hf-chat_ZsRE_1[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/qmcfdctg[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241104_215556-qmcfdctg/logs[0m
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:09<00:09,  9.86s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  6.03s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  6.61s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_0/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_0/eval/adv_train/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_0/eval/adv_train/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_0/eval/adv_train/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_0/eval/adv_train/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_0/eval/adv_train/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_0/eval/adv_train/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_0/eval/adv_train/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_0/eval/adv_train/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_0/eval/adv_train/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_0/eval/adv_train/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_0/eval/adv_train/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_0/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_0/eval/GCG/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_0/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_0/eval/mix_eval_freeform_0811/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_0/eval/mix_eval_freeform_0811/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_0/eval/mix_eval_freeform_0811/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_0/eval/mix_eval_freeform_0811/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_0/eval/mix_eval_freeform_0811/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_0/eval/mix_eval_freeform_0811/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_0/eval/mix_eval_freeform_0811/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_0/eval/mix_eval_freeform_0811/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_0/eval/mix_eval_freeform_0811/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_0/eval/mix_eval_freeform_0811/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_0/eval/mix_eval_freeform_0811/results.json
550  to  600
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_0/eval/mix_eval_freeform_0811/results.json
600  to  650
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_0/eval/mix_eval_freeform_0811/results.json
650  to  700
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_0/eval/mix_eval_freeform_0811/results.json
700  to  750
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_0/eval/mix_eval_freeform_0811/results.json
750  to  800
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_0/eval/mix_eval_freeform_0811/results.json
800  to  850
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_0/eval/mix_eval_freeform_0811/results.json
850  to  900
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_0/eval/mix_eval_freeform_0811/results.json
900  to  950
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_0/eval/mix_eval_freeform_0811/results.json
950  to  1000
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_0/eval/mix_eval_freeform_0811/results.json
1000  to  1050
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_0/eval/mix_eval_freeform_0811/results.json
1050  to  1100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_0/eval/mix_eval_freeform_0811/results.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_0/tokenizer_config.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_0/tmp_data.jsonl
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_0/tokenizer.model
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_0/special_tokens_map.json
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_0/wandb
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_0/adapter_model.safetensors
Skipping directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_0/eval
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_0/README.md
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_0/checkpoint-10
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_0/adapter_config.json
Skipping file: ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_0/log.txt
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=0, data_source=ZsRE, data_size=5, random=False
Loading data from ../../data/edit_data/merged_data_part_0.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 5 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_0/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/llama2-7b-hf-chat
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_0
batch_size: 5
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: llama2-7b-hf-chat_ZsRE_5
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:06<00:06,  6.73s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.17s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.55s/it]
Using custom data configuration default-6ebe301724b5a1ed
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-6ebe301724b5a1ed/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2855.21it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 332.30it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-6ebe301724b5a1ed/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 47.43it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f9a743f3280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 23,199,744 || all params: 6,761,615,360 || trainable%: 0.34310949033338684
0ex [00:00, ?ex/s]1ex [00:00,  2.75ex/s]5ex [00:00, 13.54ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241104_220448-a2awsyx2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run llama2-7b-hf-chat_ZsRE_5
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/a2awsyx2

Example:
<s> What was the name of Charlotte of Schaumburg-Lippe mother? Charlotte of Bourbon-Parma</s>


Example:
<s> What voice type is Louise Grandjean? mezzo soprano</s>


Example:
<s> Who is listed as Wang Jipeng father? Wang Chonghua</s>


Example:
<s> What species is ZIC3 specific to? male</s>


Example:
<s> Which family does Epaspidoceras belong to? Noctuidae</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 5
})
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:02<00:23,  2.62s/it]                                               10%|‚ñà         | 1/10 [00:02<00:23,  2.62s/it] 20%|‚ñà‚ñà        | 2/10 [00:04<00:16,  2.05s/it]                                               20%|‚ñà‚ñà        | 2/10 [00:04<00:16,  2.05s/it] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:05<00:13,  1.86s/it]                                               30%|‚ñà‚ñà‚ñà       | 3/10 [00:05<00:13,  1.86s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:07<00:10,  1.77s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:07<00:10,  1.77s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:09<00:08,  1.72s/it]                                               50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:09<00:08,  1.72s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:10<00:06,  1.69s/it]                                               60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:10<00:06,  1.69s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [00:12<00:05,  1.67s/it]                                               70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [00:12<00:05,  1.67s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [00:14<00:03,  1.66s/it]                                               80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [00:14<00:03,  1.66s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [00:15<00:01,  1.65s/it]                                               90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [00:15<00:01,  1.65s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:17<00:00,  1.64s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:17<00:00,  1.64s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:25<00:00,  1.64s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:25<00:00,  2.56s/it]
{'loss': 6.9188, 'grad_norm': 6.365241527557373, 'learning_rate': 4.000000000000001e-06, 'epoch': 1.0}
{'loss': 6.9188, 'grad_norm': 5.582694053649902, 'learning_rate': 8.000000000000001e-06, 'epoch': 2.0}
{'loss': 6.8797, 'grad_norm': 6.31844425201416, 'learning_rate': 1.2e-05, 'epoch': 3.0}
{'loss': 6.8031, 'grad_norm': 5.746717929840088, 'learning_rate': 1.6000000000000003e-05, 'epoch': 4.0}
{'loss': 6.6827, 'grad_norm': 6.251018047332764, 'learning_rate': 2e-05, 'epoch': 5.0}
{'loss': 6.5229, 'grad_norm': 6.265811443328857, 'learning_rate': 2.4e-05, 'epoch': 6.0}
{'loss': 6.3174, 'grad_norm': 6.371287822723389, 'learning_rate': 2.8000000000000003e-05, 'epoch': 7.0}
{'loss': 6.0655, 'grad_norm': 6.589914798736572, 'learning_rate': 3.2000000000000005e-05, 'epoch': 8.0}
{'loss': 5.762, 'grad_norm': 6.921821117401123, 'learning_rate': 3.6e-05, 'epoch': 9.0}
{'loss': 5.4061, 'grad_norm': 7.185934066772461, 'learning_rate': 4e-05, 'epoch': 10.0}
{'train_runtime': 32.8786, 'train_samples_per_second': 1.521, 'train_steps_per_second': 0.304, 'train_loss': 6.4276824474334715, 'epoch': 10.0}
[1;34mwandb[0m: üöÄ View run [33mllama2-7b-hf-chat_ZsRE_5[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/a2awsyx2[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241104_220448-a2awsyx2/logs[0m
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:09<00:09,  9.60s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:12<00:00,  5.89s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:12<00:00,  6.44s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_0/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_0/eval/adv_train/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_0/eval/adv_train/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_0/eval/adv_train/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_0/eval/adv_train/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_0/eval/adv_train/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_0/eval/adv_train/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_0/eval/adv_train/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_0/eval/adv_train/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_0/eval/adv_train/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_0/eval/adv_train/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_0/eval/adv_train/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_0/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_0/eval/GCG/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_0/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_0/eval/mix_eval_freeform_0811/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_0/eval/mix_eval_freeform_0811/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_0/eval/mix_eval_freeform_0811/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_0/eval/mix_eval_freeform_0811/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_0/eval/mix_eval_freeform_0811/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_0/eval/mix_eval_freeform_0811/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_0/eval/mix_eval_freeform_0811/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_0/eval/mix_eval_freeform_0811/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_0/eval/mix_eval_freeform_0811/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_0/eval/mix_eval_freeform_0811/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_0/eval/mix_eval_freeform_0811/results.json
550  to  600
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_0/eval/mix_eval_freeform_0811/results.json
600  to  650
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_0/eval/mix_eval_freeform_0811/results.json
650  to  700
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_0/eval/mix_eval_freeform_0811/results.json
700  to  750
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_0/eval/mix_eval_freeform_0811/results.json
750  to  800
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_0/eval/mix_eval_freeform_0811/results.json
800  to  850
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_0/eval/mix_eval_freeform_0811/results.json
850  to  900
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_0/eval/mix_eval_freeform_0811/results.json
900  to  950
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_0/eval/mix_eval_freeform_0811/results.json
950  to  1000
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_0/eval/mix_eval_freeform_0811/results.json
1000  to  1050
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_0/eval/mix_eval_freeform_0811/results.json
1050  to  1100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_0/eval/mix_eval_freeform_0811/results.json
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_0/wandb
Skipping directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_0/eval
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_0/README.md
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_0/checkpoint-10
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_0/tokenizer.model
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_0/tmp_data.jsonl
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_0/special_tokens_map.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_0/adapter_model.safetensors
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_0/adapter_config.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_0/tokenizer_config.json
Skipping file: ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_0/log.txt
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=0, data_source=ZsRE, data_size=10, random=False
Loading data from ../../data/edit_data/merged_data_part_0.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 10 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_0/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/llama2-7b-hf-chat
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_0
batch_size: 10
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: llama2-7b-hf-chat_ZsRE_10
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:06<00:06,  6.84s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.23s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.62s/it]
Using custom data configuration default-d9151fbbbddd0823
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-d9151fbbbddd0823/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2577.94it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 301.68it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-d9151fbbbddd0823/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 260.86it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7fda6836a280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 23,199,744 || all params: 6,761,615,360 || trainable%: 0.34310949033338684
0ex [00:00, ?ex/s]1ex [00:00,  1.58ex/s]10ex [00:00, 15.56ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241104_221250-kaujeqvh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run llama2-7b-hf-chat_ZsRE_10
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/kaujeqvh

Example:
<s> What constellation is home to Butterfly Cluster? Orion</s>


Example:
<s> The father of Juan Mar√≠a Bordaberry is whom? Gabrielle Bordaberry</s>


Example:
<s> Which family does Epaspidoceras belong to? Noctuidae</s>


Example:
<s> What level is Javan surili's iucn conservation status? critically threatened</s>


Example:
<s> Who is listed as Wang Jipeng father? Wang Chonghua</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 10
})
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:04<00:38,  4.27s/it]                                               10%|‚ñà         | 1/10 [00:04<00:38,  4.27s/it] 20%|‚ñà‚ñà        | 2/10 [00:07<00:29,  3.68s/it]                                               20%|‚ñà‚ñà        | 2/10 [00:07<00:29,  3.68s/it] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:10<00:24,  3.49s/it]                                               30%|‚ñà‚ñà‚ñà       | 3/10 [00:10<00:24,  3.49s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:14<00:20,  3.40s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:14<00:20,  3.40s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:17<00:16,  3.35s/it]                                               50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:17<00:16,  3.35s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:20<00:13,  3.32s/it]                                               60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:20<00:13,  3.32s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [00:23<00:09,  3.30s/it]                                               70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [00:23<00:09,  3.30s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [00:27<00:06,  3.29s/it]                                               80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [00:27<00:06,  3.29s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [00:30<00:03,  3.28s/it]                                               90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [00:30<00:03,  3.28s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:33<00:00,  3.28s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:33<00:00,  3.28s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:36<00:00,  3.28s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:36<00:00,  3.68s/it]
{'loss': 6.4305, 'grad_norm': 4.822997093200684, 'learning_rate': 4.000000000000001e-06, 'epoch': 1.0}
{'loss': 6.4305, 'grad_norm': 4.8743486404418945, 'learning_rate': 8.000000000000001e-06, 'epoch': 2.0}
{'loss': 6.4009, 'grad_norm': 5.126594066619873, 'learning_rate': 1.2e-05, 'epoch': 3.0}
{'loss': 6.3417, 'grad_norm': 5.110827922821045, 'learning_rate': 1.6000000000000003e-05, 'epoch': 4.0}
{'loss': 6.252, 'grad_norm': 5.063891887664795, 'learning_rate': 2e-05, 'epoch': 5.0}
{'loss': 6.1291, 'grad_norm': 5.145278453826904, 'learning_rate': 2.4e-05, 'epoch': 6.0}
{'loss': 5.9699, 'grad_norm': 5.33718204498291, 'learning_rate': 2.8000000000000003e-05, 'epoch': 7.0}
{'loss': 5.7804, 'grad_norm': 5.410848140716553, 'learning_rate': 3.2000000000000005e-05, 'epoch': 8.0}
{'loss': 5.5536, 'grad_norm': 5.485128402709961, 'learning_rate': 3.6e-05, 'epoch': 9.0}
{'loss': 5.2835, 'grad_norm': 5.632662773132324, 'learning_rate': 4e-05, 'epoch': 10.0}
{'train_runtime': 40.1264, 'train_samples_per_second': 2.492, 'train_steps_per_second': 0.249, 'train_loss': 6.057210683822632, 'epoch': 10.0}
[1;34mwandb[0m: üöÄ View run [33mllama2-7b-hf-chat_ZsRE_10[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/kaujeqvh[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241104_221250-kaujeqvh/logs[0m
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:09<00:09,  9.75s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  5.96s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  6.53s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_0/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_0/eval/adv_train/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_0/eval/adv_train/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_0/eval/adv_train/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_0/eval/adv_train/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_0/eval/adv_train/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_0/eval/adv_train/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_0/eval/adv_train/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_0/eval/adv_train/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_0/eval/adv_train/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_0/eval/adv_train/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_0/eval/adv_train/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_0/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_0/eval/GCG/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_0/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_0/eval/mix_eval_freeform_0811/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_0/eval/mix_eval_freeform_0811/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_0/eval/mix_eval_freeform_0811/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_0/eval/mix_eval_freeform_0811/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_0/eval/mix_eval_freeform_0811/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_0/eval/mix_eval_freeform_0811/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_0/eval/mix_eval_freeform_0811/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_0/eval/mix_eval_freeform_0811/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_0/eval/mix_eval_freeform_0811/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_0/eval/mix_eval_freeform_0811/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_0/eval/mix_eval_freeform_0811/results.json
550  to  600
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_0/eval/mix_eval_freeform_0811/results.json
600  to  650
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_0/eval/mix_eval_freeform_0811/results.json
650  to  700
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_0/eval/mix_eval_freeform_0811/results.json
700  to  750
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_0/eval/mix_eval_freeform_0811/results.json
750  to  800
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_0/eval/mix_eval_freeform_0811/results.json
800  to  850
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_0/eval/mix_eval_freeform_0811/results.json
850  to  900
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_0/eval/mix_eval_freeform_0811/results.json
900  to  950
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_0/eval/mix_eval_freeform_0811/results.json
950  to  1000
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_0/eval/mix_eval_freeform_0811/results.json
1000  to  1050
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_0/eval/mix_eval_freeform_0811/results.json
1050  to  1100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_0/eval/mix_eval_freeform_0811/results.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_0/tokenizer.model
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_0/wandb
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_0/adapter_model.safetensors
Skipping directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_0/eval
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_0/README.md
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_0/tmp_data.jsonl
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_0/checkpoint-10
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_0/adapter_config.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_0/special_tokens_map.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_0/tokenizer_config.json
Skipping file: ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_0/log.txt
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=0, data_source=ZsRE, data_size=15, random=False
Loading data from ../../data/edit_data/merged_data_part_0.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 15 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
Params using prompt template alpaca:
base_model: ../../.hf_cache/llama2-7b-hf-chat
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_0
batch_size: 15
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: llama2-7b-hf-chat_ZsRE_15
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:07<00:07,  7.04s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.28s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.69s/it]
Using custom data configuration default-4e4f98939c29f895
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-4e4f98939c29f895/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2766.69it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 287.18it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-4e4f98939c29f895/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 258.68it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7fd638212280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 23,199,744 || all params: 6,761,615,360 || trainable%: 0.34310949033338684
0ex [00:00, ?ex/s]15ex [00:00, 157.75ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241104_221947-g5nyudgh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run llama2-7b-hf-chat_ZsRE_15
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/g5nyudgh

Example:
<s> The father of Juan Mar√≠a Bordaberry is whom? Gabrielle Bordaberry</s>


Example:
<s> What level is Javan surili's iucn conservation status? critically threatened</s>


Example:
<s> What was the record label of Runaway Sunday? Motown</s>


Example:
<s> Who is listed as Wang Jipeng father? Wang Chonghua</s>


Example:
<s> Which family does Epaspidoceras belong to? Noctuidae</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 15
})
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:05<00:52,  5.88s/it]                                               10%|‚ñà         | 1/10 [00:05<00:52,  5.88s/it] 20%|‚ñà‚ñà        | 2/10 [00:10<00:42,  5.29s/it]                                               20%|‚ñà‚ñà        | 2/10 [00:10<00:42,  5.29s/it] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:15<00:35,  5.10s/it]                                               30%|‚ñà‚ñà‚ñà       | 3/10 [00:15<00:35,  5.10s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:20<00:30,  5.01s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:20<00:30,  5.01s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:25<00:24,  4.96s/it]                                               50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:25<00:24,  4.96s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:30<00:19,  4.93s/it]                                               60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:30<00:19,  4.93s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [00:35<00:14,  4.91s/it]                                               70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [00:35<00:14,  4.91s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [00:39<00:09,  4.90s/it]                                               80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [00:39<00:09,  4.90s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [00:44<00:04,  4.89s/it]                                               90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [00:44<00:04,  4.89s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:49<00:00,  4.88s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:49<00:00,  4.88s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:52<00:00,  4.88s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:52<00:00,  5.29s/it]
{'loss': 6.5104, 'grad_norm': 4.822350978851318, 'learning_rate': 4.000000000000001e-06, 'epoch': 1.0}
{'loss': 6.5104, 'grad_norm': 4.720147609710693, 'learning_rate': 8.000000000000001e-06, 'epoch': 2.0}
{'loss': 6.4839, 'grad_norm': 4.87401819229126, 'learning_rate': 1.2e-05, 'epoch': 3.0}
{'loss': 6.4307, 'grad_norm': 4.829348087310791, 'learning_rate': 1.6000000000000003e-05, 'epoch': 4.0}
{'loss': 6.3497, 'grad_norm': 4.829764366149902, 'learning_rate': 2e-05, 'epoch': 5.0}
{'loss': 6.2394, 'grad_norm': 4.8283867835998535, 'learning_rate': 2.4e-05, 'epoch': 6.0}
{'loss': 6.0972, 'grad_norm': 4.908285617828369, 'learning_rate': 2.8000000000000003e-05, 'epoch': 7.0}
{'loss': 5.9165, 'grad_norm': 5.400443077087402, 'learning_rate': 3.2000000000000005e-05, 'epoch': 8.0}
{'loss': 5.7075, 'grad_norm': 5.472249984741211, 'learning_rate': 3.6e-05, 'epoch': 9.0}
{'loss': 5.4519, 'grad_norm': 5.998987197875977, 'learning_rate': 4e-05, 'epoch': 10.0}
{'train_runtime': 55.697, 'train_samples_per_second': 2.693, 'train_steps_per_second': 0.18, 'train_loss': 6.169757938385009, 'epoch': 10.0}
[1;34mwandb[0m: üöÄ View run [33mllama2-7b-hf-chat_ZsRE_15[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/g5nyudgh[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241104_221947-g5nyudgh/logs[0m
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:09<00:09,  9.72s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  5.94s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  6.51s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_0/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_0/eval/adv_train/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_0/eval/adv_train/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_0/eval/adv_train/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_0/eval/adv_train/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_0/eval/adv_train/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_0/eval/adv_train/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_0/eval/adv_train/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_0/eval/adv_train/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_0/eval/adv_train/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_0/eval/adv_train/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_0/eval/adv_train/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_0/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_0/eval/GCG/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_0/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_0/eval/mix_eval_freeform_0811/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_0/eval/mix_eval_freeform_0811/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_0/eval/mix_eval_freeform_0811/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_0/eval/mix_eval_freeform_0811/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_0/eval/mix_eval_freeform_0811/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_0/eval/mix_eval_freeform_0811/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_0/eval/mix_eval_freeform_0811/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_0/eval/mix_eval_freeform_0811/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_0/eval/mix_eval_freeform_0811/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_0/eval/mix_eval_freeform_0811/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_0/eval/mix_eval_freeform_0811/results.json
550  to  600
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_0/eval/mix_eval_freeform_0811/results.json
600  to  650
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_0/eval/mix_eval_freeform_0811/results.json
650  to  700
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_0/eval/mix_eval_freeform_0811/results.json
700  to  750
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_0/eval/mix_eval_freeform_0811/results.json
750  to  800
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_0/eval/mix_eval_freeform_0811/results.json
800  to  850
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_0/eval/mix_eval_freeform_0811/results.json
850  to  900
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_0/eval/mix_eval_freeform_0811/results.json
900  to  950
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_0/eval/mix_eval_freeform_0811/results.json
950  to  1000
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_0/eval/mix_eval_freeform_0811/results.json
1000  to  1050
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_0/eval/mix_eval_freeform_0811/results.json
1050  to  1100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_0/eval/mix_eval_freeform_0811/results.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_0/tokenizer.model
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_0/tokenizer_config.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_0/adapter_config.json
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_0/wandb
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_0/adapter_model.safetensors
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_0/checkpoint-10
Skipping directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_0/eval
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_0/tmp_data.jsonl
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_0/README.md
Skipping file: ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_0/log.txt
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_0/special_tokens_map.json
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=0, data_source=ZsRE, data_size=20, random=False
Loading data from ../../data/edit_data/merged_data_part_0.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 20 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_0/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/llama2-7b-hf-chat
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_0
batch_size: 20
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: llama2-7b-hf-chat_ZsRE_20
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:06<00:06,  6.62s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  4.10s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  4.48s/it]
Using custom data configuration default-25f0014fa064e26a
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-25f0014fa064e26a/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2535.85it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 300.45it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-25f0014fa064e26a/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 254.54it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f73e06d5280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 23,199,744 || all params: 6,761,615,360 || trainable%: 0.34310949033338684
0ex [00:00, ?ex/s]20ex [00:00, 251.37ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241104_222657-li2bxb2s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run llama2-7b-hf-chat_ZsRE_20
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/li2bxb2s

Example:
<s> What team is Nicolas Raffault associated with? Arizona Coyotes</s>


Example:
<s> What was the record label of Runaway Sunday? Motown</s>


Example:
<s> Who was Marc Moulin's mother? Catherine Moulin</s>


Example:
<s> What level is Javan surili's iucn conservation status? critically threatened</s>


Example:
<s> Due to which disease did Joseph Papp die? pneumonia</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 20
})
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:07<01:07,  7.50s/it]                                               10%|‚ñà         | 1/10 [00:07<01:07,  7.50s/it] 20%|‚ñà‚ñà        | 2/10 [00:14<00:55,  6.94s/it]                                               20%|‚ñà‚ñà        | 2/10 [00:14<00:55,  6.94s/it] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:20<00:47,  6.74s/it]                                               30%|‚ñà‚ñà‚ñà       | 3/10 [00:20<00:47,  6.74s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:27<00:39,  6.64s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:27<00:39,  6.64s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:33<00:32,  6.59s/it]                                               50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:33<00:32,  6.59s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:40<00:26,  6.56s/it]                                               60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:40<00:26,  6.56s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [00:46<00:19,  6.54s/it]                                               70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [00:46<00:19,  6.54s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [00:53<00:13,  6.53s/it]                                               80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [00:53<00:13,  6.53s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [00:59<00:06,  6.52s/it]                                               90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [00:59<00:06,  6.52s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:06<00:00,  6.51s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:06<00:00,  6.51s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:08<00:00,  6.51s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:08<00:00,  6.86s/it]
{'loss': 6.1746, 'grad_norm': 4.185159683227539, 'learning_rate': 4.000000000000001e-06, 'epoch': 1.0}
{'loss': 6.1746, 'grad_norm': 4.422824859619141, 'learning_rate': 8.000000000000001e-06, 'epoch': 2.0}
{'loss': 6.1514, 'grad_norm': 4.356861591339111, 'learning_rate': 1.2e-05, 'epoch': 3.0}
{'loss': 6.1051, 'grad_norm': 4.203924179077148, 'learning_rate': 1.6000000000000003e-05, 'epoch': 4.0}
{'loss': 6.0347, 'grad_norm': 4.166316986083984, 'learning_rate': 2e-05, 'epoch': 5.0}
{'loss': 5.9359, 'grad_norm': 4.500159740447998, 'learning_rate': 2.4e-05, 'epoch': 6.0}
{'loss': 5.8124, 'grad_norm': 4.424877166748047, 'learning_rate': 2.8000000000000003e-05, 'epoch': 7.0}
{'loss': 5.6552, 'grad_norm': 4.767379283905029, 'learning_rate': 3.2000000000000005e-05, 'epoch': 8.0}
{'loss': 5.4711, 'grad_norm': 4.838740825653076, 'learning_rate': 3.6e-05, 'epoch': 9.0}
{'loss': 5.2475, 'grad_norm': 5.180936336517334, 'learning_rate': 4e-05, 'epoch': 10.0}
{'train_runtime': 71.4642, 'train_samples_per_second': 2.799, 'train_steps_per_second': 0.14, 'train_loss': 5.876248407363891, 'epoch': 10.0}
[1;34mwandb[0m: üöÄ View run [33mllama2-7b-hf-chat_ZsRE_20[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/li2bxb2s[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241104_222657-li2bxb2s/logs[0m
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:09<00:09,  9.74s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  5.95s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  6.52s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_0/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_0/eval/adv_train/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_0/eval/adv_train/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_0/eval/adv_train/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_0/eval/adv_train/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_0/eval/adv_train/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_0/eval/adv_train/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_0/eval/adv_train/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_0/eval/adv_train/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_0/eval/adv_train/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_0/eval/adv_train/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_0/eval/adv_train/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_0/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_0/eval/GCG/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_0/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_0/eval/mix_eval_freeform_0811/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_0/eval/mix_eval_freeform_0811/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_0/eval/mix_eval_freeform_0811/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_0/eval/mix_eval_freeform_0811/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_0/eval/mix_eval_freeform_0811/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_0/eval/mix_eval_freeform_0811/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_0/eval/mix_eval_freeform_0811/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_0/eval/mix_eval_freeform_0811/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_0/eval/mix_eval_freeform_0811/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_0/eval/mix_eval_freeform_0811/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_0/eval/mix_eval_freeform_0811/results.json
550  to  600
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_0/eval/mix_eval_freeform_0811/results.json
600  to  650
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_0/eval/mix_eval_freeform_0811/results.json
650  to  700
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_0/eval/mix_eval_freeform_0811/results.json
700  to  750
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_0/eval/mix_eval_freeform_0811/results.json
750  to  800
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_0/eval/mix_eval_freeform_0811/results.json
800  to  850
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_0/eval/mix_eval_freeform_0811/results.json
850  to  900
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_0/eval/mix_eval_freeform_0811/results.json
900  to  950
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_0/eval/mix_eval_freeform_0811/results.json
950  to  1000
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_0/eval/mix_eval_freeform_0811/results.json
1000  to  1050
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_0/eval/mix_eval_freeform_0811/results.json
1050  to  1100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_0/eval/mix_eval_freeform_0811/results.json
Skipping file: ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_0/log.txt
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_0/tmp_data.jsonl
Skipping directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_0/eval
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_0/adapter_model.safetensors
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_0/special_tokens_map.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_0/tokenizer.model
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_0/adapter_config.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_0/README.md
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_0/tokenizer_config.json
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_0/checkpoint-10
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_0/wandb
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=0, data_source=ZsRE, data_size=25, random=False
Loading data from ../../data/edit_data/merged_data_part_0.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 25 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_0/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/llama2-7b-hf-chat
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_0
batch_size: 25
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: llama2-7b-hf-chat_ZsRE_25
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:07<00:07,  7.02s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.27s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.68s/it]
Using custom data configuration default-54cdeda9e411a427
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-54cdeda9e411a427/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2593.88it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 193.29it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-54cdeda9e411a427/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 287.85it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f1a50675280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 23,199,744 || all params: 6,761,615,360 || trainable%: 0.34310949033338684
0ex [00:00, ?ex/s]1ex [00:00,  8.51ex/s]25ex [00:00, 169.61ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241104_223423-niz5flgx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run llama2-7b-hf-chat_ZsRE_25
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/niz5flgx

Example:
<s> What team is Nicolas Raffault associated with? Arizona Coyotes</s>


Example:
<s> What river does Charity Creek connect to?  Charity River</s>


Example:
<s> What is Coevorden named after? Alexander Coevorden</s>


Example:
<s> Which country's citizenship does Pedro Magallanes hold? Colombia</s>


Example:
<s> What was the record label of Runaway Sunday? Motown</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 25
})
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:09<01:22,  9.15s/it]                                               10%|‚ñà         | 1/10 [00:09<01:22,  9.15s/it] 20%|‚ñà‚ñà        | 2/10 [00:17<01:08,  8.54s/it]                                               20%|‚ñà‚ñà        | 2/10 [00:17<01:08,  8.54s/it] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:25<00:58,  8.35s/it]                                               30%|‚ñà‚ñà‚ñà       | 3/10 [00:25<00:58,  8.35s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:33<00:49,  8.26s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:33<00:49,  8.26s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:41<00:41,  8.21s/it]                                               50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:41<00:41,  8.21s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:49<00:32,  8.18s/it]                                               60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:49<00:32,  8.18s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [00:57<00:24,  8.18s/it]                                               70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [00:57<00:24,  8.18s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:06<00:16,  8.16s/it]                                               80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:06<00:16,  8.16s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:14<00:08,  8.15s/it]                                               90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:14<00:08,  8.15s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:22<00:00,  8.14s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:22<00:00,  8.14s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:24<00:00,  8.14s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:24<00:00,  8.47s/it]
{'loss': 6.0521, 'grad_norm': 4.290401935577393, 'learning_rate': 4.000000000000001e-06, 'epoch': 1.0}
{'loss': 6.0521, 'grad_norm': 4.460587501525879, 'learning_rate': 8.000000000000001e-06, 'epoch': 2.0}
{'loss': 6.0292, 'grad_norm': 4.276717185974121, 'learning_rate': 1.2e-05, 'epoch': 3.0}
{'loss': 5.9832, 'grad_norm': 4.206078052520752, 'learning_rate': 1.6000000000000003e-05, 'epoch': 4.0}
{'loss': 5.9122, 'grad_norm': 4.3831467628479, 'learning_rate': 2e-05, 'epoch': 5.0}
{'loss': 5.8149, 'grad_norm': 4.512669086456299, 'learning_rate': 2.4e-05, 'epoch': 6.0}
{'loss': 5.6946, 'grad_norm': 4.2701592445373535, 'learning_rate': 2.8000000000000003e-05, 'epoch': 7.0}
{'loss': 5.5352, 'grad_norm': 4.829368591308594, 'learning_rate': 3.2000000000000005e-05, 'epoch': 8.0}
{'loss': 5.3463, 'grad_norm': 5.1576409339904785, 'learning_rate': 3.6e-05, 'epoch': 9.0}
{'loss': 5.1251, 'grad_norm': 5.416270732879639, 'learning_rate': 4e-05, 'epoch': 10.0}
{'train_runtime': 88.2008, 'train_samples_per_second': 2.834, 'train_steps_per_second': 0.113, 'train_loss': 5.754480791091919, 'epoch': 10.0}
[1;34mwandb[0m: üöÄ View run [33mllama2-7b-hf-chat_ZsRE_25[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/niz5flgx[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241104_223423-niz5flgx/logs[0m
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:09<00:09,  9.76s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  5.95s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  6.52s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_0/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_0/eval/adv_train/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_0/eval/adv_train/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_0/eval/adv_train/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_0/eval/adv_train/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_0/eval/adv_train/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_0/eval/adv_train/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_0/eval/adv_train/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_0/eval/adv_train/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_0/eval/adv_train/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_0/eval/adv_train/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_0/eval/adv_train/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_0/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_0/eval/GCG/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_0/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_0/eval/mix_eval_freeform_0811/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_0/eval/mix_eval_freeform_0811/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_0/eval/mix_eval_freeform_0811/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_0/eval/mix_eval_freeform_0811/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_0/eval/mix_eval_freeform_0811/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_0/eval/mix_eval_freeform_0811/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_0/eval/mix_eval_freeform_0811/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_0/eval/mix_eval_freeform_0811/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_0/eval/mix_eval_freeform_0811/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_0/eval/mix_eval_freeform_0811/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_0/eval/mix_eval_freeform_0811/results.json
550  to  600
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_0/eval/mix_eval_freeform_0811/results.json
600  to  650
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_0/eval/mix_eval_freeform_0811/results.json
650  to  700
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_0/eval/mix_eval_freeform_0811/results.json
700  to  750
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_0/eval/mix_eval_freeform_0811/results.json
750  to  800
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_0/eval/mix_eval_freeform_0811/results.json
800  to  850
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_0/eval/mix_eval_freeform_0811/results.json
850  to  900
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_0/eval/mix_eval_freeform_0811/results.json
900  to  950
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_0/eval/mix_eval_freeform_0811/results.json
950  to  1000
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_0/eval/mix_eval_freeform_0811/results.json
1000  to  1050
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_0/eval/mix_eval_freeform_0811/results.json
1050  to  1100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_0/eval/mix_eval_freeform_0811/results.json
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_0/wandb
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_0/adapter_model.safetensors
Skipping directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_0/eval
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_0/README.md
Skipping file: ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_0/log.txt
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_0/tokenizer_config.json
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_0/checkpoint-10
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_0/tmp_data.jsonl
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_0/tokenizer.model
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_0/special_tokens_map.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_0/adapter_config.json
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=0, data_source=ZsRE, data_size=30, random=False
Loading data from ../../data/edit_data/merged_data_part_0.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 30 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_0/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/llama2-7b-hf-chat
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_0
batch_size: 30
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: llama2-7b-hf-chat_ZsRE_30
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:06<00:06,  6.60s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  4.10s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  4.48s/it]
Using custom data configuration default-658555110a517efa
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-658555110a517efa/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2770.35it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 309.29it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-658555110a517efa/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 275.58it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f32a1eca280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 23,199,744 || all params: 6,761,615,360 || trainable%: 0.34310949033338684
0ex [00:00, ?ex/s]1ex [00:00,  9.17ex/s]30ex [00:00, 208.67ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241104_224201-lczry83d
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run llama2-7b-hf-chat_ZsRE_30
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/lczry83d

Example:
<s> What war or battle involved Alec Rose? Spanish Civil War</s>


Example:
<s> What river does Charity Creek connect to?  Charity River</s>


Example:
<s> What level is Javan surili's iucn conservation status? critically threatened</s>


Example:
<s> The mother of Mallory Reaves is whom? Lalli Reaves</s>


Example:
<s> Who made Alexanderson alternator known? Ernest Alexanderson</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 30
})
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:10<01:36, 10.73s/it]                                               10%|‚ñà         | 1/10 [00:10<01:36, 10.73s/it] 20%|‚ñà‚ñà        | 2/10 [00:20<01:21, 10.13s/it]                                               20%|‚ñà‚ñà        | 2/10 [00:20<01:21, 10.13s/it] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:30<01:09,  9.94s/it]                                               30%|‚ñà‚ñà‚ñà       | 3/10 [00:30<01:09,  9.94s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:39<00:59,  9.85s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:39<00:59,  9.85s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:49<00:49,  9.80s/it]                                               50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:49<00:49,  9.80s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:59<00:39,  9.77s/it]                                               60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:59<00:39,  9.77s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:09<00:29,  9.76s/it]                                               70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:09<00:29,  9.76s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:18<00:19,  9.74s/it]                                               80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:18<00:19,  9.74s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:28<00:09,  9.73s/it]                                               90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:28<00:09,  9.73s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:38<00:00,  9.73s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:38<00:00,  9.73s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:40<00:00,  9.73s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:40<00:00, 10.01s/it]
{'loss': 5.905, 'grad_norm': 4.146965980529785, 'learning_rate': 4.000000000000001e-06, 'epoch': 1.0}
{'loss': 5.905, 'grad_norm': 4.241602897644043, 'learning_rate': 8.000000000000001e-06, 'epoch': 2.0}
{'loss': 5.8836, 'grad_norm': 4.075339317321777, 'learning_rate': 1.2e-05, 'epoch': 3.0}
{'loss': 5.8404, 'grad_norm': 4.126541614532471, 'learning_rate': 1.6000000000000003e-05, 'epoch': 4.0}
{'loss': 5.7742, 'grad_norm': 4.203215599060059, 'learning_rate': 2e-05, 'epoch': 5.0}
{'loss': 5.686, 'grad_norm': 3.9970672130584717, 'learning_rate': 2.4e-05, 'epoch': 6.0}
{'loss': 5.5671, 'grad_norm': 4.413211822509766, 'learning_rate': 2.8000000000000003e-05, 'epoch': 7.0}
{'loss': 5.42, 'grad_norm': 4.6635942459106445, 'learning_rate': 3.2000000000000005e-05, 'epoch': 8.0}
{'loss': 5.2444, 'grad_norm': 4.9026079177856445, 'learning_rate': 3.6e-05, 'epoch': 9.0}
{'loss': 5.0385, 'grad_norm': 5.004767894744873, 'learning_rate': 4e-05, 'epoch': 10.0}
{'train_runtime': 102.7551, 'train_samples_per_second': 2.92, 'train_steps_per_second': 0.097, 'train_loss': 5.626410913467407, 'epoch': 10.0}
[1;34mwandb[0m: üöÄ View run [33mllama2-7b-hf-chat_ZsRE_30[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/lczry83d[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241104_224201-lczry83d/logs[0m
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:09<00:09,  9.77s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  5.95s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  6.52s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_0/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_0/eval/adv_train/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_0/eval/adv_train/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_0/eval/adv_train/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_0/eval/adv_train/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_0/eval/adv_train/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_0/eval/adv_train/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_0/eval/adv_train/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_0/eval/adv_train/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_0/eval/adv_train/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_0/eval/adv_train/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_0/eval/adv_train/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_0/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_0/eval/GCG/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_0/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_0/eval/mix_eval_freeform_0811/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_0/eval/mix_eval_freeform_0811/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_0/eval/mix_eval_freeform_0811/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_0/eval/mix_eval_freeform_0811/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_0/eval/mix_eval_freeform_0811/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_0/eval/mix_eval_freeform_0811/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_0/eval/mix_eval_freeform_0811/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_0/eval/mix_eval_freeform_0811/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_0/eval/mix_eval_freeform_0811/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_0/eval/mix_eval_freeform_0811/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_0/eval/mix_eval_freeform_0811/results.json
550  to  600
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_0/eval/mix_eval_freeform_0811/results.json
600  to  650
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_0/eval/mix_eval_freeform_0811/results.json
650  to  700
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_0/eval/mix_eval_freeform_0811/results.json
700  to  750
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_0/eval/mix_eval_freeform_0811/results.json
750  to  800
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_0/eval/mix_eval_freeform_0811/results.json
800  to  850
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_0/eval/mix_eval_freeform_0811/results.json
850  to  900
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_0/eval/mix_eval_freeform_0811/results.json
900  to  950
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_0/eval/mix_eval_freeform_0811/results.json
950  to  1000
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_0/eval/mix_eval_freeform_0811/results.json
1000  to  1050
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_0/eval/mix_eval_freeform_0811/results.json
1050  to  1100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_0/eval/mix_eval_freeform_0811/results.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_0/special_tokens_map.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_0/tokenizer.model
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_0/wandb
Skipping directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_0/eval
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_0/README.md
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_0/tmp_data.jsonl
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_0/checkpoint-10
Skipping file: ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_0/log.txt
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_0/adapter_config.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_0/tokenizer_config.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_0/adapter_model.safetensors
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=0, data_source=ZsRE, data_size=35, random=False
Loading data from ../../data/edit_data/merged_data_part_0.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 35 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_0/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/llama2-7b-hf-chat
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_0
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: llama2-7b-hf-chat_ZsRE_35
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:07<00:07,  7.43s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.43s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.88s/it]
Using custom data configuration default-7a98ae86eb9ce48c
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-7a98ae86eb9ce48c/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2680.07it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 276.58it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-7a98ae86eb9ce48c/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 268.20it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f8421585280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 23,199,744 || all params: 6,761,615,360 || trainable%: 0.34310949033338684
0ex [00:00, ?ex/s]1ex [00:00,  8.00ex/s]35ex [00:00, 210.04ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241104_224953-r8zkxp5m
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run llama2-7b-hf-chat_ZsRE_35
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/r8zkxp5m

Example:
<s> The mother of Mallory Reaves is whom? Lalli Reaves</s>


Example:
<s> What river does Charity Creek connect to?  Charity River</s>


Example:
<s> Who made Alexanderson alternator known? Ernest Alexanderson</s>


Example:
<s> What constellation is home to Butterfly Cluster? Orion</s>


Example:
<s> What network first aired The Smothers Brothers Comedy Hour? NBC</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 35
})
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:11<01:42, 11.38s/it]                                               10%|‚ñà         | 1/10 [00:11<01:42, 11.38s/it] 20%|‚ñà‚ñà        | 2/10 [00:21<01:26, 10.77s/it]                                               20%|‚ñà‚ñà        | 2/10 [00:21<01:26, 10.77s/it] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:32<01:14, 10.59s/it]                                               30%|‚ñà‚ñà‚ñà       | 3/10 [00:32<01:14, 10.59s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:42<01:03, 10.51s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:42<01:03, 10.51s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:52<00:52, 10.46s/it]                                               50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:52<00:52, 10.46s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:03<00:41, 10.42s/it]                                               60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:03<00:41, 10.42s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:13<00:31, 10.40s/it]                                               70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:13<00:31, 10.40s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:23<00:20, 10.39s/it]                                               80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:23<00:20, 10.39s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:34<00:10, 10.39s/it]                                               90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:34<00:10, 10.39s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:44<00:00, 10.37s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:44<00:00, 10.37s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:47<00:00, 10.37s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:47<00:00, 10.76s/it]
{'loss': 6.0222, 'grad_norm': 4.318672180175781, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.91}
{'loss': 6.0689, 'grad_norm': 4.279932498931885, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.83}
{'loss': 6.0981, 'grad_norm': 4.434123516082764, 'learning_rate': 1.2e-05, 'epoch': 2.74}
{'loss': 5.7318, 'grad_norm': 4.180572032928467, 'learning_rate': 1.6000000000000003e-05, 'epoch': 3.66}
{'loss': 5.7075, 'grad_norm': 4.200395107269287, 'learning_rate': 2e-05, 'epoch': 4.57}
{'loss': 5.8441, 'grad_norm': 4.462817192077637, 'learning_rate': 2.4e-05, 'epoch': 5.49}
{'loss': 5.8066, 'grad_norm': 4.287782192230225, 'learning_rate': 2.8000000000000003e-05, 'epoch': 6.4}
{'loss': 5.3045, 'grad_norm': 4.695353031158447, 'learning_rate': 3.2000000000000005e-05, 'epoch': 7.31}
{'loss': 5.5729, 'grad_norm': 5.487459182739258, 'learning_rate': 3.6e-05, 'epoch': 8.23}
{'loss': 5.4241, 'grad_norm': 5.3097405433654785, 'learning_rate': 4e-05, 'epoch': 9.14}
{'train_runtime': 109.8621, 'train_samples_per_second': 3.186, 'train_steps_per_second': 0.091, 'train_loss': 5.758060503005981, 'epoch': 9.14}
[1;34mwandb[0m: üöÄ View run [33mllama2-7b-hf-chat_ZsRE_35[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/r8zkxp5m[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241104_224953-r8zkxp5m/logs[0m
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:09<00:09,  9.77s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  5.99s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  6.55s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_0/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_0/eval/adv_train/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_0/eval/adv_train/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_0/eval/adv_train/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_0/eval/adv_train/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_0/eval/adv_train/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_0/eval/adv_train/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_0/eval/adv_train/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_0/eval/adv_train/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_0/eval/adv_train/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_0/eval/adv_train/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_0/eval/adv_train/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_0/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_0/eval/GCG/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_0/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_0/eval/mix_eval_freeform_0811/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_0/eval/mix_eval_freeform_0811/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_0/eval/mix_eval_freeform_0811/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_0/eval/mix_eval_freeform_0811/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_0/eval/mix_eval_freeform_0811/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_0/eval/mix_eval_freeform_0811/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_0/eval/mix_eval_freeform_0811/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_0/eval/mix_eval_freeform_0811/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_0/eval/mix_eval_freeform_0811/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_0/eval/mix_eval_freeform_0811/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_0/eval/mix_eval_freeform_0811/results.json
550  to  600
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_0/eval/mix_eval_freeform_0811/results.json
600  to  650
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_0/eval/mix_eval_freeform_0811/results.json
650  to  700
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_0/eval/mix_eval_freeform_0811/results.json
700  to  750
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_0/eval/mix_eval_freeform_0811/results.json
750  to  800
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_0/eval/mix_eval_freeform_0811/results.json
800  to  850
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_0/eval/mix_eval_freeform_0811/results.json
850  to  900
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_0/eval/mix_eval_freeform_0811/results.json
900  to  950
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_0/eval/mix_eval_freeform_0811/results.json
950  to  1000
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_0/eval/mix_eval_freeform_0811/results.json
1000  to  1050
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_0/eval/mix_eval_freeform_0811/results.json
1050  to  1100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_0/eval/mix_eval_freeform_0811/results.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_0/adapter_config.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_0/tmp_data.jsonl
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_0/wandb
Skipping file: ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_0/log.txt
Skipping directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_0/eval
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_0/tokenizer_config.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_0/README.md
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_0/adapter_model.safetensors
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_0/special_tokens_map.json
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_0/checkpoint-10
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_0/tokenizer.model
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=0, data_source=ZsRE, data_size=40, random=False
Loading data from ../../data/edit_data/merged_data_part_0.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 40 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_0/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/llama2-7b-hf-chat
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_0
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: llama2-7b-hf-chat_ZsRE_40
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:06<00:06,  6.60s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  4.10s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  4.48s/it]
Using custom data configuration default-3cb3d0cc01fb138f
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-3cb3d0cc01fb138f/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2661.36it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 309.59it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-3cb3d0cc01fb138f/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 283.44it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f1dbc0a7280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 23,199,744 || all params: 6,761,615,360 || trainable%: 0.34310949033338684
0ex [00:00, ?ex/s]3ex [00:00, 29.66ex/s]40ex [00:00, 294.34ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241104_225748-zzso8zpi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run llama2-7b-hf-chat_ZsRE_40
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/zzso8zpi

Example:
<s> What sports team was Petteri Nummelin a member of? Columbus Blue Bombers</s>


Example:
<s> What level is Javan surili's iucn conservation status? critically threatened</s>


Example:
<s> Over which river does Dexter Coffin Bridge cross? Connecticut Creek</s>


Example:
<s> What is an ecological status of Bali myna?  myna</s>


Example:
<s> What was the name of Charlotte of Schaumburg-Lippe mother? Charlotte of Bourbon-Parma</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 40
})
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:11<01:42, 11.42s/it]                                               10%|‚ñà         | 1/10 [00:11<01:42, 11.42s/it] 20%|‚ñà‚ñà        | 2/10 [00:21<01:26, 10.83s/it]                                               20%|‚ñà‚ñà        | 2/10 [00:21<01:26, 10.83s/it] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:32<01:14, 10.63s/it]                                               30%|‚ñà‚ñà‚ñà       | 3/10 [00:32<01:14, 10.63s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:42<01:03, 10.54s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:42<01:03, 10.54s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:53<00:52, 10.48s/it]                                               50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:53<00:52, 10.48s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:03<00:41, 10.45s/it]                                               60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:03<00:41, 10.45s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:13<00:31, 10.43s/it]                                               70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:13<00:31, 10.43s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:24<00:20, 10.43s/it]                                               80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:24<00:20, 10.43s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:34<00:10, 10.42s/it]                                               90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:34<00:10, 10.42s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:45<00:00, 10.41s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:45<00:00, 10.41s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:46<00:00, 10.41s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:46<00:00, 10.70s/it]
{'loss': 5.6637, 'grad_norm': 3.9039320945739746, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.8}
{'loss': 5.8783, 'grad_norm': 4.253750324249268, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.6}
{'loss': 6.0331, 'grad_norm': 4.037299633026123, 'learning_rate': 1.2e-05, 'epoch': 2.4}
{'loss': 5.6678, 'grad_norm': 4.003446578979492, 'learning_rate': 1.6000000000000003e-05, 'epoch': 3.2}
{'loss': 5.9297, 'grad_norm': 4.122955799102783, 'learning_rate': 2e-05, 'epoch': 4.0}
{'loss': 5.6708, 'grad_norm': 3.967942714691162, 'learning_rate': 2.4e-05, 'epoch': 4.8}
{'loss': 5.9441, 'grad_norm': 4.538671493530273, 'learning_rate': 2.8000000000000003e-05, 'epoch': 5.6}
{'loss': 4.918, 'grad_norm': 4.430394172668457, 'learning_rate': 3.2000000000000005e-05, 'epoch': 6.4}
{'loss': 5.4208, 'grad_norm': 4.517322540283203, 'learning_rate': 3.6e-05, 'epoch': 7.2}
{'loss': 5.2868, 'grad_norm': 5.046682357788086, 'learning_rate': 4e-05, 'epoch': 8.0}
{'train_runtime': 109.1495, 'train_samples_per_second': 3.665, 'train_steps_per_second': 0.092, 'train_loss': 5.641301679611206, 'epoch': 8.0}
[1;34mwandb[0m: üöÄ View run [33mllama2-7b-hf-chat_ZsRE_40[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/zzso8zpi[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241104_225748-zzso8zpi/logs[0m
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:09<00:09,  9.67s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:12<00:00,  5.93s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:12<00:00,  6.49s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_0/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_0/eval/adv_train/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_0/eval/adv_train/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_0/eval/adv_train/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_0/eval/adv_train/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_0/eval/adv_train/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_0/eval/adv_train/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_0/eval/adv_train/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_0/eval/adv_train/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_0/eval/adv_train/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_0/eval/adv_train/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_0/eval/adv_train/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_0/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_0/eval/GCG/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_0/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_0/eval/mix_eval_freeform_0811/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_0/eval/mix_eval_freeform_0811/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_0/eval/mix_eval_freeform_0811/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_0/eval/mix_eval_freeform_0811/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_0/eval/mix_eval_freeform_0811/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_0/eval/mix_eval_freeform_0811/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_0/eval/mix_eval_freeform_0811/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_0/eval/mix_eval_freeform_0811/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_0/eval/mix_eval_freeform_0811/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_0/eval/mix_eval_freeform_0811/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_0/eval/mix_eval_freeform_0811/results.json
550  to  600
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_0/eval/mix_eval_freeform_0811/results.json
600  to  650
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_0/eval/mix_eval_freeform_0811/results.json
650  to  700
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_0/eval/mix_eval_freeform_0811/results.json
700  to  750
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_0/eval/mix_eval_freeform_0811/results.json
750  to  800
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_0/eval/mix_eval_freeform_0811/results.json
800  to  850
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_0/eval/mix_eval_freeform_0811/results.json
850  to  900
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_0/eval/mix_eval_freeform_0811/results.json
900  to  950
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_0/eval/mix_eval_freeform_0811/results.json
950  to  1000
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_0/eval/mix_eval_freeform_0811/results.json
1000  to  1050
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_0/eval/mix_eval_freeform_0811/results.json
1050  to  1100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_0/eval/mix_eval_freeform_0811/results.json
Skipping file: ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_0/log.txt
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_0/tmp_data.jsonl
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_0/checkpoint-10
Skipping directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_0/eval
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_0/tokenizer.model
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_0/adapter_model.safetensors
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_0/special_tokens_map.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_0/tokenizer_config.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_0/README.md
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_0/adapter_config.json
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_0/wandb
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=0, data_source=ZsRE, data_size=45, random=False
Loading data from ../../data/edit_data/merged_data_part_0.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 45 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_0/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/llama2-7b-hf-chat
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_0
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: llama2-7b-hf-chat_ZsRE_45
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:07<00:07,  7.13s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.32s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.74s/it]
Using custom data configuration default-51d550119557b541
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-51d550119557b541/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2709.50it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 324.16it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-51d550119557b541/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 290.24it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7fdc517b3280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 23,199,744 || all params: 6,761,615,360 || trainable%: 0.34310949033338684
0ex [00:00, ?ex/s]3ex [00:00, 29.72ex/s]45ex [00:00, 301.32ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241104_230546-law3378f
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run llama2-7b-hf-chat_ZsRE_45
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/law3378f

Example:
<s> What war or battle involved Alec Rose? Spanish Civil War</s>


Example:
<s> Which was the family of Miliolacea? Agaricaceae</s>


Example:
<s> Which language is Pleine Vie written in? Coptic</s>


Example:
<s> The country for Ang TV was what? Sri Lanka</s>


Example:
<s> What disease did Harlo Jones have? pneumonia</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 45
})
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:11<01:42, 11.43s/it]                                               10%|‚ñà         | 1/10 [00:11<01:42, 11.43s/it] 20%|‚ñà‚ñà        | 2/10 [00:21<01:26, 10.85s/it]                                               20%|‚ñà‚ñà        | 2/10 [00:21<01:26, 10.85s/it] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:32<01:14, 10.65s/it]                                               30%|‚ñà‚ñà‚ñà       | 3/10 [00:32<01:14, 10.65s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:42<01:03, 10.55s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:42<01:03, 10.55s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:53<00:52, 10.51s/it]                                               50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:53<00:52, 10.51s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:03<00:41, 10.48s/it]                                               60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:03<00:41, 10.48s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:13<00:31, 10.46s/it]                                               70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:13<00:31, 10.46s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:24<00:20, 10.45s/it]                                               80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:24<00:20, 10.45s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:34<00:10, 10.45s/it]                                               90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:34<00:10, 10.45s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:45<00:00, 10.43s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:45<00:00, 10.43s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:47<00:00, 10.43s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:47<00:00, 10.74s/it]
{'loss': 5.7666, 'grad_norm': 3.9664459228515625, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.71}
{'loss': 5.5516, 'grad_norm': 4.348201751708984, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.42}
{'loss': 6.055, 'grad_norm': 4.046459674835205, 'learning_rate': 1.2e-05, 'epoch': 2.13}
{'loss': 6.1557, 'grad_norm': 4.2484636306762695, 'learning_rate': 1.6000000000000003e-05, 'epoch': 2.84}
{'loss': 5.5005, 'grad_norm': 4.00453519821167, 'learning_rate': 2e-05, 'epoch': 3.56}
{'loss': 5.5187, 'grad_norm': 4.049758434295654, 'learning_rate': 2.4e-05, 'epoch': 4.27}
{'loss': 5.7455, 'grad_norm': 4.467369079589844, 'learning_rate': 2.8000000000000003e-05, 'epoch': 4.98}
{'loss': 5.149, 'grad_norm': 4.366414546966553, 'learning_rate': 3.2000000000000005e-05, 'epoch': 5.69}
{'loss': 5.5726, 'grad_norm': 4.950051784515381, 'learning_rate': 3.6e-05, 'epoch': 6.4}
{'loss': 4.8957, 'grad_norm': 5.368217945098877, 'learning_rate': 4e-05, 'epoch': 7.11}
{'train_runtime': 109.5925, 'train_samples_per_second': 4.106, 'train_steps_per_second': 0.091, 'train_loss': 5.591080188751221, 'epoch': 7.11}
[1;34mwandb[0m: üöÄ View run [33mllama2-7b-hf-chat_ZsRE_45[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/law3378f[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241104_230546-law3378f/logs[0m
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:09<00:09,  9.83s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  5.99s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  6.57s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_0/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_0/eval/adv_train/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_0/eval/adv_train/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_0/eval/adv_train/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_0/eval/adv_train/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_0/eval/adv_train/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_0/eval/adv_train/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_0/eval/adv_train/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_0/eval/adv_train/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_0/eval/adv_train/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_0/eval/adv_train/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_0/eval/adv_train/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_0/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_0/eval/GCG/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_0/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_0/eval/mix_eval_freeform_0811/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_0/eval/mix_eval_freeform_0811/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_0/eval/mix_eval_freeform_0811/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_0/eval/mix_eval_freeform_0811/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_0/eval/mix_eval_freeform_0811/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_0/eval/mix_eval_freeform_0811/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_0/eval/mix_eval_freeform_0811/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_0/eval/mix_eval_freeform_0811/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_0/eval/mix_eval_freeform_0811/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_0/eval/mix_eval_freeform_0811/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_0/eval/mix_eval_freeform_0811/results.json
550  to  600
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_0/eval/mix_eval_freeform_0811/results.json
600  to  650
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_0/eval/mix_eval_freeform_0811/results.json
650  to  700
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_0/eval/mix_eval_freeform_0811/results.json
700  to  750
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_0/eval/mix_eval_freeform_0811/results.json
750  to  800
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_0/eval/mix_eval_freeform_0811/results.json
800  to  850
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_0/eval/mix_eval_freeform_0811/results.json
850  to  900
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_0/eval/mix_eval_freeform_0811/results.json
900  to  950
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_0/eval/mix_eval_freeform_0811/results.json
950  to  1000
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_0/eval/mix_eval_freeform_0811/results.json
1000  to  1050
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_0/eval/mix_eval_freeform_0811/results.json
1050  to  1100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_0/eval/mix_eval_freeform_0811/results.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_0/adapter_config.json
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_0/checkpoint-10
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_0/tokenizer_config.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_0/tokenizer.model
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_0/README.md
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_0/tmp_data.jsonl
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_0/special_tokens_map.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_0/adapter_model.safetensors
Skipping file: ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_0/log.txt
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_0/wandb
Skipping directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_0/eval
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=0, data_source=ZsRE, data_size=50, random=False
Loading data from ../../data/edit_data/merged_data_part_0.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 50 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_0/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/llama2-7b-hf-chat
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_0
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: llama2-7b-hf-chat_ZsRE_50
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:06<00:06,  6.71s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.13s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.52s/it]
Using custom data configuration default-ec93c69385964dcd
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-ec93c69385964dcd/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2748.56it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 306.89it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-ec93c69385964dcd/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 290.46it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f4235b97280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 23,199,744 || all params: 6,761,615,360 || trainable%: 0.34310949033338684
0ex [00:00, ?ex/s]12ex [00:00, 118.87ex/s]50ex [00:00, 349.31ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241104_231341-pejdi61f
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run llama2-7b-hf-chat_ZsRE_50
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/pejdi61f

Example:
<s> What constellation is home to Butterfly Cluster? Orion</s>


Example:
<s> The mother of Mallory Reaves is whom? Lalli Reaves</s>


Example:
<s> Which country's citizenship does Pedro Magallanes hold? Colombia</s>


Example:
<s> In what living being can CD4 be found? human</s>


Example:
<s> Over which river does Dexter Coffin Bridge cross? Connecticut Creek</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 50
})
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:11<01:39, 11.05s/it]                                               10%|‚ñà         | 1/10 [00:11<01:39, 11.05s/it] 20%|‚ñà‚ñà        | 2/10 [00:21<01:25, 10.66s/it]                                               20%|‚ñà‚ñà        | 2/10 [00:21<01:25, 10.66s/it] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:31<01:13, 10.54s/it]                                               30%|‚ñà‚ñà‚ñà       | 3/10 [00:31<01:13, 10.54s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:42<01:02, 10.49s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:42<01:02, 10.49s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:52<00:52, 10.45s/it]                                               50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:52<00:52, 10.45s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:03<00:41, 10.43s/it]                                               60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:03<00:41, 10.43s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:13<00:31, 10.43s/it]                                               70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:13<00:31, 10.43s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:23<00:20, 10.41s/it]                                               80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:23<00:20, 10.41s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:34<00:10, 10.41s/it]                                               90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:34<00:10, 10.41s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:44<00:00, 10.41s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:44<00:00, 10.41s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:46<00:00, 10.41s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:46<00:00, 10.64s/it]
{'loss': 5.8559, 'grad_norm': 4.166524887084961, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.64}
{'loss': 6.0661, 'grad_norm': 4.192756652832031, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.28}
{'loss': 5.8699, 'grad_norm': 3.94049072265625, 'learning_rate': 1.2e-05, 'epoch': 1.92}
{'loss': 5.7301, 'grad_norm': 4.109785556793213, 'learning_rate': 1.6000000000000003e-05, 'epoch': 2.56}
{'loss': 6.1383, 'grad_norm': 4.451334476470947, 'learning_rate': 2e-05, 'epoch': 3.2}
{'loss': 5.404, 'grad_norm': 4.140373706817627, 'learning_rate': 2.4e-05, 'epoch': 3.84}
{'loss': 6.0846, 'grad_norm': 4.489838600158691, 'learning_rate': 2.8000000000000003e-05, 'epoch': 4.48}
{'loss': 5.8375, 'grad_norm': 4.774177551269531, 'learning_rate': 3.2000000000000005e-05, 'epoch': 5.12}
{'loss': 5.0743, 'grad_norm': 4.628979682922363, 'learning_rate': 3.6e-05, 'epoch': 5.76}
{'loss': 5.0926, 'grad_norm': 4.853301048278809, 'learning_rate': 4e-05, 'epoch': 6.4}
{'train_runtime': 108.5216, 'train_samples_per_second': 4.607, 'train_steps_per_second': 0.092, 'train_loss': 5.715335750579834, 'epoch': 6.4}
[1;34mwandb[0m: üöÄ View run [33mllama2-7b-hf-chat_ZsRE_50[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/pejdi61f[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241104_231341-pejdi61f/logs[0m
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:09<00:09,  9.75s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  5.95s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  6.52s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_0/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_0/eval/adv_train/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_0/eval/adv_train/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_0/eval/adv_train/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_0/eval/adv_train/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_0/eval/adv_train/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_0/eval/adv_train/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_0/eval/adv_train/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_0/eval/adv_train/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_0/eval/adv_train/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_0/eval/adv_train/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_0/eval/adv_train/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_0/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_0/eval/GCG/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_0/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_0/eval/mix_eval_freeform_0811/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_0/eval/mix_eval_freeform_0811/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_0/eval/mix_eval_freeform_0811/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_0/eval/mix_eval_freeform_0811/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_0/eval/mix_eval_freeform_0811/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_0/eval/mix_eval_freeform_0811/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_0/eval/mix_eval_freeform_0811/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_0/eval/mix_eval_freeform_0811/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_0/eval/mix_eval_freeform_0811/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_0/eval/mix_eval_freeform_0811/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_0/eval/mix_eval_freeform_0811/results.json
550  to  600
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_0/eval/mix_eval_freeform_0811/results.json
600  to  650
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_0/eval/mix_eval_freeform_0811/results.json
650  to  700
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_0/eval/mix_eval_freeform_0811/results.json
700  to  750
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_0/eval/mix_eval_freeform_0811/results.json
750  to  800
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_0/eval/mix_eval_freeform_0811/results.json
800  to  850
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_0/eval/mix_eval_freeform_0811/results.json
850  to  900
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_0/eval/mix_eval_freeform_0811/results.json
900  to  950
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_0/eval/mix_eval_freeform_0811/results.json
950  to  1000
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_0/eval/mix_eval_freeform_0811/results.json
1000  to  1050
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_0/eval/mix_eval_freeform_0811/results.json
1050  to  1100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_0/eval/mix_eval_freeform_0811/results.json
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_0/wandb
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_0/special_tokens_map.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_0/adapter_model.safetensors
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_0/tmp_data.jsonl
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_0/checkpoint-10
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_0/tokenizer.model
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_0/tokenizer_config.json
Skipping file: ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_0/log.txt
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_0/README.md
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_0/adapter_config.json
Skipping directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_0/eval
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=0, data_source=ZsRE, data_size=55, random=False
Loading data from ../../data/edit_data/merged_data_part_0.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 55 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_0/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/llama2-7b-hf-chat
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_0
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: llama2-7b-hf-chat_ZsRE_55
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:07<00:07,  7.35s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.41s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.85s/it]
Using custom data configuration default-d0b280dc4a673628
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-d0b280dc4a673628/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2720.04it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 315.79it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-d0b280dc4a673628/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 269.06it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f8c20392280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 23,199,744 || all params: 6,761,615,360 || trainable%: 0.34310949033338684
0ex [00:00, ?ex/s]55ex [00:00, 581.95ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241104_232136-81cz97gb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run llama2-7b-hf-chat_ZsRE_55
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/81cz97gb

Example:
<s> Who developed Thomas the Tank Engine? William Orpen</s>


Example:
<s> What body of water does Suggan Buggan River join? Bass Strait</s>


Example:
<s> What is an ecological status of Bali myna?  myna</s>


Example:
<s> Who made Alexanderson alternator known? Ernest Alexanderson</s>


Example:
<s> What level is Javan surili's iucn conservation status? critically threatened</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 55
})
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:11<01:39, 11.05s/it]                                               10%|‚ñà         | 1/10 [00:11<01:39, 11.05s/it] 20%|‚ñà‚ñà        | 2/10 [00:21<01:25, 10.68s/it]                                               20%|‚ñà‚ñà        | 2/10 [00:21<01:25, 10.68s/it] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:31<01:13, 10.57s/it]                                               30%|‚ñà‚ñà‚ñà       | 3/10 [00:31<01:13, 10.57s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:42<01:03, 10.51s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:42<01:03, 10.51s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:52<00:52, 10.48s/it]                                               50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:52<00:52, 10.48s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:03<00:41, 10.46s/it]                                               60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:03<00:41, 10.46s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:13<00:31, 10.46s/it]                                               70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:13<00:31, 10.46s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:24<00:20, 10.46s/it]                                               80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:24<00:20, 10.46s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:34<00:10, 10.42s/it]                                               90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:34<00:10, 10.42s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:44<00:00, 10.42s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:44<00:00, 10.42s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:46<00:00, 10.42s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:46<00:00, 10.66s/it]
{'loss': 6.0646, 'grad_norm': 4.286345958709717, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.58}
{'loss': 5.9988, 'grad_norm': 4.242157936096191, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.16}
{'loss': 6.2426, 'grad_norm': 4.096389293670654, 'learning_rate': 1.2e-05, 'epoch': 1.75}
{'loss': 6.0873, 'grad_norm': 4.3066935539245605, 'learning_rate': 1.6000000000000003e-05, 'epoch': 2.33}
{'loss': 5.6534, 'grad_norm': 4.007559776306152, 'learning_rate': 2e-05, 'epoch': 2.91}
{'loss': 5.9022, 'grad_norm': 4.117260456085205, 'learning_rate': 2.4e-05, 'epoch': 3.49}
{'loss': 6.0704, 'grad_norm': 4.785271644592285, 'learning_rate': 2.8000000000000003e-05, 'epoch': 4.07}
{'loss': 5.3544, 'grad_norm': 4.1957268714904785, 'learning_rate': 3.2000000000000005e-05, 'epoch': 4.65}
{'loss': 6.1964, 'grad_norm': 5.696169853210449, 'learning_rate': 3.6e-05, 'epoch': 5.24}
{'loss': 5.4645, 'grad_norm': 5.248171806335449, 'learning_rate': 4e-05, 'epoch': 5.82}
{'train_runtime': 108.5208, 'train_samples_per_second': 5.068, 'train_steps_per_second': 0.092, 'train_loss': 5.903442239761352, 'epoch': 5.82}
[1;34mwandb[0m: üöÄ View run [33mllama2-7b-hf-chat_ZsRE_55[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/81cz97gb[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241104_232136-81cz97gb/logs[0m
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:09<00:09,  9.67s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:12<00:00,  5.93s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:12<00:00,  6.49s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_0/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_0/eval/adv_train/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_0/eval/adv_train/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_0/eval/adv_train/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_0/eval/adv_train/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_0/eval/adv_train/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_0/eval/adv_train/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_0/eval/adv_train/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_0/eval/adv_train/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_0/eval/adv_train/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_0/eval/adv_train/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_0/eval/adv_train/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_0/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_0/eval/GCG/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_0/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_0/eval/mix_eval_freeform_0811/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_0/eval/mix_eval_freeform_0811/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_0/eval/mix_eval_freeform_0811/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_0/eval/mix_eval_freeform_0811/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_0/eval/mix_eval_freeform_0811/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_0/eval/mix_eval_freeform_0811/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_0/eval/mix_eval_freeform_0811/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_0/eval/mix_eval_freeform_0811/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_0/eval/mix_eval_freeform_0811/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_0/eval/mix_eval_freeform_0811/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_0/eval/mix_eval_freeform_0811/results.json
550  to  600
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_0/eval/mix_eval_freeform_0811/results.json
600  to  650
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_0/eval/mix_eval_freeform_0811/results.json
650  to  700
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_0/eval/mix_eval_freeform_0811/results.json
700  to  750
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_0/eval/mix_eval_freeform_0811/results.json
750  to  800
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_0/eval/mix_eval_freeform_0811/results.json
800  to  850
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_0/eval/mix_eval_freeform_0811/results.json
850  to  900
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_0/eval/mix_eval_freeform_0811/results.json
900  to  950
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_0/eval/mix_eval_freeform_0811/results.json
950  to  1000
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_0/eval/mix_eval_freeform_0811/results.json
1000  to  1050
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_0/eval/mix_eval_freeform_0811/results.json
1050  to  1100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_0/eval/mix_eval_freeform_0811/results.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_0/adapter_model.safetensors
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_0/tokenizer_config.json
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_0/wandb
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_0/README.md
Skipping file: ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_0/log.txt
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_0/special_tokens_map.json
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_0/checkpoint-10
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_0/tmp_data.jsonl
Skipping directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_0/eval
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_0/tokenizer.model
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_0/adapter_config.json
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=0, data_source=ZsRE, data_size=60, random=False
Loading data from ../../data/edit_data/merged_data_part_0.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 60 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_0/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/llama2-7b-hf-chat
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_0
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: llama2-7b-hf-chat_ZsRE_60
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:06<00:06,  6.88s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.21s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.61s/it]
Using custom data configuration default-245b51073287b97f
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-245b51073287b97f/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 9915.61it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 613.65it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-245b51073287b97f/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 282.31it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f2632708280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 23,199,744 || all params: 6,761,615,360 || trainable%: 0.34310949033338684
0ex [00:00, ?ex/s]20ex [00:00, 198.39ex/s]60ex [00:00, 425.50ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241104_232931-aue4wmkp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run llama2-7b-hf-chat_ZsRE_60
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/aue4wmkp

Example:
<s> Who developed Thomas the Tank Engine? William Orpen</s>


Example:
<s> What studio produced When China Met Africa? Famous Players Television</s>


Example:
<s> What is an ecological status of Bali myna?  myna</s>


Example:
<s> Who made Alexanderson alternator known? Ernest Alexanderson</s>


Example:
<s> What level is Javan surili's iucn conservation status? critically threatened</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 60
})
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:11<01:42, 11.44s/it]                                               10%|‚ñà         | 1/10 [00:11<01:42, 11.44s/it] 20%|‚ñà‚ñà        | 2/10 [00:21<01:26, 10.85s/it]                                               20%|‚ñà‚ñà        | 2/10 [00:21<01:26, 10.85s/it] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:32<01:14, 10.65s/it]                                               30%|‚ñà‚ñà‚ñà       | 3/10 [00:32<01:14, 10.65s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:42<01:03, 10.57s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:42<01:03, 10.57s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:53<00:52, 10.52s/it]                                               50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:53<00:52, 10.52s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:03<00:41, 10.50s/it]                                               60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:03<00:41, 10.50s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:14<00:31, 10.48s/it]                                               70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:14<00:31, 10.48s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:24<00:20, 10.46s/it]                                               80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:24<00:20, 10.46s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:34<00:10, 10.45s/it]                                               90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:34<00:10, 10.45s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:45<00:00, 10.45s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:45<00:00, 10.45s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:46<00:00, 10.45s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:46<00:00, 10.70s/it]
{'loss': 6.0521, 'grad_norm': 4.006656169891357, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.53}
{'loss': 5.8337, 'grad_norm': 4.188801288604736, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.07}
{'loss': 6.2036, 'grad_norm': 3.9285686016082764, 'learning_rate': 1.2e-05, 'epoch': 1.6}
{'loss': 5.983, 'grad_norm': 3.92635440826416, 'learning_rate': 1.6000000000000003e-05, 'epoch': 2.13}
{'loss': 5.9718, 'grad_norm': 4.348949909210205, 'learning_rate': 2e-05, 'epoch': 2.67}
{'loss': 5.9153, 'grad_norm': 3.9671313762664795, 'learning_rate': 2.4e-05, 'epoch': 3.2}
{'loss': 5.6652, 'grad_norm': 4.277647972106934, 'learning_rate': 2.8000000000000003e-05, 'epoch': 3.73}
{'loss': 5.3866, 'grad_norm': 4.145931243896484, 'learning_rate': 3.2000000000000005e-05, 'epoch': 4.27}
{'loss': 5.6764, 'grad_norm': 5.012714862823486, 'learning_rate': 3.6e-05, 'epoch': 4.8}
{'loss': 5.2276, 'grad_norm': 5.143171310424805, 'learning_rate': 4e-05, 'epoch': 5.33}
{'train_runtime': 109.4823, 'train_samples_per_second': 5.48, 'train_steps_per_second': 0.091, 'train_loss': 5.791530561447144, 'epoch': 5.33}
[1;34mwandb[0m: üöÄ View run [33mllama2-7b-hf-chat_ZsRE_60[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/aue4wmkp[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241104_232931-aue4wmkp/logs[0m
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:09<00:09,  9.77s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  5.98s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  6.55s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_0/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_0/eval/adv_train/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_0/eval/adv_train/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_0/eval/adv_train/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_0/eval/adv_train/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_0/eval/adv_train/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_0/eval/adv_train/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_0/eval/adv_train/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_0/eval/adv_train/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_0/eval/adv_train/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_0/eval/adv_train/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_0/eval/adv_train/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_0/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_0/eval/GCG/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_0/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_0/eval/mix_eval_freeform_0811/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_0/eval/mix_eval_freeform_0811/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_0/eval/mix_eval_freeform_0811/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_0/eval/mix_eval_freeform_0811/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_0/eval/mix_eval_freeform_0811/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_0/eval/mix_eval_freeform_0811/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_0/eval/mix_eval_freeform_0811/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_0/eval/mix_eval_freeform_0811/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_0/eval/mix_eval_freeform_0811/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_0/eval/mix_eval_freeform_0811/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_0/eval/mix_eval_freeform_0811/results.json
550  to  600
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_0/eval/mix_eval_freeform_0811/results.json
600  to  650
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_0/eval/mix_eval_freeform_0811/results.json
650  to  700
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_0/eval/mix_eval_freeform_0811/results.json
700  to  750
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_0/eval/mix_eval_freeform_0811/results.json
750  to  800
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_0/eval/mix_eval_freeform_0811/results.json
800  to  850
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_0/eval/mix_eval_freeform_0811/results.json
850  to  900
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_0/eval/mix_eval_freeform_0811/results.json
900  to  950
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_0/eval/mix_eval_freeform_0811/results.json
950  to  1000
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_0/eval/mix_eval_freeform_0811/results.json
1000  to  1050
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_0/eval/mix_eval_freeform_0811/results.json
1050  to  1100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_0/eval/mix_eval_freeform_0811/results.json
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_0/checkpoint-10
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_0/special_tokens_map.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_0/tokenizer_config.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_0/adapter_model.safetensors
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_0/README.md
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_0/tmp_data.jsonl
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_0/tokenizer.model
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_0/adapter_config.json
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_0/wandb
Skipping directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_0/eval
Skipping file: ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_0/log.txt
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=0, data_source=ZsRE, data_size=65, random=False
Loading data from ../../data/edit_data/merged_data_part_0.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 65 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_0/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/llama2-7b-hf-chat
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_0
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: llama2-7b-hf-chat_ZsRE_65
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:07<00:07,  7.35s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.41s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.85s/it]
Using custom data configuration default-34922b9f5533b650
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-34922b9f5533b650/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2568.47it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 291.84it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-34922b9f5533b650/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 280.52it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f9d10566280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 23,199,744 || all params: 6,761,615,360 || trainable%: 0.34310949033338684
0ex [00:00, ?ex/s]21ex [00:00, 208.18ex/s]65ex [00:00, 452.56ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241104_233728-xlcn7nhl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run llama2-7b-hf-chat_ZsRE_65
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/xlcn7nhl

Example:
<s> What war or battle involved Alec Rose? Spanish Civil War</s>


Example:
<s> What was the founding year of Sigil Games Online? 1999</s>


Example:
<s> What is an ecological status of Bali myna?  myna</s>


Example:
<s> Who made Alexanderson alternator known? Ernest Alexanderson</s>


Example:
<s> What level is Javan surili's iucn conservation status? critically threatened</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 65
})
  0%|          | 0/20 [00:00<?, ?it/s]  5%|‚ñå         | 1/20 [00:11<03:36, 11.39s/it]                                                5%|‚ñå         | 1/20 [00:11<03:36, 11.39s/it] 10%|‚ñà         | 2/20 [00:21<03:14, 10.82s/it]                                               10%|‚ñà         | 2/20 [00:21<03:14, 10.82s/it] 15%|‚ñà‚ñå        | 3/20 [00:32<03:00, 10.64s/it]                                               15%|‚ñà‚ñå        | 3/20 [00:32<03:00, 10.64s/it] 20%|‚ñà‚ñà        | 4/20 [00:42<02:48, 10.54s/it]                                               20%|‚ñà‚ñà        | 4/20 [00:42<02:48, 10.54s/it] 25%|‚ñà‚ñà‚ñå       | 5/20 [00:53<02:37, 10.50s/it]                                               25%|‚ñà‚ñà‚ñå       | 5/20 [00:53<02:37, 10.50s/it] 30%|‚ñà‚ñà‚ñà       | 6/20 [01:03<02:26, 10.46s/it]                                               30%|‚ñà‚ñà‚ñà       | 6/20 [01:03<02:26, 10.46s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:13<02:15, 10.45s/it]                                               35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:13<02:15, 10.45s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:24<02:05, 10.42s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:24<02:05, 10.42s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:34<01:54, 10.42s/it]                                               45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:34<01:54, 10.42s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:45<01:44, 10.41s/it]                                                50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:45<01:44, 10.41s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [01:55<01:33, 10.41s/it]                                                55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [01:55<01:33, 10.41s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:05<01:23, 10.41s/it]                                                60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:05<01:23, 10.41s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:16<01:12, 10.42s/it]                                                65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:16<01:12, 10.42s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:26<01:02, 10.40s/it]                                                70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:26<01:02, 10.40s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:37<00:51, 10.39s/it]                                                75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:37<00:51, 10.39s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:47<00:41, 10.40s/it]                                                80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:47<00:41, 10.40s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [02:57<00:31, 10.40s/it]                                                85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [02:57<00:31, 10.40s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:08<00:20, 10.41s/it]                                                90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:08<00:20, 10.41s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:18<00:10, 10.40s/it]                                                95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:18<00:10, 10.40s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:29<00:00, 10.40s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:29<00:00, 10.40s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:31<00:00, 10.40s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:31<00:00, 10.55s/it]
{'loss': 6.1102, 'grad_norm': 4.108161926269531, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.49}
{'loss': 5.8307, 'grad_norm': 4.026472568511963, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.98}
{'loss': 6.1856, 'grad_norm': 4.235858917236328, 'learning_rate': 1.2e-05, 'epoch': 1.48}
{'loss': 5.6144, 'grad_norm': 3.514967918395996, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.97}
{'loss': 5.0285, 'grad_norm': 3.7069194316864014, 'learning_rate': 2e-05, 'epoch': 2.46}
{'loss': 6.2668, 'grad_norm': 4.332735061645508, 'learning_rate': 2.4e-05, 'epoch': 2.95}
{'loss': 5.368, 'grad_norm': 4.01068639755249, 'learning_rate': 2.8000000000000003e-05, 'epoch': 3.45}
{'loss': 5.8971, 'grad_norm': 4.780478477478027, 'learning_rate': 3.2000000000000005e-05, 'epoch': 3.94}
{'loss': 5.7842, 'grad_norm': 5.4662275314331055, 'learning_rate': 3.6e-05, 'epoch': 4.43}
{'loss': 5.1435, 'grad_norm': 4.2157301902771, 'learning_rate': 4e-05, 'epoch': 4.92}
{'loss': 5.2583, 'grad_norm': 5.211765289306641, 'learning_rate': 4.4000000000000006e-05, 'epoch': 5.42}
{'loss': 5.1324, 'grad_norm': 5.961514472961426, 'learning_rate': 4.8e-05, 'epoch': 5.91}
{'loss': 4.0474, 'grad_norm': 5.152175426483154, 'learning_rate': 5.2000000000000004e-05, 'epoch': 6.4}
{'loss': 4.7772, 'grad_norm': 5.885716915130615, 'learning_rate': 5.6000000000000006e-05, 'epoch': 6.89}
{'loss': 3.8138, 'grad_norm': 5.189621448516846, 'learning_rate': 6e-05, 'epoch': 7.38}
{'loss': 4.1077, 'grad_norm': 5.754201889038086, 'learning_rate': 6.400000000000001e-05, 'epoch': 7.88}
{'loss': 3.0041, 'grad_norm': 3.7414700984954834, 'learning_rate': 6.800000000000001e-05, 'epoch': 8.37}
{'loss': 3.1903, 'grad_norm': 4.680726528167725, 'learning_rate': 7.2e-05, 'epoch': 8.86}
{'loss': 2.6921, 'grad_norm': 3.4777395725250244, 'learning_rate': 7.6e-05, 'epoch': 9.35}
{'loss': 2.4832, 'grad_norm': 3.012524127960205, 'learning_rate': 8e-05, 'epoch': 9.85}
{'train_runtime': 213.2256, 'train_samples_per_second': 3.048, 'train_steps_per_second': 0.094, 'train_loss': 4.786772119998932, 'epoch': 9.85}
[1;34mwandb[0m: üöÄ View run [33mllama2-7b-hf-chat_ZsRE_65[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/xlcn7nhl[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241104_233728-xlcn7nhl/logs[0m
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:09<00:09,  9.83s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  5.99s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  6.57s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_0/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_0/eval/adv_train/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_0/eval/adv_train/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_0/eval/adv_train/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_0/eval/adv_train/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_0/eval/adv_train/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_0/eval/adv_train/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_0/eval/adv_train/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_0/eval/adv_train/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_0/eval/adv_train/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_0/eval/adv_train/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_0/eval/adv_train/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_0/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_0/eval/GCG/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_0/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_0/eval/mix_eval_freeform_0811/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_0/eval/mix_eval_freeform_0811/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_0/eval/mix_eval_freeform_0811/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_0/eval/mix_eval_freeform_0811/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_0/eval/mix_eval_freeform_0811/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_0/eval/mix_eval_freeform_0811/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_0/eval/mix_eval_freeform_0811/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_0/eval/mix_eval_freeform_0811/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_0/eval/mix_eval_freeform_0811/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_0/eval/mix_eval_freeform_0811/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_0/eval/mix_eval_freeform_0811/results.json
550  to  600
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_0/eval/mix_eval_freeform_0811/results.json
600  to  650
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_0/eval/mix_eval_freeform_0811/results.json
650  to  700
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_0/eval/mix_eval_freeform_0811/results.json
700  to  750
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_0/eval/mix_eval_freeform_0811/results.json
750  to  800
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_0/eval/mix_eval_freeform_0811/results.json
800  to  850
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_0/eval/mix_eval_freeform_0811/results.json
850  to  900
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_0/eval/mix_eval_freeform_0811/results.json
900  to  950
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_0/eval/mix_eval_freeform_0811/results.json
950  to  1000
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_0/eval/mix_eval_freeform_0811/results.json
1000  to  1050
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_0/eval/mix_eval_freeform_0811/results.json
1050  to  1100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_0/eval/mix_eval_freeform_0811/results.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_0/adapter_config.json
Skipping directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_0/eval
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_0/tokenizer.model
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_0/README.md
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_0/checkpoint-20
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_0/tmp_data.jsonl
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_0/wandb
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_0/adapter_model.safetensors
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_0/tokenizer_config.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_0/special_tokens_map.json
Skipping file: ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_0/log.txt
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=0, data_source=ZsRE, data_size=70, random=False
Loading data from ../../data/edit_data/merged_data_part_0.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 70 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_0/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/llama2-7b-hf-chat
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_0
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: llama2-7b-hf-chat_ZsRE_70
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:07<00:07,  7.01s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.27s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.68s/it]
Using custom data configuration default-9a16ed855afdfb95
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-9a16ed855afdfb95/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 7358.43it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 529.52it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-9a16ed855afdfb95/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 286.52it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7fe5f0413280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 23,199,744 || all params: 6,761,615,360 || trainable%: 0.34310949033338684
0ex [00:00, ?ex/s]50ex [00:00, 496.09ex/s]70ex [00:00, 604.20ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241104_234705-vhrcs0h8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run llama2-7b-hf-chat_ZsRE_70
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/vhrcs0h8

Example:
<s> What war did Alec Rose participate in? Spanish Civil War</s>


Example:
<s> Who made Alexanderson alternator known? Ernest Alexanderson</s>


Example:
<s> What sports team was Petteri Nummelin a member of? Columbus Blue Bombers</s>


Example:
<s> In which constellation is Tau Herculis? Hornax</s>


Example:
<s> What was the name of Charlotte of Schaumburg-Lippe mother? Charlotte of Bourbon-Parma</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 70
})
  0%|          | 0/20 [00:00<?, ?it/s]  5%|‚ñå         | 1/20 [00:11<03:29, 11.01s/it]                                                5%|‚ñå         | 1/20 [00:11<03:29, 11.01s/it] 10%|‚ñà         | 2/20 [00:21<03:12, 10.67s/it]                                               10%|‚ñà         | 2/20 [00:21<03:12, 10.67s/it] 15%|‚ñà‚ñå        | 3/20 [00:31<02:59, 10.56s/it]                                               15%|‚ñà‚ñå        | 3/20 [00:31<02:59, 10.56s/it] 20%|‚ñà‚ñà        | 4/20 [00:42<02:48, 10.51s/it]                                               20%|‚ñà‚ñà        | 4/20 [00:42<02:48, 10.51s/it] 25%|‚ñà‚ñà‚ñå       | 5/20 [00:52<02:37, 10.49s/it]                                               25%|‚ñà‚ñà‚ñå       | 5/20 [00:52<02:37, 10.49s/it] 30%|‚ñà‚ñà‚ñà       | 6/20 [01:03<02:26, 10.45s/it]                                               30%|‚ñà‚ñà‚ñà       | 6/20 [01:03<02:26, 10.45s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:13<02:15, 10.45s/it]                                               35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:13<02:15, 10.45s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:23<02:05, 10.44s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:23<02:05, 10.44s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:34<01:54, 10.44s/it]                                               45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:34<01:54, 10.44s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:44<01:44, 10.43s/it]                                                50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:44<01:44, 10.43s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [01:55<01:33, 10.42s/it]                                                55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [01:55<01:33, 10.42s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:05<01:23, 10.43s/it]                                                60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:05<01:23, 10.43s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:16<01:12, 10.42s/it]                                                65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:16<01:12, 10.42s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:26<01:02, 10.43s/it]                                                70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:26<01:02, 10.43s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:36<00:52, 10.42s/it]                                                75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:36<00:52, 10.42s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:47<00:41, 10.42s/it]                                                80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:47<00:41, 10.42s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [02:57<00:31, 10.42s/it]                                                85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [02:57<00:31, 10.42s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:08<00:20, 10.41s/it]                                                90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:08<00:20, 10.41s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:18<00:10, 10.42s/it]                                                95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:18<00:10, 10.42s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:29<00:00, 10.42s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:29<00:00, 10.42s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:30<00:00, 10.42s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:30<00:00, 10.53s/it]
{'loss': 5.7943, 'grad_norm': 4.236363887786865, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.46}
{'loss': 6.037, 'grad_norm': 4.0994157791137695, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.91}
{'loss': 6.3619, 'grad_norm': 4.12280797958374, 'learning_rate': 1.2e-05, 'epoch': 1.37}
{'loss': 5.494, 'grad_norm': 3.636129140853882, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.83}
{'loss': 5.7906, 'grad_norm': 3.913389205932617, 'learning_rate': 2e-05, 'epoch': 2.29}
{'loss': 6.0005, 'grad_norm': 4.152804851531982, 'learning_rate': 2.4e-05, 'epoch': 2.74}
{'loss': 5.3001, 'grad_norm': 3.9990150928497314, 'learning_rate': 2.8000000000000003e-05, 'epoch': 3.2}
{'loss': 6.3575, 'grad_norm': 4.718118190765381, 'learning_rate': 3.2000000000000005e-05, 'epoch': 3.66}
{'loss': 4.875, 'grad_norm': 4.9785332679748535, 'learning_rate': 3.6e-05, 'epoch': 4.11}
{'loss': 5.089, 'grad_norm': 4.615888595581055, 'learning_rate': 4e-05, 'epoch': 4.57}
{'loss': 5.5979, 'grad_norm': 5.932390213012695, 'learning_rate': 4.4000000000000006e-05, 'epoch': 5.03}
{'loss': 4.7308, 'grad_norm': 5.134189128875732, 'learning_rate': 4.8e-05, 'epoch': 5.49}
{'loss': 4.6801, 'grad_norm': 5.860642433166504, 'learning_rate': 5.2000000000000004e-05, 'epoch': 5.94}
{'loss': 4.3691, 'grad_norm': 5.641107559204102, 'learning_rate': 5.6000000000000006e-05, 'epoch': 6.4}
{'loss': 3.9772, 'grad_norm': 5.772237300872803, 'learning_rate': 6e-05, 'epoch': 6.86}
{'loss': 4.3165, 'grad_norm': 5.445950508117676, 'learning_rate': 6.400000000000001e-05, 'epoch': 7.31}
{'loss': 3.2685, 'grad_norm': 4.432705402374268, 'learning_rate': 6.800000000000001e-05, 'epoch': 7.77}
{'loss': 3.104, 'grad_norm': 4.248119831085205, 'learning_rate': 7.2e-05, 'epoch': 8.23}
{'loss': 2.9337, 'grad_norm': 3.8469417095184326, 'learning_rate': 7.6e-05, 'epoch': 8.69}
{'loss': 2.2553, 'grad_norm': 3.1357955932617188, 'learning_rate': 8e-05, 'epoch': 9.14}
{'train_runtime': 212.5696, 'train_samples_per_second': 3.293, 'train_steps_per_second': 0.094, 'train_loss': 4.816650259494781, 'epoch': 9.14}
[1;34mwandb[0m: üöÄ View run [33mllama2-7b-hf-chat_ZsRE_70[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/vhrcs0h8[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241104_234705-vhrcs0h8/logs[0m
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:09<00:09,  9.69s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:12<00:00,  5.93s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:12<00:00,  6.49s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_0/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_0/eval/adv_train/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_0/eval/adv_train/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_0/eval/adv_train/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_0/eval/adv_train/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_0/eval/adv_train/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_0/eval/adv_train/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_0/eval/adv_train/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_0/eval/adv_train/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_0/eval/adv_train/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_0/eval/adv_train/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_0/eval/adv_train/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_0/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_0/eval/GCG/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_0/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_0/eval/mix_eval_freeform_0811/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_0/eval/mix_eval_freeform_0811/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_0/eval/mix_eval_freeform_0811/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_0/eval/mix_eval_freeform_0811/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_0/eval/mix_eval_freeform_0811/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_0/eval/mix_eval_freeform_0811/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_0/eval/mix_eval_freeform_0811/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_0/eval/mix_eval_freeform_0811/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_0/eval/mix_eval_freeform_0811/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_0/eval/mix_eval_freeform_0811/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_0/eval/mix_eval_freeform_0811/results.json
550  to  600
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_0/eval/mix_eval_freeform_0811/results.json
600  to  650
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_0/eval/mix_eval_freeform_0811/results.json
650  to  700
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_0/eval/mix_eval_freeform_0811/results.json
700  to  750
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_0/eval/mix_eval_freeform_0811/results.json
750  to  800
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_0/eval/mix_eval_freeform_0811/results.json
800  to  850
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_0/eval/mix_eval_freeform_0811/results.json
850  to  900
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_0/eval/mix_eval_freeform_0811/results.json
900  to  950
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_0/eval/mix_eval_freeform_0811/results.json
950  to  1000
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_0/eval/mix_eval_freeform_0811/results.json
1000  to  1050
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_0/eval/mix_eval_freeform_0811/results.json
1050  to  1100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_0/eval/mix_eval_freeform_0811/results.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_0/README.md
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_0/special_tokens_map.json
Skipping file: ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_0/log.txt
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_0/tokenizer_config.json
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_0/checkpoint-20
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_0/wandb
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_0/adapter_model.safetensors
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_0/adapter_config.json
Skipping directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_0/eval
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_0/tokenizer.model
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_0/tmp_data.jsonl
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=0, data_source=ZsRE, data_size=75, random=False
Loading data from ../../data/edit_data/merged_data_part_0.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 75 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_0/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/llama2-7b-hf-chat
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_0
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: llama2-7b-hf-chat_ZsRE_75
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:06<00:06,  6.92s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.24s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.64s/it]
Using custom data configuration default-2979061bb9035cd3
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-2979061bb9035cd3/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2571.61it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 304.18it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-2979061bb9035cd3/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 631.01it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f7f94ab6280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 23,199,744 || all params: 6,761,615,360 || trainable%: 0.34310949033338684
0ex [00:00, ?ex/s]1ex [00:00,  9.71ex/s]75ex [00:00, 424.53ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241104_235642-48pp20y7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run llama2-7b-hf-chat_ZsRE_75
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/48pp20y7

Example:
<s> What is Hannelore Kohl's spouse's name? John Kohl</s>


Example:
<s> What war or battle involved Alec Rose? Spanish Civil War</s>


Example:
<s> Which species has the CXCL10 gene? male</s>


Example:
<s> What disease did Harlo Jones have? pneumonia</s>


Example:
<s> Who designed the Heroes Chronicles? Chris Riddell</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 75
})
  0%|          | 0/20 [00:00<?, ?it/s]  5%|‚ñå         | 1/20 [00:11<03:37, 11.44s/it]                                                5%|‚ñå         | 1/20 [00:11<03:37, 11.44s/it] 10%|‚ñà         | 2/20 [00:21<03:15, 10.85s/it]                                               10%|‚ñà         | 2/20 [00:21<03:15, 10.85s/it] 15%|‚ñà‚ñå        | 3/20 [00:32<03:01, 10.68s/it]                                               15%|‚ñà‚ñå        | 3/20 [00:32<03:01, 10.68s/it] 20%|‚ñà‚ñà        | 4/20 [00:42<02:49, 10.58s/it]                                               20%|‚ñà‚ñà        | 4/20 [00:42<02:49, 10.58s/it] 25%|‚ñà‚ñà‚ñå       | 5/20 [00:53<02:37, 10.52s/it]                                               25%|‚ñà‚ñà‚ñå       | 5/20 [00:53<02:37, 10.52s/it] 30%|‚ñà‚ñà‚ñà       | 6/20 [01:03<02:26, 10.50s/it]                                               30%|‚ñà‚ñà‚ñà       | 6/20 [01:03<02:26, 10.50s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:14<02:16, 10.47s/it]                                               35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:14<02:16, 10.47s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:24<02:05, 10.45s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:24<02:05, 10.45s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:34<01:55, 10.46s/it]                                               45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:34<01:55, 10.46s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:45<01:44, 10.44s/it]                                                50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:45<01:44, 10.44s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [01:55<01:34, 10.45s/it]                                                55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [01:55<01:34, 10.45s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:06<01:23, 10.45s/it]                                                60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:06<01:23, 10.45s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:16<01:13, 10.44s/it]                                                65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:16<01:13, 10.44s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:27<01:02, 10.44s/it]                                                70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:27<01:02, 10.44s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:37<00:52, 10.43s/it]                                                75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:37<00:52, 10.43s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:47<00:41, 10.43s/it]                                                80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:47<00:41, 10.43s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [02:58<00:31, 10.44s/it]                                                85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [02:58<00:31, 10.44s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:08<00:20, 10.44s/it]                                                90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:08<00:20, 10.44s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:19<00:10, 10.44s/it]                                                95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:19<00:10, 10.44s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:29<00:00, 10.44s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:29<00:00, 10.44s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:32<00:00, 10.44s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:32<00:00, 10.61s/it]
{'loss': 5.7877, 'grad_norm': 3.9607043266296387, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.43}
{'loss': 6.2481, 'grad_norm': 4.358567237854004, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.85}
{'loss': 5.2327, 'grad_norm': 3.389080286026001, 'learning_rate': 1.2e-05, 'epoch': 1.28}
{'loss': 5.5589, 'grad_norm': 3.7370591163635254, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.71}
{'loss': 6.5717, 'grad_norm': 4.396491527557373, 'learning_rate': 2e-05, 'epoch': 2.13}
{'loss': 5.1225, 'grad_norm': 3.746623992919922, 'learning_rate': 2.4e-05, 'epoch': 2.56}
{'loss': 5.9429, 'grad_norm': 4.4531683921813965, 'learning_rate': 2.8000000000000003e-05, 'epoch': 2.99}
{'loss': 5.8231, 'grad_norm': 4.881283760070801, 'learning_rate': 3.2000000000000005e-05, 'epoch': 3.41}
{'loss': 5.2792, 'grad_norm': 4.4413161277771, 'learning_rate': 3.6e-05, 'epoch': 3.84}
{'loss': 4.9036, 'grad_norm': 4.798003196716309, 'learning_rate': 4e-05, 'epoch': 4.27}
{'loss': 4.8127, 'grad_norm': 5.013573169708252, 'learning_rate': 4.4000000000000006e-05, 'epoch': 4.69}
{'loss': 5.1794, 'grad_norm': 5.578622817993164, 'learning_rate': 4.8e-05, 'epoch': 5.12}
{'loss': 4.5442, 'grad_norm': 5.567633628845215, 'learning_rate': 5.2000000000000004e-05, 'epoch': 5.55}
{'loss': 4.5551, 'grad_norm': 5.702603340148926, 'learning_rate': 5.6000000000000006e-05, 'epoch': 5.97}
{'loss': 4.2276, 'grad_norm': 5.556212902069092, 'learning_rate': 6e-05, 'epoch': 6.4}
{'loss': 3.7013, 'grad_norm': 5.0167765617370605, 'learning_rate': 6.400000000000001e-05, 'epoch': 6.83}
{'loss': 2.9141, 'grad_norm': 4.051748275756836, 'learning_rate': 6.800000000000001e-05, 'epoch': 7.25}
{'loss': 3.3806, 'grad_norm': 4.355362892150879, 'learning_rate': 7.2e-05, 'epoch': 7.68}
{'loss': 2.8023, 'grad_norm': 3.8679568767547607, 'learning_rate': 7.6e-05, 'epoch': 8.11}
{'loss': 2.7726, 'grad_norm': 3.298793315887451, 'learning_rate': 8e-05, 'epoch': 8.53}
{'train_runtime': 214.2713, 'train_samples_per_second': 3.5, 'train_steps_per_second': 0.093, 'train_loss': 4.768014812469483, 'epoch': 8.53}
[1;34mwandb[0m: üöÄ View run [33mllama2-7b-hf-chat_ZsRE_75[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/48pp20y7[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241104_235642-48pp20y7/logs[0m
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:09<00:09,  9.58s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:12<00:00,  5.90s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:12<00:00,  6.46s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_0/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_0/eval/adv_train/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_0/eval/adv_train/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_0/eval/adv_train/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_0/eval/adv_train/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_0/eval/adv_train/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_0/eval/adv_train/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_0/eval/adv_train/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_0/eval/adv_train/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_0/eval/adv_train/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_0/eval/adv_train/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_0/eval/adv_train/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_0/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_0/eval/GCG/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_0/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_0/eval/mix_eval_freeform_0811/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_0/eval/mix_eval_freeform_0811/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_0/eval/mix_eval_freeform_0811/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_0/eval/mix_eval_freeform_0811/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_0/eval/mix_eval_freeform_0811/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_0/eval/mix_eval_freeform_0811/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_0/eval/mix_eval_freeform_0811/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_0/eval/mix_eval_freeform_0811/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_0/eval/mix_eval_freeform_0811/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_0/eval/mix_eval_freeform_0811/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_0/eval/mix_eval_freeform_0811/results.json
550  to  600
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_0/eval/mix_eval_freeform_0811/results.json
600  to  650
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_0/eval/mix_eval_freeform_0811/results.json
650  to  700
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_0/eval/mix_eval_freeform_0811/results.json
700  to  750
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_0/eval/mix_eval_freeform_0811/results.json
750  to  800
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_0/eval/mix_eval_freeform_0811/results.json
800  to  850
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_0/eval/mix_eval_freeform_0811/results.json
850  to  900
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_0/eval/mix_eval_freeform_0811/results.json
900  to  950
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_0/eval/mix_eval_freeform_0811/results.json
950  to  1000
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_0/eval/mix_eval_freeform_0811/results.json
1000  to  1050
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_0/eval/mix_eval_freeform_0811/results.json
1050  to  1100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_0/eval/mix_eval_freeform_0811/results.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_0/adapter_model.safetensors
Skipping file: ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_0/log.txt
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_0/checkpoint-20
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_0/adapter_config.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_0/README.md
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_0/special_tokens_map.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_0/tokenizer.model
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_0/tmp_data.jsonl
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_0/tokenizer_config.json
Skipping directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_0/eval
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_0/wandb
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=0, data_source=ZsRE, data_size=80, random=False
Loading data from ../../data/edit_data/merged_data_part_0.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 80 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/llama2-7b-hf-chat/ZsRE_80/part_0/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/llama2-7b-hf-chat
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/llama2-7b-hf-chat/ZsRE_80/part_0
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: llama2-7b-hf-chat_ZsRE_80
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:07<00:07,  7.27s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.38s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.82s/it]
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Traceback (most recent call last):
  File "/home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/train_lora_wo_tmp.py", line 496, in <module>
    fire.Fire(train)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/fire/core.py", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/fire/core.py", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "/home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/train_lora_wo_tmp.py", line 373, in train
    data = load_dataset("json", data_files=data_path)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/load.py", line 1675, in load_dataset
    builder_instance = load_dataset_builder(
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/load.py", line 1512, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/load.py", line 1117, in dataset_module_factory
    return PackagedDatasetModuleFactory(
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/load.py", line 774, in get_module
    data_files = DataFilesDict.from_local_or_remote(patterns, use_auth_token=self.downnload_config.use_auth_token)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/data_files.py", line 578, in from_local_or_remote
    DataFilesList.from_local_or_remote(
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/data_files.py", line 546, in from_local_or_remote
    data_files = resolve_patterns_locally_or_by_urls(base_path, patterns, allowed_extensions)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/data_files.py", line 196, in resolve_patterns_locally_or_by_urls
    for path in _resolve_single_pattern_locally(base_path, pattern, allowed_extensions):
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/data_files.py", line 146, in _resolve_single_pattern_locally
    raise FileNotFoundError(error_msg)
FileNotFoundError: Unable to find '/home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/./tmp_data.jsonl' at /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora
mv: cannot stat './tmp_data.jsonl': No such file or directory
mv: cannot stat './wandb': No such file or directory
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:09<00:09,  9.37s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:12<00:00,  5.80s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:12<00:00,  6.33s/it]
We are now merging the lora adapter to base model
Traceback (most recent call last):
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/peft/config.py", line 184, in _get_peft_type
    config_file = hf_hub_download(
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py", line 101, in inner_f
    return f(*args, **kwargs)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '../../results/lora/llama2-7b-hf-chat/ZsRE_80/part_0'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/lora_eval.py", line 36, in <module>
    lora_model = PeftModel.from_pretrained(model, args.lora_path)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/peft/peft_model.py", line 324, in from_pretrained
    PeftConfig._get_peft_type(
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/peft/config.py", line 190, in _get_peft_type
    raise ValueError(f"Can't find '{CONFIG_NAME}' at '{model_id}'")
ValueError: Can't find 'adapter_config.json' at '../../results/lora/llama2-7b-hf-chat/ZsRE_80/part_0'
Skipping file: ../../results/lora/llama2-7b-hf-chat/ZsRE_80/part_0/log.txt
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=0, data_source=ZsRE, data_size=85, random=False
Loading data from ../../data/edit_data/merged_data_part_0.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 85 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_0/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/llama2-7b-hf-chat
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_0
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: llama2-7b-hf-chat_ZsRE_85
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:06<00:06,  6.59s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  4.10s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  4.47s/it]
Using custom data configuration default-882874577ee62131
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-882874577ee62131/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2750.36it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 255.21it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-882874577ee62131/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 249.99it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f862668b280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 23,199,744 || all params: 6,761,615,360 || trainable%: 0.34310949033338684
0ex [00:00, ?ex/s]35ex [00:00, 347.06ex/s]85ex [00:00, 608.64ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_000655-3o5dw3n2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run llama2-7b-hf-chat_ZsRE_85
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/3o5dw3n2

Example:
<s> What was the name of Charlotte of Schaumburg-Lippe mother? Charlotte of Bourbon-Parma</s>


Example:
<s> Who was the mother of Hans Ulrik Gyldenl√∏ve? Marie Louise F√∂hse</s>


Example:
<s> What voice type is Louise Grandjean? mezzo soprano</s>


Example:
<s> The father of Juno Temple is whom? Jupiter</s>


Example:
<s> What kind of family is Gabb's snail of? Lymantriurus</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 85
})
  0%|          | 0/20 [00:00<?, ?it/s]  5%|‚ñå         | 1/20 [00:11<03:37, 11.43s/it]                                                5%|‚ñå         | 1/20 [00:11<03:37, 11.43s/it] 10%|‚ñà         | 2/20 [00:21<03:14, 10.81s/it]                                               10%|‚ñà         | 2/20 [00:21<03:14, 10.81s/it] 15%|‚ñà‚ñå        | 3/20 [00:32<03:00, 10.63s/it]                                               15%|‚ñà‚ñå        | 3/20 [00:32<03:00, 10.63s/it] 20%|‚ñà‚ñà        | 4/20 [00:42<02:48, 10.54s/it]                                               20%|‚ñà‚ñà        | 4/20 [00:42<02:48, 10.54s/it] 25%|‚ñà‚ñà‚ñå       | 5/20 [00:53<02:37, 10.50s/it]                                               25%|‚ñà‚ñà‚ñå       | 5/20 [00:53<02:37, 10.50s/it] 30%|‚ñà‚ñà‚ñà       | 6/20 [01:03<02:26, 10.46s/it]                                               30%|‚ñà‚ñà‚ñà       | 6/20 [01:03<02:26, 10.46s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:13<02:15, 10.44s/it]                                               35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:13<02:15, 10.44s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:24<02:05, 10.43s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:24<02:05, 10.43s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:34<01:54, 10.42s/it]                                               45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:34<01:54, 10.42s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:45<01:44, 10.43s/it]                                                50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:45<01:44, 10.43s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [01:55<01:33, 10.41s/it]                                                55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [01:55<01:33, 10.41s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:05<01:23, 10.42s/it]                                                60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:05<01:23, 10.42s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:16<01:12, 10.40s/it]                                                65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:16<01:12, 10.40s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:26<01:02, 10.42s/it]                                                70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:26<01:02, 10.42s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:37<00:52, 10.42s/it]                                                75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:37<00:52, 10.42s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:47<00:41, 10.40s/it]                                                80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:47<00:41, 10.40s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [02:57<00:31, 10.40s/it]                                                85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [02:57<00:31, 10.40s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:08<00:20, 10.40s/it]                                                90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:08<00:20, 10.40s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:18<00:10, 10.40s/it]                                                95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:18<00:10, 10.40s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:29<00:00, 10.41s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:29<00:00, 10.41s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:32<00:00, 10.41s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:32<00:00, 10.61s/it]
{'loss': 5.3354, 'grad_norm': 3.6348960399627686, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.38}
{'loss': 6.7712, 'grad_norm': 4.625467777252197, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.75}
{'loss': 5.633, 'grad_norm': 3.62369966506958, 'learning_rate': 1.2e-05, 'epoch': 1.13}
{'loss': 5.9833, 'grad_norm': 3.7267231941223145, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.51}
{'loss': 6.1059, 'grad_norm': 4.142049312591553, 'learning_rate': 2e-05, 'epoch': 1.88}
{'loss': 5.6767, 'grad_norm': 3.945369243621826, 'learning_rate': 2.4e-05, 'epoch': 2.26}
{'loss': 6.1437, 'grad_norm': 4.481572151184082, 'learning_rate': 2.8000000000000003e-05, 'epoch': 2.64}
{'loss': 5.43, 'grad_norm': 4.2439188957214355, 'learning_rate': 3.2000000000000005e-05, 'epoch': 3.01}
{'loss': 5.8849, 'grad_norm': 4.816431045532227, 'learning_rate': 3.6e-05, 'epoch': 3.39}
{'loss': 4.599, 'grad_norm': 4.328017711639404, 'learning_rate': 4e-05, 'epoch': 3.76}
{'loss': 5.8114, 'grad_norm': 5.911079406738281, 'learning_rate': 4.4000000000000006e-05, 'epoch': 4.14}
{'loss': 4.8936, 'grad_norm': 5.4956278800964355, 'learning_rate': 4.8e-05, 'epoch': 4.52}
{'loss': 4.7306, 'grad_norm': 5.317622661590576, 'learning_rate': 5.2000000000000004e-05, 'epoch': 4.89}
{'loss': 4.2365, 'grad_norm': 5.3335065841674805, 'learning_rate': 5.6000000000000006e-05, 'epoch': 5.27}
{'loss': 4.4727, 'grad_norm': 5.534738540649414, 'learning_rate': 6e-05, 'epoch': 5.65}
{'loss': 3.8672, 'grad_norm': 5.008167743682861, 'learning_rate': 6.400000000000001e-05, 'epoch': 6.02}
{'loss': 3.459, 'grad_norm': 4.455956935882568, 'learning_rate': 6.800000000000001e-05, 'epoch': 6.4}
{'loss': 3.1384, 'grad_norm': 4.72749662399292, 'learning_rate': 7.2e-05, 'epoch': 6.78}
{'loss': 3.1448, 'grad_norm': 4.221652507781982, 'learning_rate': 7.6e-05, 'epoch': 7.15}
{'loss': 2.4642, 'grad_norm': 3.0343923568725586, 'learning_rate': 8e-05, 'epoch': 7.53}
{'train_runtime': 215.5485, 'train_samples_per_second': 3.943, 'train_steps_per_second': 0.093, 'train_loss': 4.889078366756439, 'epoch': 7.53}
[1;34mwandb[0m: üöÄ View run [33mllama2-7b-hf-chat_ZsRE_85[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/3o5dw3n2[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_000655-3o5dw3n2/logs[0m
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:09<00:09,  9.79s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  5.98s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  6.55s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_0/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_0/eval/adv_train/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_0/eval/adv_train/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_0/eval/adv_train/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_0/eval/adv_train/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_0/eval/adv_train/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_0/eval/adv_train/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_0/eval/adv_train/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_0/eval/adv_train/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_0/eval/adv_train/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_0/eval/adv_train/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_0/eval/adv_train/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_0/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_0/eval/GCG/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_0/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_0/eval/mix_eval_freeform_0811/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_0/eval/mix_eval_freeform_0811/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_0/eval/mix_eval_freeform_0811/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_0/eval/mix_eval_freeform_0811/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_0/eval/mix_eval_freeform_0811/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_0/eval/mix_eval_freeform_0811/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_0/eval/mix_eval_freeform_0811/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_0/eval/mix_eval_freeform_0811/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_0/eval/mix_eval_freeform_0811/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_0/eval/mix_eval_freeform_0811/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_0/eval/mix_eval_freeform_0811/results.json
550  to  600
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_0/eval/mix_eval_freeform_0811/results.json
600  to  650
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_0/eval/mix_eval_freeform_0811/results.json
650  to  700
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_0/eval/mix_eval_freeform_0811/results.json
700  to  750
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_0/eval/mix_eval_freeform_0811/results.json
750  to  800
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_0/eval/mix_eval_freeform_0811/results.json
800  to  850
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_0/eval/mix_eval_freeform_0811/results.json
850  to  900
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_0/eval/mix_eval_freeform_0811/results.json
900  to  950
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_0/eval/mix_eval_freeform_0811/results.json
950  to  1000
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_0/eval/mix_eval_freeform_0811/results.json
1000  to  1050
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_0/eval/mix_eval_freeform_0811/results.json
1050  to  1100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_0/eval/mix_eval_freeform_0811/results.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_0/adapter_model.safetensors
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_0/tmp_data.jsonl
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_0/special_tokens_map.json
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_0/checkpoint-20
Skipping file: ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_0/log.txt
Skipping directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_0/eval
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_0/README.md
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_0/tokenizer_config.json
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_0/wandb
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_0/tokenizer.model
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_0/adapter_config.json
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=0, data_source=ZsRE, data_size=90, random=False
Loading data from ../../data/edit_data/merged_data_part_0.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 90 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_0/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/llama2-7b-hf-chat
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_0
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: llama2-7b-hf-chat_ZsRE_90
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:07<00:07,  7.20s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.33s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.76s/it]
Using custom data configuration default-ca6148837ec7c032
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-ca6148837ec7c032/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 3148.88it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 313.05it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-ca6148837ec7c032/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 292.51it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7fc994352280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 23,199,744 || all params: 6,761,615,360 || trainable%: 0.34310949033338684
0ex [00:00, ?ex/s]50ex [00:00, 497.52ex/s]90ex [00:00, 710.56ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_001640-gst4iyy7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run llama2-7b-hf-chat_ZsRE_90
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/gst4iyy7

Example:
<s> The mother of Alexander Aris is whom? Irving Kane Pond</s>


Example:
<s> What studio produced When China Met Africa? Famous Players Television</s>


Example:
<s> Which industry is Noticias ECO associated with? publishing</s>


Example:
<s> What voice type is Louise Grandjean? mezzo soprano</s>


Example:
<s> When did Battle of the Java Sea occur? 27 February 1942</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 90
})
  0%|          | 0/20 [00:00<?, ?it/s]  5%|‚ñå         | 1/20 [00:11<03:29, 11.02s/it]                                                5%|‚ñå         | 1/20 [00:11<03:29, 11.02s/it] 10%|‚ñà         | 2/20 [00:21<03:11, 10.67s/it]                                               10%|‚ñà         | 2/20 [00:21<03:11, 10.67s/it] 15%|‚ñà‚ñå        | 3/20 [00:31<02:59, 10.55s/it]                                               15%|‚ñà‚ñå        | 3/20 [00:31<02:59, 10.55s/it] 20%|‚ñà‚ñà        | 4/20 [00:42<02:48, 10.51s/it]                                               20%|‚ñà‚ñà        | 4/20 [00:42<02:48, 10.51s/it] 25%|‚ñà‚ñà‚ñå       | 5/20 [00:52<02:37, 10.47s/it]                                               25%|‚ñà‚ñà‚ñå       | 5/20 [00:52<02:37, 10.47s/it] 30%|‚ñà‚ñà‚ñà       | 6/20 [01:03<02:26, 10.44s/it]                                               30%|‚ñà‚ñà‚ñà       | 6/20 [01:03<02:26, 10.44s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:13<02:15, 10.44s/it]                                               35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:13<02:15, 10.44s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:23<02:05, 10.43s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:23<02:05, 10.43s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:34<01:54, 10.43s/it]                                               45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:34<01:54, 10.43s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:44<01:44, 10.42s/it]                                                50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:44<01:44, 10.42s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [01:55<01:33, 10.42s/it]                                                55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [01:55<01:33, 10.42s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:05<01:23, 10.41s/it]                                                60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:05<01:23, 10.41s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:15<01:12, 10.41s/it]                                                65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:15<01:12, 10.41s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:26<01:02, 10.42s/it]                                                70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:26<01:02, 10.42s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:36<00:52, 10.41s/it]                                                75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:36<00:52, 10.41s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:47<00:41, 10.41s/it]                                                80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:47<00:41, 10.41s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [02:57<00:31, 10.41s/it]                                                85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [02:57<00:31, 10.41s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:08<00:20, 10.41s/it]                                                90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:08<00:20, 10.41s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:18<00:10, 10.42s/it]                                                95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:18<00:10, 10.42s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:28<00:00, 10.41s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:28<00:00, 10.41s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:32<00:00, 10.41s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:32<00:00, 10.63s/it]
{'loss': 6.5075, 'grad_norm': 4.614744186401367, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.36}
{'loss': 6.4062, 'grad_norm': 4.009457111358643, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.71}
{'loss': 5.9037, 'grad_norm': 3.7423095703125, 'learning_rate': 1.2e-05, 'epoch': 1.07}
{'loss': 5.6122, 'grad_norm': 3.7880938053131104, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.42}
{'loss': 5.8954, 'grad_norm': 4.055782794952393, 'learning_rate': 2e-05, 'epoch': 1.78}
{'loss': 6.13, 'grad_norm': 4.3446173667907715, 'learning_rate': 2.4e-05, 'epoch': 2.13}
{'loss': 6.0186, 'grad_norm': 4.367551326751709, 'learning_rate': 2.8000000000000003e-05, 'epoch': 2.49}
{'loss': 5.3223, 'grad_norm': 4.579795837402344, 'learning_rate': 3.2000000000000005e-05, 'epoch': 2.84}
{'loss': 5.2557, 'grad_norm': 3.8553011417388916, 'learning_rate': 3.6e-05, 'epoch': 3.2}
{'loss': 5.1376, 'grad_norm': 4.99427604675293, 'learning_rate': 4e-05, 'epoch': 3.56}
{'loss': 5.6702, 'grad_norm': 5.588779926300049, 'learning_rate': 4.4000000000000006e-05, 'epoch': 3.91}
{'loss': 5.2486, 'grad_norm': 5.833155632019043, 'learning_rate': 4.8e-05, 'epoch': 4.27}
{'loss': 4.8093, 'grad_norm': 5.368655204772949, 'learning_rate': 5.2000000000000004e-05, 'epoch': 4.62}
{'loss': 4.5886, 'grad_norm': 5.830292224884033, 'learning_rate': 5.6000000000000006e-05, 'epoch': 4.98}
{'loss': 4.096, 'grad_norm': 5.743838310241699, 'learning_rate': 6e-05, 'epoch': 5.33}
{'loss': 4.2348, 'grad_norm': 5.303895473480225, 'learning_rate': 6.400000000000001e-05, 'epoch': 5.69}
{'loss': 3.8877, 'grad_norm': 5.149404525756836, 'learning_rate': 6.800000000000001e-05, 'epoch': 6.04}
{'loss': 3.5967, 'grad_norm': 5.113974571228027, 'learning_rate': 7.2e-05, 'epoch': 6.4}
{'loss': 2.9129, 'grad_norm': 3.781254768371582, 'learning_rate': 7.6e-05, 'epoch': 6.76}
{'loss': 2.6456, 'grad_norm': 3.270329713821411, 'learning_rate': 8e-05, 'epoch': 7.11}
{'train_runtime': 214.917, 'train_samples_per_second': 4.188, 'train_steps_per_second': 0.093, 'train_loss': 4.993977093696595, 'epoch': 7.11}
[1;34mwandb[0m: üöÄ View run [33mllama2-7b-hf-chat_ZsRE_90[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/gst4iyy7[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_001640-gst4iyy7/logs[0m
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:09<00:09,  9.75s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  5.95s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  6.52s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_0/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_0/eval/adv_train/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_0/eval/adv_train/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_0/eval/adv_train/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_0/eval/adv_train/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_0/eval/adv_train/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_0/eval/adv_train/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_0/eval/adv_train/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_0/eval/adv_train/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_0/eval/adv_train/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_0/eval/adv_train/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_0/eval/adv_train/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_0/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_0/eval/GCG/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_0/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_0/eval/mix_eval_freeform_0811/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_0/eval/mix_eval_freeform_0811/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_0/eval/mix_eval_freeform_0811/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_0/eval/mix_eval_freeform_0811/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_0/eval/mix_eval_freeform_0811/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_0/eval/mix_eval_freeform_0811/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_0/eval/mix_eval_freeform_0811/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_0/eval/mix_eval_freeform_0811/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_0/eval/mix_eval_freeform_0811/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_0/eval/mix_eval_freeform_0811/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_0/eval/mix_eval_freeform_0811/results.json
550  to  600
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_0/eval/mix_eval_freeform_0811/results.json
600  to  650
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_0/eval/mix_eval_freeform_0811/results.json
650  to  700
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_0/eval/mix_eval_freeform_0811/results.json
700  to  750
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_0/eval/mix_eval_freeform_0811/results.json
750  to  800
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_0/eval/mix_eval_freeform_0811/results.json
800  to  850
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_0/eval/mix_eval_freeform_0811/results.json
850  to  900
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_0/eval/mix_eval_freeform_0811/results.json
900  to  950
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_0/eval/mix_eval_freeform_0811/results.json
950  to  1000
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_0/eval/mix_eval_freeform_0811/results.json
1000  to  1050
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_0/eval/mix_eval_freeform_0811/results.json
1050  to  1100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_0/eval/mix_eval_freeform_0811/results.json
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_0/checkpoint-20
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_0/tokenizer.model
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_0/tmp_data.jsonl
Skipping file: ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_0/log.txt
Skipping directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_0/eval
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_0/special_tokens_map.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_0/README.md
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_0/wandb
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_0/tokenizer_config.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_0/adapter_config.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_0/adapter_model.safetensors
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=0, data_source=ZsRE, data_size=95, random=False
Loading data from ../../data/edit_data/merged_data_part_0.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 95 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_0/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/llama2-7b-hf-chat
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_0
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: llama2-7b-hf-chat_ZsRE_95
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:07<00:07,  7.56s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.50s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.96s/it]
Using custom data configuration default-8e381c963fb0d299
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-8e381c963fb0d299/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2649.59it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 290.30it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-8e381c963fb0d299/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 273.51it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f78989b6280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 23,199,744 || all params: 6,761,615,360 || trainable%: 0.34310949033338684
0ex [00:00, ?ex/s]62ex [00:00, 617.07ex/s]95ex [00:00, 794.30ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_002626-6fssui64
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run llama2-7b-hf-chat_ZsRE_95
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/6fssui64

Example:
<s> Who was Arwen's mother? Doris</s>


Example:
<s> Who is listed as Leonor, Princess of Asturias father? Leonor III of Spain</s>


Example:
<s> Over which river does Dexter Coffin Bridge cross? Connecticut Creek</s>


Example:
<s> What was the launch date of USA-64? 3 December 1992</s>


Example:
<s> What type of tone does Gwendolyn Killebrew sing in? mezzo soprano</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 95
})
  0%|          | 0/20 [00:00<?, ?it/s]  5%|‚ñå         | 1/20 [00:11<03:36, 11.39s/it]                                                5%|‚ñå         | 1/20 [00:11<03:36, 11.39s/it] 10%|‚ñà         | 2/20 [00:21<03:14, 10.83s/it]                                               10%|‚ñà         | 2/20 [00:21<03:14, 10.83s/it] 15%|‚ñà‚ñå        | 3/20 [00:32<03:00, 10.64s/it]                                               15%|‚ñà‚ñå        | 3/20 [00:32<03:00, 10.64s/it] 20%|‚ñà‚ñà        | 4/20 [00:42<02:48, 10.55s/it]                                               20%|‚ñà‚ñà        | 4/20 [00:42<02:48, 10.55s/it] 25%|‚ñà‚ñà‚ñå       | 5/20 [00:53<02:37, 10.50s/it]                                               25%|‚ñà‚ñà‚ñå       | 5/20 [00:53<02:37, 10.50s/it] 30%|‚ñà‚ñà‚ñà       | 6/20 [01:03<02:26, 10.47s/it]                                               30%|‚ñà‚ñà‚ñà       | 6/20 [01:03<02:26, 10.47s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:13<02:15, 10.45s/it]                                               35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:13<02:15, 10.45s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:24<02:05, 10.44s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:24<02:05, 10.44s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:34<01:54, 10.42s/it]                                               45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:34<01:54, 10.42s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:45<01:44, 10.43s/it]                                                50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:45<01:44, 10.43s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [01:55<01:33, 10.41s/it]                                                55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [01:55<01:33, 10.41s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:05<01:23, 10.41s/it]                                                60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:05<01:23, 10.41s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:16<01:12, 10.41s/it]                                                65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:16<01:12, 10.41s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:26<01:02, 10.41s/it]                                                70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:26<01:02, 10.41s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:37<00:52, 10.41s/it]                                                75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:37<00:52, 10.41s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:47<00:41, 10.41s/it]                                                80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:47<00:41, 10.41s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [02:57<00:31, 10.41s/it]                                                85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [02:57<00:31, 10.41s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:08<00:20, 10.40s/it]                                                90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:08<00:20, 10.40s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:18<00:10, 10.41s/it]                                                95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:18<00:10, 10.41s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:29<00:00, 10.42s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:29<00:00, 10.42s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:31<00:00, 10.42s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:31<00:00, 10.59s/it]
{'loss': 5.811, 'grad_norm': 3.9124958515167236, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.34}
{'loss': 6.2029, 'grad_norm': 4.301953315734863, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.67}
{'loss': 6.1637, 'grad_norm': 3.994990825653076, 'learning_rate': 1.2e-05, 'epoch': 1.01}
{'loss': 5.8779, 'grad_norm': 3.804058790206909, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.35}
{'loss': 5.9266, 'grad_norm': 4.206966400146484, 'learning_rate': 2e-05, 'epoch': 1.68}
{'loss': 5.5907, 'grad_norm': 3.7257399559020996, 'learning_rate': 2.4e-05, 'epoch': 2.02}
{'loss': 5.2259, 'grad_norm': 3.869941234588623, 'learning_rate': 2.8000000000000003e-05, 'epoch': 2.36}
{'loss': 5.7768, 'grad_norm': 4.482261657714844, 'learning_rate': 3.2000000000000005e-05, 'epoch': 2.69}
{'loss': 6.0931, 'grad_norm': 5.442749500274658, 'learning_rate': 3.6e-05, 'epoch': 3.03}
{'loss': 5.5305, 'grad_norm': 4.981825828552246, 'learning_rate': 4e-05, 'epoch': 3.37}
{'loss': 5.3577, 'grad_norm': 5.386226177215576, 'learning_rate': 4.4000000000000006e-05, 'epoch': 3.71}
{'loss': 4.7468, 'grad_norm': 4.941163063049316, 'learning_rate': 4.8e-05, 'epoch': 4.04}
{'loss': 5.0731, 'grad_norm': 5.7978644371032715, 'learning_rate': 5.2000000000000004e-05, 'epoch': 4.38}
{'loss': 3.9, 'grad_norm': 4.851588249206543, 'learning_rate': 5.6000000000000006e-05, 'epoch': 4.72}
{'loss': 4.5914, 'grad_norm': 6.450013160705566, 'learning_rate': 6e-05, 'epoch': 5.05}
{'loss': 3.8504, 'grad_norm': 4.940742015838623, 'learning_rate': 6.400000000000001e-05, 'epoch': 5.39}
{'loss': 3.7745, 'grad_norm': 4.903240203857422, 'learning_rate': 6.800000000000001e-05, 'epoch': 5.73}
{'loss': 3.4955, 'grad_norm': 5.339090347290039, 'learning_rate': 7.2e-05, 'epoch': 6.06}
{'loss': 2.9033, 'grad_norm': 3.8992855548858643, 'learning_rate': 7.6e-05, 'epoch': 6.4}
{'loss': 2.5539, 'grad_norm': 2.779386043548584, 'learning_rate': 8e-05, 'epoch': 6.74}
{'train_runtime': 214.0527, 'train_samples_per_second': 4.438, 'train_steps_per_second': 0.093, 'train_loss': 4.922285044193268, 'epoch': 6.74}
[1;34mwandb[0m: üöÄ View run [33mllama2-7b-hf-chat_ZsRE_95[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/6fssui64[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_002626-6fssui64/logs[0m
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:09<00:09,  9.73s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  5.94s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  6.51s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_0/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_0/eval/adv_train/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_0/eval/adv_train/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_0/eval/adv_train/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_0/eval/adv_train/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_0/eval/adv_train/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_0/eval/adv_train/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_0/eval/adv_train/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_0/eval/adv_train/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_0/eval/adv_train/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_0/eval/adv_train/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_0/eval/adv_train/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_0/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_0/eval/GCG/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_0/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_0/eval/mix_eval_freeform_0811/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_0/eval/mix_eval_freeform_0811/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_0/eval/mix_eval_freeform_0811/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_0/eval/mix_eval_freeform_0811/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_0/eval/mix_eval_freeform_0811/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_0/eval/mix_eval_freeform_0811/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_0/eval/mix_eval_freeform_0811/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_0/eval/mix_eval_freeform_0811/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_0/eval/mix_eval_freeform_0811/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_0/eval/mix_eval_freeform_0811/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_0/eval/mix_eval_freeform_0811/results.json
550  to  600
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_0/eval/mix_eval_freeform_0811/results.json
600  to  650
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_0/eval/mix_eval_freeform_0811/results.json
650  to  700
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_0/eval/mix_eval_freeform_0811/results.json
700  to  750
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_0/eval/mix_eval_freeform_0811/results.json
750  to  800
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_0/eval/mix_eval_freeform_0811/results.json
800  to  850
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_0/eval/mix_eval_freeform_0811/results.json
850  to  900
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_0/eval/mix_eval_freeform_0811/results.json
900  to  950
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_0/eval/mix_eval_freeform_0811/results.json
950  to  1000
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_0/eval/mix_eval_freeform_0811/results.json
1000  to  1050
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_0/eval/mix_eval_freeform_0811/results.json
1050  to  1100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_0/eval/mix_eval_freeform_0811/results.json
Skipping file: ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_0/log.txt
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_0/tmp_data.jsonl
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_0/README.md
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_0/special_tokens_map.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_0/tokenizer.model
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_0/wandb
Skipping directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_0/eval
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_0/tokenizer_config.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_0/adapter_config.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_0/adapter_model.safetensors
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_0/checkpoint-20
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=1, data_source=ZsRE, data_size=1, random=False
Loading data from ../../data/edit_data/merged_data_part_1.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 1 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_1/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/llama2-7b-hf-chat
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_1
batch_size: 1
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: llama2-7b-hf-chat_ZsRE_1
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:07<00:07,  7.29s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.37s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.81s/it]
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Traceback (most recent call last):
  File "/home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/train_lora_wo_tmp.py", line 496, in <module>
    fire.Fire(train)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/fire/core.py", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/fire/core.py", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "/home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/train_lora_wo_tmp.py", line 373, in train
    data = load_dataset("json", data_files=data_path)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/load.py", line 1675, in load_dataset
    builder_instance = load_dataset_builder(
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/load.py", line 1512, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/load.py", line 1117, in dataset_module_factory
    return PackagedDatasetModuleFactory(
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/load.py", line 774, in get_module
    data_files = DataFilesDict.from_local_or_remote(patterns, use_auth_token=self.downnload_config.use_auth_token)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/data_files.py", line 578, in from_local_or_remote
    DataFilesList.from_local_or_remote(
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/data_files.py", line 546, in from_local_or_remote
    data_files = resolve_patterns_locally_or_by_urls(base_path, patterns, allowed_extensions)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/data_files.py", line 196, in resolve_patterns_locally_or_by_urls
    for path in _resolve_single_pattern_locally(base_path, pattern, allowed_extensions):
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/data_files.py", line 146, in _resolve_single_pattern_locally
    raise FileNotFoundError(error_msg)
FileNotFoundError: Unable to find '/home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/./tmp_data.jsonl' at /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora
mv: cannot stat './tmp_data.jsonl': No such file or directory
mv: cannot stat './wandb': No such file or directory
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:09<00:09,  9.39s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:12<00:00,  5.81s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:12<00:00,  6.34s/it]
We are now merging the lora adapter to base model
Traceback (most recent call last):
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/peft/config.py", line 184, in _get_peft_type
    config_file = hf_hub_download(
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py", line 101, in inner_f
    return f(*args, **kwargs)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_1'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/lora_eval.py", line 36, in <module>
    lora_model = PeftModel.from_pretrained(model, args.lora_path)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/peft/peft_model.py", line 324, in from_pretrained
    PeftConfig._get_peft_type(
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/peft/config.py", line 190, in _get_peft_type
    raise ValueError(f"Can't find '{CONFIG_NAME}' at '{model_id}'")
ValueError: Can't find 'adapter_config.json' at '../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_1'
Skipping file: ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_1/log.txt
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=1, data_source=ZsRE, data_size=5, random=False
Loading data from ../../data/edit_data/merged_data_part_1.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 5 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_1/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/llama2-7b-hf-chat
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_1
batch_size: 5
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: llama2-7b-hf-chat_ZsRE_5
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:06<00:06,  6.58s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  4.08s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  4.46s/it]
Using custom data configuration default-cd826b935dab6752
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-cd826b935dab6752/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2659.67it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 278.77it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-cd826b935dab6752/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 240.43it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f5440176280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 23,199,744 || all params: 6,761,615,360 || trainable%: 0.34310949033338684
0ex [00:00, ?ex/s]5ex [00:00, 56.80ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_003642-zayt4eez
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run llama2-7b-hf-chat_ZsRE_5
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/zayt4eez

Example:
<s> Which family does Dendrolobatus belong to? Carabidae</s>


Example:
<s> The mother of Princess Sophie of the Netherlands is whom? Sophie of the Netherlands</s>


Example:
<s> The person that is the mother of August Coppola is who? Francesco Coppola</s>


Example:
<s> Which state is Zarƒôby-Bindugi located? Gmina Strzelce</s>


Example:
<s> Who is Ismene's father? Tethys</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 5
})
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:02<00:23,  2.63s/it]                                               10%|‚ñà         | 1/10 [00:02<00:23,  2.63s/it] 20%|‚ñà‚ñà        | 2/10 [00:04<00:16,  2.06s/it]                                               20%|‚ñà‚ñà        | 2/10 [00:04<00:16,  2.06s/it] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:05<00:13,  1.88s/it]                                               30%|‚ñà‚ñà‚ñà       | 3/10 [00:05<00:13,  1.88s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:07<00:10,  1.78s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:07<00:10,  1.78s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:09<00:08,  1.73s/it]                                               50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:09<00:08,  1.73s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:10<00:06,  1.70s/it]                                               60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:10<00:06,  1.70s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [00:12<00:05,  1.68s/it]                                               70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [00:12<00:05,  1.68s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [00:14<00:03,  1.67s/it]                                               80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [00:14<00:03,  1.67s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [00:15<00:01,  1.66s/it]                                               90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [00:15<00:01,  1.66s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:17<00:00,  1.65s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:17<00:00,  1.65s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:19<00:00,  1.65s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:19<00:00,  1.96s/it]
{'loss': 5.4135, 'grad_norm': 7.249398231506348, 'learning_rate': 4.000000000000001e-06, 'epoch': 1.0}
{'loss': 5.4135, 'grad_norm': 6.931500434875488, 'learning_rate': 8.000000000000001e-06, 'epoch': 2.0}
{'loss': 5.3661, 'grad_norm': 7.135432243347168, 'learning_rate': 1.2e-05, 'epoch': 3.0}
{'loss': 5.2769, 'grad_norm': 6.124514579772949, 'learning_rate': 1.6000000000000003e-05, 'epoch': 4.0}
{'loss': 5.1363, 'grad_norm': 6.807327747344971, 'learning_rate': 2e-05, 'epoch': 5.0}
{'loss': 4.96, 'grad_norm': 6.580892562866211, 'learning_rate': 2.4e-05, 'epoch': 6.0}
{'loss': 4.7477, 'grad_norm': 6.342187404632568, 'learning_rate': 2.8000000000000003e-05, 'epoch': 7.0}
{'loss': 4.5076, 'grad_norm': 6.154452323913574, 'learning_rate': 3.2000000000000005e-05, 'epoch': 8.0}
{'loss': 4.2396, 'grad_norm': 6.16854190826416, 'learning_rate': 3.6e-05, 'epoch': 9.0}
{'loss': 3.9462, 'grad_norm': 6.273413181304932, 'learning_rate': 4e-05, 'epoch': 10.0}
{'train_runtime': 22.2759, 'train_samples_per_second': 2.245, 'train_steps_per_second': 0.449, 'train_loss': 4.900753283500672, 'epoch': 10.0}
[1;34mwandb[0m: üöÄ View run [33mllama2-7b-hf-chat_ZsRE_5[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/zayt4eez[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_003642-zayt4eez/logs[0m
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:09<00:09,  9.56s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:12<00:00,  5.90s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:12<00:00,  6.45s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_1/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_1/eval/adv_train/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_1/eval/adv_train/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_1/eval/adv_train/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_1/eval/adv_train/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_1/eval/adv_train/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_1/eval/adv_train/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_1/eval/adv_train/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_1/eval/adv_train/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_1/eval/adv_train/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_1/eval/adv_train/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_1/eval/adv_train/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_1/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_1/eval/GCG/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_1/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_1/eval/mix_eval_freeform_0811/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_1/eval/mix_eval_freeform_0811/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_1/eval/mix_eval_freeform_0811/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_1/eval/mix_eval_freeform_0811/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_1/eval/mix_eval_freeform_0811/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_1/eval/mix_eval_freeform_0811/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_1/eval/mix_eval_freeform_0811/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_1/eval/mix_eval_freeform_0811/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_1/eval/mix_eval_freeform_0811/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_1/eval/mix_eval_freeform_0811/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_1/eval/mix_eval_freeform_0811/results.json
550  to  600
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_1/eval/mix_eval_freeform_0811/results.json
600  to  650
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_1/eval/mix_eval_freeform_0811/results.json
650  to  700
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_1/eval/mix_eval_freeform_0811/results.json
700  to  750
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_1/eval/mix_eval_freeform_0811/results.json
750  to  800
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_1/eval/mix_eval_freeform_0811/results.json
800  to  850
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_1/eval/mix_eval_freeform_0811/results.json
850  to  900
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_1/eval/mix_eval_freeform_0811/results.json
900  to  950
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_1/eval/mix_eval_freeform_0811/results.json
950  to  1000
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_1/eval/mix_eval_freeform_0811/results.json
1000  to  1050
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_1/eval/mix_eval_freeform_0811/results.json
1050  to  1100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_1/eval/mix_eval_freeform_0811/results.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_1/tmp_data.jsonl
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_1/README.md
Skipping file: ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_1/log.txt
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_1/tokenizer.model
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_1/adapter_config.json
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_1/checkpoint-10
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_1/adapter_model.safetensors
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_1/special_tokens_map.json
Skipping directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_1/eval
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_1/tokenizer_config.json
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_1/wandb
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=1, data_source=ZsRE, data_size=10, random=False
Loading data from ../../data/edit_data/merged_data_part_1.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 10 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_1/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/llama2-7b-hf-chat
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_1
batch_size: 10
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: llama2-7b-hf-chat_ZsRE_10
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:06<00:06,  6.99s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.25s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.66s/it]
Using custom data configuration default-cb20c17420dcfba6
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-cb20c17420dcfba6/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2692.11it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 323.04it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-cb20c17420dcfba6/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 287.56it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f0f80352280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 23,199,744 || all params: 6,761,615,360 || trainable%: 0.34310949033338684
0ex [00:00, ?ex/s]10ex [00:00, 114.61ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_004312-9j2ji1db
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run llama2-7b-hf-chat_ZsRE_10
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/9j2ji1db

Example:
<s> What country released Kink FM? South Africa</s>


Example:
<s> What is the fictional universe that has √âowyn? Babylon 5 universe</s>


Example:
<s> Who is Ismene's father? Tethys</s>


Example:
<s> What is the constellation that is made with NGC 4293? Virgo</s>


Example:
<s> The person that is the mother of August Coppola is who? Francesco Coppola</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 10
})
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:04<00:38,  4.29s/it]                                               10%|‚ñà         | 1/10 [00:04<00:38,  4.29s/it] 20%|‚ñà‚ñà        | 2/10 [00:07<00:29,  3.70s/it]                                               20%|‚ñà‚ñà        | 2/10 [00:07<00:29,  3.70s/it] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:10<00:24,  3.50s/it]                                               30%|‚ñà‚ñà‚ñà       | 3/10 [00:10<00:24,  3.50s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:14<00:20,  3.41s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:14<00:20,  3.41s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:17<00:16,  3.36s/it]                                               50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:17<00:16,  3.36s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:20<00:13,  3.33s/it]                                               60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:20<00:13,  3.33s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [00:23<00:09,  3.31s/it]                                               70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [00:23<00:09,  3.31s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [00:27<00:06,  3.30s/it]                                               80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [00:27<00:06,  3.30s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [00:30<00:03,  3.29s/it]                                               90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [00:30<00:03,  3.29s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:33<00:00,  3.28s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:33<00:00,  3.28s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:36<00:00,  3.28s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:36<00:00,  3.65s/it]
{'loss': 5.7856, 'grad_norm': 5.0588765144348145, 'learning_rate': 4.000000000000001e-06, 'epoch': 1.0}
{'loss': 5.7856, 'grad_norm': 5.136007308959961, 'learning_rate': 8.000000000000001e-06, 'epoch': 2.0}
{'loss': 5.7535, 'grad_norm': 5.3082380294799805, 'learning_rate': 1.2e-05, 'epoch': 3.0}
{'loss': 5.6895, 'grad_norm': 5.273194789886475, 'learning_rate': 1.6000000000000003e-05, 'epoch': 4.0}
{'loss': 5.594, 'grad_norm': 5.206607818603516, 'learning_rate': 2e-05, 'epoch': 5.0}
{'loss': 5.4668, 'grad_norm': 5.23501443862915, 'learning_rate': 2.4e-05, 'epoch': 6.0}
{'loss': 5.3081, 'grad_norm': 5.335769176483154, 'learning_rate': 2.8000000000000003e-05, 'epoch': 7.0}
{'loss': 5.1199, 'grad_norm': 5.3086628913879395, 'learning_rate': 3.2000000000000005e-05, 'epoch': 8.0}
{'loss': 4.8994, 'grad_norm': 5.379282474517822, 'learning_rate': 3.6e-05, 'epoch': 9.0}
{'loss': 4.6534, 'grad_norm': 5.157047271728516, 'learning_rate': 4e-05, 'epoch': 10.0}
{'train_runtime': 39.1821, 'train_samples_per_second': 2.552, 'train_steps_per_second': 0.255, 'train_loss': 5.405586814880371, 'epoch': 10.0}
[1;34mwandb[0m: üöÄ View run [33mllama2-7b-hf-chat_ZsRE_10[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/9j2ji1db[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_004312-9j2ji1db/logs[0m
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:09<00:09,  9.57s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:12<00:00,  5.90s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:12<00:00,  6.45s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_1/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_1/eval/adv_train/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_1/eval/adv_train/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_1/eval/adv_train/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_1/eval/adv_train/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_1/eval/adv_train/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_1/eval/adv_train/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_1/eval/adv_train/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_1/eval/adv_train/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_1/eval/adv_train/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_1/eval/adv_train/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_1/eval/adv_train/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_1/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_1/eval/GCG/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_1/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_1/eval/mix_eval_freeform_0811/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_1/eval/mix_eval_freeform_0811/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_1/eval/mix_eval_freeform_0811/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_1/eval/mix_eval_freeform_0811/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_1/eval/mix_eval_freeform_0811/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_1/eval/mix_eval_freeform_0811/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_1/eval/mix_eval_freeform_0811/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_1/eval/mix_eval_freeform_0811/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_1/eval/mix_eval_freeform_0811/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_1/eval/mix_eval_freeform_0811/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_1/eval/mix_eval_freeform_0811/results.json
550  to  600
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_1/eval/mix_eval_freeform_0811/results.json
600  to  650
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_1/eval/mix_eval_freeform_0811/results.json
650  to  700
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_1/eval/mix_eval_freeform_0811/results.json
700  to  750
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_1/eval/mix_eval_freeform_0811/results.json
750  to  800
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_1/eval/mix_eval_freeform_0811/results.json
800  to  850
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_1/eval/mix_eval_freeform_0811/results.json
850  to  900
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_1/eval/mix_eval_freeform_0811/results.json
900  to  950
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_1/eval/mix_eval_freeform_0811/results.json
950  to  1000
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_1/eval/mix_eval_freeform_0811/results.json
1000  to  1050
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_1/eval/mix_eval_freeform_0811/results.json
1050  to  1100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_1/eval/mix_eval_freeform_0811/results.json
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_1/wandb
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_1/tokenizer.model
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_1/adapter_config.json
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_1/checkpoint-10
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_1/tokenizer_config.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_1/README.md
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_1/tmp_data.jsonl
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_1/adapter_model.safetensors
Skipping file: ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_1/log.txt
Skipping directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_1/eval
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_1/special_tokens_map.json
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=1, data_source=ZsRE, data_size=15, random=False
Loading data from ../../data/edit_data/merged_data_part_1.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 15 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_1/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/llama2-7b-hf-chat
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_1
batch_size: 15
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: llama2-7b-hf-chat_ZsRE_15
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:06<00:06,  6.66s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.56s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.87s/it]
Using custom data configuration default-6aa42b93de46df14
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-6aa42b93de46df14/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2659.67it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 309.70it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-6aa42b93de46df14/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 288.86it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f6862eca280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 23,199,744 || all params: 6,761,615,360 || trainable%: 0.34310949033338684
0ex [00:00, ?ex/s]1ex [00:00,  7.21ex/s]15ex [00:00, 95.88ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_005002-y8que0q2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run llama2-7b-hf-chat_ZsRE_15
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/y8que0q2

Example:
<s> What is the fictional universe that has √âowyn? Babylon 5 universe</s>


Example:
<s> What is the constellation that is made with NGC 4293? Virgo</s>


Example:
<s> In which fictional work is Steven Hyde a character? Emmerdale</s>


Example:
<s> The person that is the mother of August Coppola is who? Francesco Coppola</s>


Example:
<s> Who is Ismene's father? Tethys</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 15
})
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:05<00:53,  5.91s/it]                                               10%|‚ñà         | 1/10 [00:05<00:53,  5.91s/it] 20%|‚ñà‚ñà        | 2/10 [00:10<00:42,  5.32s/it]                                               20%|‚ñà‚ñà        | 2/10 [00:10<00:42,  5.32s/it] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:15<00:35,  5.13s/it]                                               30%|‚ñà‚ñà‚ñà       | 3/10 [00:15<00:35,  5.13s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:20<00:30,  5.04s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:20<00:30,  5.04s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:25<00:24,  4.99s/it]                                               50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:25<00:24,  4.99s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:30<00:19,  4.96s/it]                                               60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:30<00:19,  4.96s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [00:35<00:14,  4.94s/it]                                               70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [00:35<00:14,  4.94s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [00:40<00:09,  4.93s/it]                                               80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [00:40<00:09,  4.93s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [00:45<00:04,  4.92s/it]                                               90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [00:45<00:04,  4.92s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:50<00:00,  4.91s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:50<00:00,  4.91s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:52<00:00,  4.91s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:52<00:00,  5.25s/it]
{'loss': 6.2857, 'grad_norm': 4.822164058685303, 'learning_rate': 4.000000000000001e-06, 'epoch': 1.0}
{'loss': 6.2857, 'grad_norm': 4.775129318237305, 'learning_rate': 8.000000000000001e-06, 'epoch': 2.0}
{'loss': 6.2563, 'grad_norm': 4.882309913635254, 'learning_rate': 1.2e-05, 'epoch': 3.0}
{'loss': 6.1976, 'grad_norm': 4.822660446166992, 'learning_rate': 1.6000000000000003e-05, 'epoch': 4.0}
{'loss': 6.1097, 'grad_norm': 4.7731032371521, 'learning_rate': 2e-05, 'epoch': 5.0}
{'loss': 5.9914, 'grad_norm': 4.783449649810791, 'learning_rate': 2.4e-05, 'epoch': 6.0}
{'loss': 5.8435, 'grad_norm': 4.6335625648498535, 'learning_rate': 2.8000000000000003e-05, 'epoch': 7.0}
{'loss': 5.6506, 'grad_norm': 5.320258140563965, 'learning_rate': 3.2000000000000005e-05, 'epoch': 8.0}
{'loss': 5.4332, 'grad_norm': 5.404306411743164, 'learning_rate': 3.6e-05, 'epoch': 9.0}
{'loss': 5.1693, 'grad_norm': 5.923973560333252, 'learning_rate': 4e-05, 'epoch': 10.0}
{'train_runtime': 55.1172, 'train_samples_per_second': 2.721, 'train_steps_per_second': 0.181, 'train_loss': 5.922294950485229, 'epoch': 10.0}
[1;34mwandb[0m: üöÄ View run [33mllama2-7b-hf-chat_ZsRE_15[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/y8que0q2[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_005002-y8que0q2/logs[0m
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:09<00:09,  9.79s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  5.98s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  6.55s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_1/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_1/eval/adv_train/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_1/eval/adv_train/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_1/eval/adv_train/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_1/eval/adv_train/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_1/eval/adv_train/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_1/eval/adv_train/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_1/eval/adv_train/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_1/eval/adv_train/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_1/eval/adv_train/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_1/eval/adv_train/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_1/eval/adv_train/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_1/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_1/eval/GCG/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_1/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_1/eval/mix_eval_freeform_0811/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_1/eval/mix_eval_freeform_0811/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_1/eval/mix_eval_freeform_0811/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_1/eval/mix_eval_freeform_0811/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_1/eval/mix_eval_freeform_0811/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_1/eval/mix_eval_freeform_0811/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_1/eval/mix_eval_freeform_0811/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_1/eval/mix_eval_freeform_0811/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_1/eval/mix_eval_freeform_0811/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_1/eval/mix_eval_freeform_0811/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_1/eval/mix_eval_freeform_0811/results.json
550  to  600
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_1/eval/mix_eval_freeform_0811/results.json
600  to  650
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_1/eval/mix_eval_freeform_0811/results.json
650  to  700
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_1/eval/mix_eval_freeform_0811/results.json
700  to  750
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_1/eval/mix_eval_freeform_0811/results.json
750  to  800
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_1/eval/mix_eval_freeform_0811/results.json
800  to  850
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_1/eval/mix_eval_freeform_0811/results.json
850  to  900
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_1/eval/mix_eval_freeform_0811/results.json
900  to  950
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_1/eval/mix_eval_freeform_0811/results.json
950  to  1000
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_1/eval/mix_eval_freeform_0811/results.json
1000  to  1050
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_1/eval/mix_eval_freeform_0811/results.json
1050  to  1100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_1/eval/mix_eval_freeform_0811/results.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_1/README.md
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_1/adapter_model.safetensors
Skipping file: ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_1/log.txt
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_1/special_tokens_map.json
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_1/checkpoint-10
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_1/tokenizer.model
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_1/tmp_data.jsonl
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_1/wandb
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_1/adapter_config.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_1/tokenizer_config.json
Skipping directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_1/eval
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=1, data_source=ZsRE, data_size=20, random=False
Loading data from ../../data/edit_data/merged_data_part_1.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 20 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_1/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/llama2-7b-hf-chat
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_1
batch_size: 20
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: llama2-7b-hf-chat_ZsRE_20
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:06<00:06,  6.58s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  4.09s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  4.46s/it]
Using custom data configuration default-8dae597f644afe9d
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-8dae597f644afe9d/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2664.74it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 310.51it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-8dae597f644afe9d/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 282.71it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f79e4192280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 23,199,744 || all params: 6,761,615,360 || trainable%: 0.34310949033338684
0ex [00:00, ?ex/s]1ex [00:00,  2.94ex/s]20ex [00:00, 55.04ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_005704-gttgjol8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run llama2-7b-hf-chat_ZsRE_20
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/gttgjol8

Example:
<s> The appearance of Olivia Johnson is seen in what work? Oz</s>


Example:
<s> In which fictional work is Steven Hyde a character? Emmerdale</s>


Example:
<s> Who is Eteocles's father? Danehill</s>


Example:
<s> What is the constellation that is made with NGC 4293? Virgo</s>


Example:
<s> What city did Abel Seyler live when he died? Tirana</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 20
})
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:07<01:07,  7.54s/it]                                               10%|‚ñà         | 1/10 [00:07<01:07,  7.54s/it] 20%|‚ñà‚ñà        | 2/10 [00:14<00:55,  6.95s/it]                                               20%|‚ñà‚ñà        | 2/10 [00:14<00:55,  6.95s/it] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:20<00:47,  6.76s/it]                                               30%|‚ñà‚ñà‚ñà       | 3/10 [00:20<00:47,  6.76s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:27<00:39,  6.66s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:27<00:39,  6.66s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:33<00:33,  6.61s/it]                                               50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:33<00:33,  6.61s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:40<00:26,  6.58s/it]                                               60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:40<00:26,  6.58s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [00:46<00:19,  6.57s/it]                                               70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [00:46<00:19,  6.57s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [00:53<00:13,  6.55s/it]                                               80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [00:53<00:13,  6.55s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [00:59<00:06,  6.54s/it]                                               90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [00:59<00:06,  6.54s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:06<00:00,  6.54s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:06<00:00,  6.54s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:08<00:00,  6.54s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:08<00:00,  6.86s/it]
{'loss': 6.4628, 'grad_norm': 4.198410511016846, 'learning_rate': 4.000000000000001e-06, 'epoch': 1.0}
{'loss': 6.4628, 'grad_norm': 4.501792907714844, 'learning_rate': 8.000000000000001e-06, 'epoch': 2.0}
{'loss': 6.4368, 'grad_norm': 4.444761276245117, 'learning_rate': 1.2e-05, 'epoch': 3.0}
{'loss': 6.385, 'grad_norm': 4.351916790008545, 'learning_rate': 1.6000000000000003e-05, 'epoch': 4.0}
{'loss': 6.3078, 'grad_norm': 4.160963535308838, 'learning_rate': 2e-05, 'epoch': 5.0}
{'loss': 6.1992, 'grad_norm': 4.570759296417236, 'learning_rate': 2.4e-05, 'epoch': 6.0}
{'loss': 6.0637, 'grad_norm': 4.6306586265563965, 'learning_rate': 2.8000000000000003e-05, 'epoch': 7.0}
{'loss': 5.8976, 'grad_norm': 4.804744243621826, 'learning_rate': 3.2000000000000005e-05, 'epoch': 8.0}
{'loss': 5.7004, 'grad_norm': 4.897433757781982, 'learning_rate': 3.6e-05, 'epoch': 9.0}
{'loss': 5.4624, 'grad_norm': 5.343273639678955, 'learning_rate': 4e-05, 'epoch': 10.0}
{'train_runtime': 71.5364, 'train_samples_per_second': 2.796, 'train_steps_per_second': 0.14, 'train_loss': 6.137831163406372, 'epoch': 10.0}
[1;34mwandb[0m: üöÄ View run [33mllama2-7b-hf-chat_ZsRE_20[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/gttgjol8[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_005704-gttgjol8/logs[0m
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:09<00:09,  9.76s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  5.96s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  6.53s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_1/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_1/eval/adv_train/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_1/eval/adv_train/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_1/eval/adv_train/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_1/eval/adv_train/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_1/eval/adv_train/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_1/eval/adv_train/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_1/eval/adv_train/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_1/eval/adv_train/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_1/eval/adv_train/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_1/eval/adv_train/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_1/eval/adv_train/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_1/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_1/eval/GCG/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_1/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_1/eval/mix_eval_freeform_0811/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_1/eval/mix_eval_freeform_0811/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_1/eval/mix_eval_freeform_0811/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_1/eval/mix_eval_freeform_0811/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_1/eval/mix_eval_freeform_0811/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_1/eval/mix_eval_freeform_0811/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_1/eval/mix_eval_freeform_0811/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_1/eval/mix_eval_freeform_0811/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_1/eval/mix_eval_freeform_0811/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_1/eval/mix_eval_freeform_0811/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_1/eval/mix_eval_freeform_0811/results.json
550  to  600
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_1/eval/mix_eval_freeform_0811/results.json
600  to  650
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_1/eval/mix_eval_freeform_0811/results.json
650  to  700
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_1/eval/mix_eval_freeform_0811/results.json
700  to  750
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_1/eval/mix_eval_freeform_0811/results.json
750  to  800
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_1/eval/mix_eval_freeform_0811/results.json
800  to  850
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_1/eval/mix_eval_freeform_0811/results.json
850  to  900
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_1/eval/mix_eval_freeform_0811/results.json
900  to  950
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_1/eval/mix_eval_freeform_0811/results.json
950  to  1000
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_1/eval/mix_eval_freeform_0811/results.json
1000  to  1050
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_1/eval/mix_eval_freeform_0811/results.json
1050  to  1100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_1/eval/mix_eval_freeform_0811/results.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_1/adapter_model.safetensors
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_1/special_tokens_map.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_1/tokenizer_config.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_1/README.md
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_1/tmp_data.jsonl
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_1/adapter_config.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_1/tokenizer.model
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_1/wandb
Skipping file: ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_1/log.txt
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_1/checkpoint-10
Skipping directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_1/eval
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=1, data_source=ZsRE, data_size=25, random=False
Loading data from ../../data/edit_data/merged_data_part_1.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 25 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_1/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/llama2-7b-hf-chat
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_1
batch_size: 25
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: llama2-7b-hf-chat_ZsRE_25
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:06<00:06,  6.89s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.21s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.61s/it]
Using custom data configuration default-1114b233db464b2d
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-1114b233db464b2d/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2534.32it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 312.08it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-1114b233db464b2d/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 292.96it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f88401aa280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 23,199,744 || all params: 6,761,615,360 || trainable%: 0.34310949033338684
0ex [00:00, ?ex/s]1ex [00:00,  7.77ex/s]25ex [00:00, 158.39ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_010431-d4euncz2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run llama2-7b-hf-chat_ZsRE_25
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/d4euncz2

Example:
<s> The appearance of Olivia Johnson is seen in what work? Oz</s>


Example:
<s> What sports team was Veljko Simiƒá a member of? FK Senica</s>


Example:
<s> What architect designed Verdala Palace? Manfred Trenz</s>


Example:
<s> Who was William Boleyn's father? Henry Boleyn</s>


Example:
<s> In which fictional work is Steven Hyde a character? Emmerdale</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 25
})
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:09<01:22,  9.15s/it]                                               10%|‚ñà         | 1/10 [00:09<01:22,  9.15s/it] 20%|‚ñà‚ñà        | 2/10 [00:17<01:08,  8.55s/it]                                               20%|‚ñà‚ñà        | 2/10 [00:17<01:08,  8.55s/it] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:25<00:58,  8.36s/it]                                               30%|‚ñà‚ñà‚ñà       | 3/10 [00:25<00:58,  8.36s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:33<00:49,  8.27s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:33<00:49,  8.27s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:41<00:41,  8.22s/it]                                               50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:41<00:41,  8.22s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:49<00:32,  8.19s/it]                                               60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:49<00:32,  8.19s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [00:57<00:24,  8.17s/it]                                               70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [00:57<00:24,  8.17s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:06<00:16,  8.16s/it]                                               80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:06<00:16,  8.16s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:14<00:08,  8.15s/it]                                               90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:14<00:08,  8.15s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:22<00:00,  8.15s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:22<00:00,  8.15s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:25<00:00,  8.15s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:25<00:00,  8.59s/it]
{'loss': 6.2707, 'grad_norm': 4.178567409515381, 'learning_rate': 4.000000000000001e-06, 'epoch': 1.0}
{'loss': 6.2707, 'grad_norm': 4.355807304382324, 'learning_rate': 8.000000000000001e-06, 'epoch': 2.0}
{'loss': 6.247, 'grad_norm': 4.249910354614258, 'learning_rate': 1.2e-05, 'epoch': 3.0}
{'loss': 6.1999, 'grad_norm': 4.121555328369141, 'learning_rate': 1.6000000000000003e-05, 'epoch': 4.0}
{'loss': 6.127, 'grad_norm': 4.3200860023498535, 'learning_rate': 2e-05, 'epoch': 5.0}
{'loss': 6.0278, 'grad_norm': 4.4324846267700195, 'learning_rate': 2.4e-05, 'epoch': 6.0}
{'loss': 5.9065, 'grad_norm': 4.209754943847656, 'learning_rate': 2.8000000000000003e-05, 'epoch': 7.0}
{'loss': 5.7481, 'grad_norm': 4.636124134063721, 'learning_rate': 3.2000000000000005e-05, 'epoch': 8.0}
{'loss': 5.5552, 'grad_norm': 5.1584625244140625, 'learning_rate': 3.6e-05, 'epoch': 9.0}
{'loss': 5.3318, 'grad_norm': 5.502707004547119, 'learning_rate': 4e-05, 'epoch': 10.0}
{'train_runtime': 88.7564, 'train_samples_per_second': 2.817, 'train_steps_per_second': 0.113, 'train_loss': 5.968474245071411, 'epoch': 10.0}
[1;34mwandb[0m: üöÄ View run [33mllama2-7b-hf-chat_ZsRE_25[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/d4euncz2[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_010431-d4euncz2/logs[0m
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:09<00:09,  9.62s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:12<00:00,  5.92s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:12<00:00,  6.48s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_1/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_1/eval/adv_train/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_1/eval/adv_train/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_1/eval/adv_train/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_1/eval/adv_train/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_1/eval/adv_train/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_1/eval/adv_train/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_1/eval/adv_train/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_1/eval/adv_train/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_1/eval/adv_train/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_1/eval/adv_train/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_1/eval/adv_train/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_1/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_1/eval/GCG/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_1/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_1/eval/mix_eval_freeform_0811/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_1/eval/mix_eval_freeform_0811/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_1/eval/mix_eval_freeform_0811/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_1/eval/mix_eval_freeform_0811/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_1/eval/mix_eval_freeform_0811/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_1/eval/mix_eval_freeform_0811/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_1/eval/mix_eval_freeform_0811/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_1/eval/mix_eval_freeform_0811/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_1/eval/mix_eval_freeform_0811/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_1/eval/mix_eval_freeform_0811/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_1/eval/mix_eval_freeform_0811/results.json
550  to  600
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_1/eval/mix_eval_freeform_0811/results.json
600  to  650
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_1/eval/mix_eval_freeform_0811/results.json
650  to  700
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_1/eval/mix_eval_freeform_0811/results.json
700  to  750
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_1/eval/mix_eval_freeform_0811/results.json
750  to  800
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_1/eval/mix_eval_freeform_0811/results.json
800  to  850
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_1/eval/mix_eval_freeform_0811/results.json
850  to  900
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_1/eval/mix_eval_freeform_0811/results.json
900  to  950
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_1/eval/mix_eval_freeform_0811/results.json
950  to  1000
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_1/eval/mix_eval_freeform_0811/results.json
1000  to  1050
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_1/eval/mix_eval_freeform_0811/results.json
1050  to  1100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_1/eval/mix_eval_freeform_0811/results.json
Skipping directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_1/eval
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_1/tokenizer.model
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_1/tokenizer_config.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_1/README.md
Skipping file: ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_1/log.txt
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_1/adapter_config.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_1/tmp_data.jsonl
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_1/special_tokens_map.json
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_1/wandb
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_1/checkpoint-10
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_1/adapter_model.safetensors
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=1, data_source=ZsRE, data_size=30, random=False
Loading data from ../../data/edit_data/merged_data_part_1.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 30 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_1/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/llama2-7b-hf-chat
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_1
batch_size: 30
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: llama2-7b-hf-chat_ZsRE_30
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:07<00:07,  7.29s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.38s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.82s/it]
Using custom data configuration default-2ad3e7a191137706
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-2ad3e7a191137706/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2757.60it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 299.94it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-2ad3e7a191137706/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 258.94it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7fadb1c96280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 23,199,744 || all params: 6,761,615,360 || trainable%: 0.34310949033338684
0ex [00:00, ?ex/s]3ex [00:00, 29.67ex/s]30ex [00:00, 225.33ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_011214-o4r0ogkt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run llama2-7b-hf-chat_ZsRE_30
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/o4r0ogkt

Example:
<s> When was the discovery of 503 Evelyn? 17 503</s>


Example:
<s> What sports team was Veljko Simiƒá a member of? FK Senica</s>


Example:
<s> What is the constellation that is made with NGC 4293? Virgo</s>


Example:
<s> When did Joe Van Holsbeeck occur? 1954</s>


Example:
<s> What label was responsible for Into the Glorious? Columbia Records</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 30
})
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:10<01:37, 10.81s/it]                                               10%|‚ñà         | 1/10 [00:10<01:37, 10.81s/it] 20%|‚ñà‚ñà        | 2/10 [00:20<01:21, 10.21s/it]                                               20%|‚ñà‚ñà        | 2/10 [00:20<01:21, 10.21s/it] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:30<01:10, 10.02s/it]                                               30%|‚ñà‚ñà‚ñà       | 3/10 [00:30<01:10, 10.02s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:40<00:59,  9.93s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:40<00:59,  9.93s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:49<00:49,  9.88s/it]                                               50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:49<00:49,  9.88s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:59<00:39,  9.85s/it]                                               60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:59<00:39,  9.85s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:09<00:29,  9.83s/it]                                               70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:09<00:29,  9.83s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:19<00:19,  9.82s/it]                                               80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:19<00:19,  9.82s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:29<00:09,  9.81s/it]                                               90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:29<00:09,  9.81s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:38<00:00,  9.81s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:38<00:00,  9.81s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:43<00:00,  9.81s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:43<00:00, 10.38s/it]
{'loss': 6.0646, 'grad_norm': 3.994621515274048, 'learning_rate': 4.000000000000001e-06, 'epoch': 1.0}
{'loss': 6.0646, 'grad_norm': 4.126139163970947, 'learning_rate': 8.000000000000001e-06, 'epoch': 2.0}
{'loss': 6.043, 'grad_norm': 3.970553159713745, 'learning_rate': 1.2e-05, 'epoch': 3.0}
{'loss': 5.9996, 'grad_norm': 3.9313161373138428, 'learning_rate': 1.6000000000000003e-05, 'epoch': 4.0}
{'loss': 5.9327, 'grad_norm': 4.078505039215088, 'learning_rate': 2e-05, 'epoch': 5.0}
{'loss': 5.8439, 'grad_norm': 3.971327304840088, 'learning_rate': 2.4e-05, 'epoch': 6.0}
{'loss': 5.7254, 'grad_norm': 4.326417446136475, 'learning_rate': 2.8000000000000003e-05, 'epoch': 7.0}
{'loss': 5.5792, 'grad_norm': 4.630914211273193, 'learning_rate': 3.2000000000000005e-05, 'epoch': 8.0}
{'loss': 5.4064, 'grad_norm': 4.78266716003418, 'learning_rate': 3.6e-05, 'epoch': 9.0}
{'loss': 5.1954, 'grad_norm': 5.202589988708496, 'learning_rate': 4e-05, 'epoch': 10.0}
{'train_runtime': 106.2573, 'train_samples_per_second': 2.823, 'train_steps_per_second': 0.094, 'train_loss': 5.785472822189331, 'epoch': 10.0}
[1;34mwandb[0m: üöÄ View run [33mllama2-7b-hf-chat_ZsRE_30[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/o4r0ogkt[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_011214-o4r0ogkt/logs[0m
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:09<00:09,  9.71s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  5.94s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  6.51s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_1/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_1/eval/adv_train/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_1/eval/adv_train/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_1/eval/adv_train/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_1/eval/adv_train/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_1/eval/adv_train/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_1/eval/adv_train/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_1/eval/adv_train/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_1/eval/adv_train/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_1/eval/adv_train/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_1/eval/adv_train/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_1/eval/adv_train/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_1/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_1/eval/GCG/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_1/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_1/eval/mix_eval_freeform_0811/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_1/eval/mix_eval_freeform_0811/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_1/eval/mix_eval_freeform_0811/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_1/eval/mix_eval_freeform_0811/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_1/eval/mix_eval_freeform_0811/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_1/eval/mix_eval_freeform_0811/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_1/eval/mix_eval_freeform_0811/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_1/eval/mix_eval_freeform_0811/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_1/eval/mix_eval_freeform_0811/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_1/eval/mix_eval_freeform_0811/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_1/eval/mix_eval_freeform_0811/results.json
550  to  600
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_1/eval/mix_eval_freeform_0811/results.json
600  to  650
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_1/eval/mix_eval_freeform_0811/results.json
650  to  700
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_1/eval/mix_eval_freeform_0811/results.json
700  to  750
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_1/eval/mix_eval_freeform_0811/results.json
750  to  800
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_1/eval/mix_eval_freeform_0811/results.json
800  to  850
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_1/eval/mix_eval_freeform_0811/results.json
850  to  900
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_1/eval/mix_eval_freeform_0811/results.json
900  to  950
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_1/eval/mix_eval_freeform_0811/results.json
950  to  1000
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_1/eval/mix_eval_freeform_0811/results.json
1000  to  1050
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_1/eval/mix_eval_freeform_0811/results.json
1050  to  1100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_1/eval/mix_eval_freeform_0811/results.json
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_1/wandb
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_1/tokenizer.model
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_1/adapter_model.safetensors
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_1/README.md
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_1/tokenizer_config.json
Skipping directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_1/eval
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_1/tmp_data.jsonl
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_1/checkpoint-10
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_1/adapter_config.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_1/special_tokens_map.json
Skipping file: ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_1/log.txt
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=1, data_source=ZsRE, data_size=35, random=False
Loading data from ../../data/edit_data/merged_data_part_1.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 35 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_1/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/llama2-7b-hf-chat
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_1
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: llama2-7b-hf-chat_ZsRE_35
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:07<00:07,  7.55s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.48s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.94s/it]
Using custom data configuration default-8660a5c1cbd74963
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-8660a5c1cbd74963/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2978.91it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 251.82it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-8660a5c1cbd74963/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 228.83it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7fc0e8f94280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 23,199,744 || all params: 6,761,615,360 || trainable%: 0.34310949033338684
0ex [00:00, ?ex/s]8ex [00:00, 79.82ex/s]35ex [00:00, 263.83ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_012028-xh9idcvh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run llama2-7b-hf-chat_ZsRE_35
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/xh9idcvh

Example:
<s> When did Joe Van Holsbeeck occur? 1954</s>


Example:
<s> What sports team was Veljko Simiƒá a member of? FK Senica</s>


Example:
<s> What label was responsible for Into the Glorious? Columbia Records</s>


Example:
<s> What country released Kink FM? South Africa</s>


Example:
<s> When did the discovery or creation of Rutherfordium occur? 1 January Rutherford</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 35
})
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:11<01:43, 11.47s/it]                                               10%|‚ñà         | 1/10 [00:11<01:43, 11.47s/it] 20%|‚ñà‚ñà        | 2/10 [00:21<01:26, 10.87s/it]                                               20%|‚ñà‚ñà        | 2/10 [00:21<01:26, 10.87s/it] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:33<01:17, 11.11s/it]                                               30%|‚ñà‚ñà‚ñà       | 3/10 [00:33<01:17, 11.11s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:43<01:05, 10.88s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:43<01:05, 10.88s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:54<00:53, 10.73s/it]                                               50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:54<00:53, 10.73s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:04<00:42, 10.63s/it]                                               60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:04<00:42, 10.63s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:15<00:31, 10.58s/it]                                               70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:15<00:31, 10.58s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:25<00:21, 10.59s/it]                                               80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:25<00:21, 10.59s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:36<00:10, 10.56s/it]                                               90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:36<00:10, 10.56s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:46<00:00, 10.52s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:46<00:00, 10.52s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:52<00:00, 10.52s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:52<00:00, 11.21s/it]
{'loss': 5.8812, 'grad_norm': 3.727627992630005, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.91}
{'loss': 6.1907, 'grad_norm': 4.230971336364746, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.83}
{'loss': 5.7355, 'grad_norm': 3.864140510559082, 'learning_rate': 1.2e-05, 'epoch': 2.74}
{'loss': 5.655, 'grad_norm': 3.8097684383392334, 'learning_rate': 1.6000000000000003e-05, 'epoch': 3.66}
{'loss': 5.8051, 'grad_norm': 3.9384243488311768, 'learning_rate': 2e-05, 'epoch': 4.57}
{'loss': 5.5161, 'grad_norm': 4.042268753051758, 'learning_rate': 2.4e-05, 'epoch': 5.49}
{'loss': 5.956, 'grad_norm': 4.5064778327941895, 'learning_rate': 2.8000000000000003e-05, 'epoch': 6.4}
{'loss': 5.5957, 'grad_norm': 4.591724395751953, 'learning_rate': 3.2000000000000005e-05, 'epoch': 7.31}
{'loss': 4.922, 'grad_norm': 4.3698225021362305, 'learning_rate': 3.6e-05, 'epoch': 8.23}
{'loss': 5.3539, 'grad_norm': 5.220706939697266, 'learning_rate': 4e-05, 'epoch': 9.14}
{'train_runtime': 132.6183, 'train_samples_per_second': 2.639, 'train_steps_per_second': 0.075, 'train_loss': 5.6611250877380375, 'epoch': 9.14}
[1;34mwandb[0m: üöÄ View run [33mllama2-7b-hf-chat_ZsRE_35[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/xh9idcvh[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_012028-xh9idcvh/logs[0m
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:09<00:09,  9.76s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  5.97s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  6.54s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_1/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_1/eval/adv_train/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_1/eval/adv_train/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_1/eval/adv_train/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_1/eval/adv_train/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_1/eval/adv_train/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_1/eval/adv_train/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_1/eval/adv_train/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_1/eval/adv_train/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_1/eval/adv_train/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_1/eval/adv_train/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_1/eval/adv_train/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_1/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_1/eval/GCG/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_1/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_1/eval/mix_eval_freeform_0811/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_1/eval/mix_eval_freeform_0811/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_1/eval/mix_eval_freeform_0811/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_1/eval/mix_eval_freeform_0811/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_1/eval/mix_eval_freeform_0811/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_1/eval/mix_eval_freeform_0811/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_1/eval/mix_eval_freeform_0811/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_1/eval/mix_eval_freeform_0811/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_1/eval/mix_eval_freeform_0811/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_1/eval/mix_eval_freeform_0811/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_1/eval/mix_eval_freeform_0811/results.json
550  to  600
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_1/eval/mix_eval_freeform_0811/results.json
600  to  650
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_1/eval/mix_eval_freeform_0811/results.json
650  to  700
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_1/eval/mix_eval_freeform_0811/results.json
700  to  750
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_1/eval/mix_eval_freeform_0811/results.json
750  to  800
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_1/eval/mix_eval_freeform_0811/results.json
800  to  850
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_1/eval/mix_eval_freeform_0811/results.json
850  to  900
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_1/eval/mix_eval_freeform_0811/results.json
900  to  950
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_1/eval/mix_eval_freeform_0811/results.json
950  to  1000
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_1/eval/mix_eval_freeform_0811/results.json
1000  to  1050
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_1/eval/mix_eval_freeform_0811/results.json
1050  to  1100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_1/eval/mix_eval_freeform_0811/results.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_1/adapter_config.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_1/tokenizer_config.json
Skipping directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_1/eval
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_1/adapter_model.safetensors
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_1/wandb
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_1/tmp_data.jsonl
Skipping file: ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_1/log.txt
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_1/README.md
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_1/tokenizer.model
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_1/special_tokens_map.json
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_1/checkpoint-10
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=1, data_source=ZsRE, data_size=40, random=False
Loading data from ../../data/edit_data/merged_data_part_1.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 40 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_1/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/llama2-7b-hf-chat
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_1
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: llama2-7b-hf-chat_ZsRE_40
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:07<00:07,  7.19s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.34s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.77s/it]
Using custom data configuration default-46ca75b0081be737
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-46ca75b0081be737/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2700.78it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 312.91it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-46ca75b0081be737/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 279.23it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7fc571146280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 23,199,744 || all params: 6,761,615,360 || trainable%: 0.34310949033338684
0ex [00:00, ?ex/s]1ex [00:00,  6.78ex/s]40ex [00:00, 205.16ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_012906-ha4vwakp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run llama2-7b-hf-chat_ZsRE_40
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/ha4vwakp

Example:
<s> Which was the official year for the approval of JS 7.62? 1966</s>


Example:
<s> What is the constellation that is made with NGC 4293? Virgo</s>


Example:
<s> Who fathered Bo Guagua? Xuan Xun</s>


Example:
<s> What label was responsible for Silence Is Easy? Elektra Records</s>


Example:
<s> Which family does Dendrolobatus belong to? Carabidae</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 40
})
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:11<01:42, 11.43s/it]                                               10%|‚ñà         | 1/10 [00:11<01:42, 11.43s/it] 20%|‚ñà‚ñà        | 2/10 [00:21<01:26, 10.86s/it]                                               20%|‚ñà‚ñà        | 2/10 [00:21<01:26, 10.86s/it] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:32<01:14, 10.65s/it]                                               30%|‚ñà‚ñà‚ñà       | 3/10 [00:32<01:14, 10.65s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:42<01:03, 10.57s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:42<01:03, 10.57s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:53<00:52, 10.51s/it]                                               50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:53<00:52, 10.51s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:03<00:41, 10.47s/it]                                               60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:03<00:41, 10.47s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:13<00:31, 10.47s/it]                                               70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:13<00:31, 10.47s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:24<00:20, 10.45s/it]                                               80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:24<00:20, 10.45s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:34<00:10, 10.44s/it]                                               90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:34<00:10, 10.44s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:45<00:00, 10.43s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:45<00:00, 10.43s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:47<00:00, 10.43s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:47<00:00, 10.79s/it]
{'loss': 5.8811, 'grad_norm': 4.027531147003174, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.8}
{'loss': 5.6983, 'grad_norm': 4.1013407707214355, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.6}
{'loss': 5.9809, 'grad_norm': 3.9684271812438965, 'learning_rate': 1.2e-05, 'epoch': 2.4}
{'loss': 5.9606, 'grad_norm': 4.028419017791748, 'learning_rate': 1.6000000000000003e-05, 'epoch': 3.2}
{'loss': 6.0757, 'grad_norm': 4.059024810791016, 'learning_rate': 2e-05, 'epoch': 4.0}
{'loss': 5.846, 'grad_norm': 4.026491165161133, 'learning_rate': 2.4e-05, 'epoch': 4.8}
{'loss': 5.5263, 'grad_norm': 4.304862976074219, 'learning_rate': 2.8000000000000003e-05, 'epoch': 5.6}
{'loss': 5.6209, 'grad_norm': 4.041720390319824, 'learning_rate': 3.2000000000000005e-05, 'epoch': 6.4}
{'loss': 5.2746, 'grad_norm': 4.867783546447754, 'learning_rate': 3.6e-05, 'epoch': 7.2}
{'loss': 5.4055, 'grad_norm': 4.97500467300415, 'learning_rate': 4e-05, 'epoch': 8.0}
{'train_runtime': 110.9793, 'train_samples_per_second': 3.604, 'train_steps_per_second': 0.09, 'train_loss': 5.726994419097901, 'epoch': 8.0}
[1;34mwandb[0m: üöÄ View run [33mllama2-7b-hf-chat_ZsRE_40[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/ha4vwakp[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_012906-ha4vwakp/logs[0m
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:09<00:09,  9.77s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  5.96s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  6.54s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_1/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_1/eval/adv_train/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_1/eval/adv_train/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_1/eval/adv_train/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_1/eval/adv_train/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_1/eval/adv_train/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_1/eval/adv_train/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_1/eval/adv_train/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_1/eval/adv_train/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_1/eval/adv_train/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_1/eval/adv_train/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_1/eval/adv_train/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_1/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_1/eval/GCG/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_1/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_1/eval/mix_eval_freeform_0811/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_1/eval/mix_eval_freeform_0811/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_1/eval/mix_eval_freeform_0811/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_1/eval/mix_eval_freeform_0811/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_1/eval/mix_eval_freeform_0811/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_1/eval/mix_eval_freeform_0811/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_1/eval/mix_eval_freeform_0811/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_1/eval/mix_eval_freeform_0811/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_1/eval/mix_eval_freeform_0811/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_1/eval/mix_eval_freeform_0811/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_1/eval/mix_eval_freeform_0811/results.json
550  to  600
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_1/eval/mix_eval_freeform_0811/results.json
600  to  650
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_1/eval/mix_eval_freeform_0811/results.json
650  to  700
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_1/eval/mix_eval_freeform_0811/results.json
700  to  750
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_1/eval/mix_eval_freeform_0811/results.json
750  to  800
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_1/eval/mix_eval_freeform_0811/results.json
800  to  850
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_1/eval/mix_eval_freeform_0811/results.json
850  to  900
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_1/eval/mix_eval_freeform_0811/results.json
900  to  950
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_1/eval/mix_eval_freeform_0811/results.json
950  to  1000
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_1/eval/mix_eval_freeform_0811/results.json
1000  to  1050
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_1/eval/mix_eval_freeform_0811/results.json
1050  to  1100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_1/eval/mix_eval_freeform_0811/results.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_1/tokenizer_config.json
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_1/wandb
Skipping directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_1/eval
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_1/tmp_data.jsonl
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_1/tokenizer.model
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_1/checkpoint-10
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_1/adapter_model.safetensors
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_1/README.md
Skipping file: ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_1/log.txt
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_1/special_tokens_map.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_1/adapter_config.json
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=1, data_source=ZsRE, data_size=45, random=False
Loading data from ../../data/edit_data/merged_data_part_1.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 45 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_1/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/llama2-7b-hf-chat
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_1
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: llama2-7b-hf-chat_ZsRE_45
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:07<00:07,  7.48s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.46s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.91s/it]
Using custom data configuration default-bd0630ac35140008
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-bd0630ac35140008/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2728.89it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 330.13it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-bd0630ac35140008/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 284.88it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7fc15c0b4280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 23,199,744 || all params: 6,761,615,360 || trainable%: 0.34310949033338684
0ex [00:00, ?ex/s]3ex [00:00, 29.85ex/s]45ex [00:00, 303.07ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_013729-g10vhfxe
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run llama2-7b-hf-chat_ZsRE_45
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/g10vhfxe

Example:
<s> When was the discovery of 503 Evelyn? 17 503</s>


Example:
<s> What is Gaston de Gerlache's father's name? Charles de Gerlache, 2nd Earl of Leicester</s>


Example:
<s> The Strengleikar is based upon what? Erlangenbau</s>


Example:
<s> Which lady gave birth to Leto? Fausta</s>


Example:
<s> What is the position of Andrea Pangrazio? Doge of Venice</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 45
})
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:11<01:42, 11.43s/it]                                               10%|‚ñà         | 1/10 [00:11<01:42, 11.43s/it] 20%|‚ñà‚ñà        | 2/10 [00:21<01:26, 10.84s/it]                                               20%|‚ñà‚ñà        | 2/10 [00:21<01:26, 10.84s/it] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:32<01:14, 10.65s/it]                                               30%|‚ñà‚ñà‚ñà       | 3/10 [00:32<01:14, 10.65s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:42<01:03, 10.55s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:42<01:03, 10.55s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:53<00:52, 10.51s/it]                                               50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:53<00:52, 10.51s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:03<00:41, 10.49s/it]                                               60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:03<00:41, 10.49s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:13<00:31, 10.46s/it]                                               70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:13<00:31, 10.46s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:24<00:20, 10.45s/it]                                               80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:24<00:20, 10.45s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:34<00:10, 10.44s/it]                                               90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:34<00:10, 10.44s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:45<00:00, 10.44s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:45<00:00, 10.44s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:47<00:00, 10.44s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:47<00:00, 10.76s/it]
{'loss': 6.1225, 'grad_norm': 3.769862174987793, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.71}
{'loss': 5.7931, 'grad_norm': 4.334162712097168, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.42}
{'loss': 5.8434, 'grad_norm': 3.7577590942382812, 'learning_rate': 1.2e-05, 'epoch': 2.13}
{'loss': 5.8331, 'grad_norm': 3.926367998123169, 'learning_rate': 1.6000000000000003e-05, 'epoch': 2.84}
{'loss': 5.4983, 'grad_norm': 3.8015995025634766, 'learning_rate': 2e-05, 'epoch': 3.56}
{'loss': 5.765, 'grad_norm': 4.177947044372559, 'learning_rate': 2.4e-05, 'epoch': 4.27}
{'loss': 5.7619, 'grad_norm': 4.1343278884887695, 'learning_rate': 2.8000000000000003e-05, 'epoch': 4.98}
{'loss': 5.6076, 'grad_norm': 4.377354145050049, 'learning_rate': 3.2000000000000005e-05, 'epoch': 5.69}
{'loss': 5.3372, 'grad_norm': 4.960418701171875, 'learning_rate': 3.6e-05, 'epoch': 6.4}
{'loss': 5.1297, 'grad_norm': 4.680873870849609, 'learning_rate': 4e-05, 'epoch': 7.11}
{'train_runtime': 109.7536, 'train_samples_per_second': 4.1, 'train_steps_per_second': 0.091, 'train_loss': 5.6691656589508055, 'epoch': 7.11}
[1;34mwandb[0m: üöÄ View run [33mllama2-7b-hf-chat_ZsRE_45[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/g10vhfxe[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_013729-g10vhfxe/logs[0m
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:09<00:09,  9.79s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  5.97s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  6.54s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_1/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_1/eval/adv_train/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_1/eval/adv_train/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_1/eval/adv_train/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_1/eval/adv_train/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_1/eval/adv_train/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_1/eval/adv_train/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_1/eval/adv_train/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_1/eval/adv_train/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_1/eval/adv_train/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_1/eval/adv_train/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_1/eval/adv_train/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_1/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_1/eval/GCG/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_1/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_1/eval/mix_eval_freeform_0811/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_1/eval/mix_eval_freeform_0811/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_1/eval/mix_eval_freeform_0811/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_1/eval/mix_eval_freeform_0811/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_1/eval/mix_eval_freeform_0811/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_1/eval/mix_eval_freeform_0811/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_1/eval/mix_eval_freeform_0811/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_1/eval/mix_eval_freeform_0811/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_1/eval/mix_eval_freeform_0811/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_1/eval/mix_eval_freeform_0811/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_1/eval/mix_eval_freeform_0811/results.json
550  to  600
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_1/eval/mix_eval_freeform_0811/results.json
600  to  650
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_1/eval/mix_eval_freeform_0811/results.json
650  to  700
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_1/eval/mix_eval_freeform_0811/results.json
700  to  750
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_1/eval/mix_eval_freeform_0811/results.json
750  to  800
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_1/eval/mix_eval_freeform_0811/results.json
800  to  850
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_1/eval/mix_eval_freeform_0811/results.json
850  to  900
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_1/eval/mix_eval_freeform_0811/results.json
900  to  950
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_1/eval/mix_eval_freeform_0811/results.json
950  to  1000
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_1/eval/mix_eval_freeform_0811/results.json
1000  to  1050
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_1/eval/mix_eval_freeform_0811/results.json
1050  to  1100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_1/eval/mix_eval_freeform_0811/results.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_1/special_tokens_map.json
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_1/wandb
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_1/adapter_model.safetensors
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_1/README.md
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_1/adapter_config.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_1/tmp_data.jsonl
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_1/tokenizer.model
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_1/tokenizer_config.json
Skipping directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_1/eval
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_1/checkpoint-10
Skipping file: ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_1/log.txt
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=1, data_source=ZsRE, data_size=50, random=False
Loading data from ../../data/edit_data/merged_data_part_1.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 50 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_1/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/llama2-7b-hf-chat
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_1
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: llama2-7b-hf-chat_ZsRE_50
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:07<00:07,  7.21s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.35s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.77s/it]
Using custom data configuration default-7bc5d1a3fba2eee1
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-7bc5d1a3fba2eee1/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2723.57it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 290.91it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-7bc5d1a3fba2eee1/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 265.14it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f7ac8552280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 23,199,744 || all params: 6,761,615,360 || trainable%: 0.34310949033338684
0ex [00:00, ?ex/s]41ex [00:00, 409.41ex/s]50ex [00:00, 461.94ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_014525-w5ktgkgt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run llama2-7b-hf-chat_ZsRE_50
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/w5ktgkgt

Example:
<s> What country released Kink FM? South Africa</s>


Example:
<s> When did Joe Van Holsbeeck occur? 1954</s>


Example:
<s> Who was William Boleyn's father? Henry Boleyn</s>


Example:
<s> Who was the mother of John Bowes, 10th Earl of Strathmore and Kinghorne? Elizabeth Bowes, 9th Earl of Strathmore and Kinghorne</s>


Example:
<s> Who fathered Bo Guagua? Xuan Xun</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 50
})
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:11<01:43, 11.55s/it]                                               10%|‚ñà         | 1/10 [00:11<01:43, 11.55s/it] 20%|‚ñà‚ñà        | 2/10 [00:21<01:27, 10.88s/it]                                               20%|‚ñà‚ñà        | 2/10 [00:21<01:27, 10.88s/it] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:32<01:14, 10.71s/it]                                               30%|‚ñà‚ñà‚ñà       | 3/10 [00:32<01:14, 10.71s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:42<01:03, 10.63s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:42<01:03, 10.63s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:53<00:52, 10.56s/it]                                               50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:53<00:52, 10.56s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:03<00:42, 10.53s/it]                                               60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:03<00:42, 10.53s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:14<00:31, 10.53s/it]                                               70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:14<00:31, 10.53s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:24<00:20, 10.50s/it]                                               80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:24<00:20, 10.50s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:35<00:10, 10.49s/it]                                               90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:35<00:10, 10.49s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:45<00:00, 10.48s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:45<00:00, 10.48s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:47<00:00, 10.48s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:47<00:00, 10.76s/it]
{'loss': 5.6336, 'grad_norm': 3.668752908706665, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.64}
{'loss': 6.4153, 'grad_norm': 4.534204959869385, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.28}
{'loss': 5.7578, 'grad_norm': 3.7332234382629395, 'learning_rate': 1.2e-05, 'epoch': 1.92}
{'loss': 5.4998, 'grad_norm': 3.7148985862731934, 'learning_rate': 1.6000000000000003e-05, 'epoch': 2.56}
{'loss': 6.0169, 'grad_norm': 4.221318244934082, 'learning_rate': 2e-05, 'epoch': 3.2}
{'loss': 5.7411, 'grad_norm': 3.896933078765869, 'learning_rate': 2.4e-05, 'epoch': 3.84}
{'loss': 5.4967, 'grad_norm': 4.159743785858154, 'learning_rate': 2.8000000000000003e-05, 'epoch': 4.48}
{'loss': 5.8579, 'grad_norm': 4.5747599601745605, 'learning_rate': 3.2000000000000005e-05, 'epoch': 5.12}
{'loss': 5.1699, 'grad_norm': 4.544702529907227, 'learning_rate': 3.6e-05, 'epoch': 5.76}
{'loss': 5.1546, 'grad_norm': 5.019099235534668, 'learning_rate': 4e-05, 'epoch': 6.4}
{'train_runtime': 109.6825, 'train_samples_per_second': 4.559, 'train_steps_per_second': 0.091, 'train_loss': 5.674370336532593, 'epoch': 6.4}
[1;34mwandb[0m: üöÄ View run [33mllama2-7b-hf-chat_ZsRE_50[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/w5ktgkgt[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_014525-w5ktgkgt/logs[0m
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:09<00:09,  9.72s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  5.95s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  6.52s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_1/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_1/eval/adv_train/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_1/eval/adv_train/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_1/eval/adv_train/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_1/eval/adv_train/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_1/eval/adv_train/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_1/eval/adv_train/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_1/eval/adv_train/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_1/eval/adv_train/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_1/eval/adv_train/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_1/eval/adv_train/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_1/eval/adv_train/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_1/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_1/eval/GCG/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_1/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_1/eval/mix_eval_freeform_0811/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_1/eval/mix_eval_freeform_0811/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_1/eval/mix_eval_freeform_0811/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_1/eval/mix_eval_freeform_0811/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_1/eval/mix_eval_freeform_0811/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_1/eval/mix_eval_freeform_0811/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_1/eval/mix_eval_freeform_0811/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_1/eval/mix_eval_freeform_0811/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_1/eval/mix_eval_freeform_0811/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_1/eval/mix_eval_freeform_0811/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_1/eval/mix_eval_freeform_0811/results.json
550  to  600
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_1/eval/mix_eval_freeform_0811/results.json
600  to  650
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_1/eval/mix_eval_freeform_0811/results.json
650  to  700
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_1/eval/mix_eval_freeform_0811/results.json
700  to  750
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_1/eval/mix_eval_freeform_0811/results.json
750  to  800
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_1/eval/mix_eval_freeform_0811/results.json
800  to  850
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_1/eval/mix_eval_freeform_0811/results.json
850  to  900
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_1/eval/mix_eval_freeform_0811/results.json
900  to  950
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_1/eval/mix_eval_freeform_0811/results.json
950  to  1000
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_1/eval/mix_eval_freeform_0811/results.json
1000  to  1050
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_1/eval/mix_eval_freeform_0811/results.json
1050  to  1100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_1/eval/mix_eval_freeform_0811/results.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_1/tokenizer_config.json
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_1/wandb
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_1/adapter_model.safetensors
Skipping file: ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_1/log.txt
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_1/checkpoint-10
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_1/adapter_config.json
Skipping directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_1/eval
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_1/tokenizer.model
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_1/tmp_data.jsonl
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_1/special_tokens_map.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_1/README.md
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=1, data_source=ZsRE, data_size=55, random=False
Loading data from ../../data/edit_data/merged_data_part_1.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 55 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_1/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/llama2-7b-hf-chat
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_1
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: llama2-7b-hf-chat_ZsRE_55
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:06<00:06,  6.59s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  4.08s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  4.46s/it]
Using custom data configuration default-c6bac2cdc1aafe10
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-c6bac2cdc1aafe10/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2642.91it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 283.90it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-c6bac2cdc1aafe10/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 525.47it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7fdb7c274280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 23,199,744 || all params: 6,761,615,360 || trainable%: 0.34310949033338684
0ex [00:00, ?ex/s]39ex [00:00, 388.17ex/s]55ex [00:00, 482.34ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_015319-wlirnjrd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run llama2-7b-hf-chat_ZsRE_55
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/wlirnjrd

Example:
<s> Who was the person who directed Bitter Apples? William Beaudine</s>


Example:
<s> Which series is Michael Scott Paper Company apart of? The Good Wife</s>


Example:
<s> What label was responsible for Silence Is Easy? Elektra Records</s>


Example:
<s> What label was responsible for Into the Glorious? Columbia Records</s>


Example:
<s> What is the constellation that is made with NGC 4293? Virgo</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 55
})
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:11<01:40, 11.16s/it]                                               10%|‚ñà         | 1/10 [00:11<01:40, 11.16s/it] 20%|‚ñà‚ñà        | 2/10 [00:21<01:25, 10.71s/it]                                               20%|‚ñà‚ñà        | 2/10 [00:21<01:25, 10.71s/it] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:31<01:13, 10.57s/it]                                               30%|‚ñà‚ñà‚ñà       | 3/10 [00:31<01:13, 10.57s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:42<01:03, 10.55s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:42<01:03, 10.55s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:52<00:52, 10.53s/it]                                               50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:52<00:52, 10.53s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:03<00:41, 10.49s/it]                                               60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:03<00:41, 10.49s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:13<00:31, 10.50s/it]                                               70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:13<00:31, 10.50s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:24<00:20, 10.48s/it]                                               80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:24<00:20, 10.48s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:34<00:10, 10.51s/it]                                               90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:34<00:10, 10.51s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:45<00:00, 10.48s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:45<00:00, 10.48s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:47<00:00, 10.48s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:47<00:00, 10.73s/it]
{'loss': 5.4959, 'grad_norm': 3.771568775177002, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.58}
{'loss': 6.4957, 'grad_norm': 4.477675437927246, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.16}
{'loss': 6.2151, 'grad_norm': 4.036856174468994, 'learning_rate': 1.2e-05, 'epoch': 1.75}
{'loss': 5.3284, 'grad_norm': 3.463348388671875, 'learning_rate': 1.6000000000000003e-05, 'epoch': 2.33}
{'loss': 6.1653, 'grad_norm': 4.215181827545166, 'learning_rate': 2e-05, 'epoch': 2.91}
{'loss': 6.0992, 'grad_norm': 4.428322792053223, 'learning_rate': 2.4e-05, 'epoch': 3.49}
{'loss': 5.7502, 'grad_norm': 4.203200340270996, 'learning_rate': 2.8000000000000003e-05, 'epoch': 4.07}
{'loss': 5.4975, 'grad_norm': 4.436769008636475, 'learning_rate': 3.2000000000000005e-05, 'epoch': 4.65}
{'loss': 6.0593, 'grad_norm': 5.106449127197266, 'learning_rate': 3.6e-05, 'epoch': 5.24}
{'loss': 5.1447, 'grad_norm': 4.6687188148498535, 'learning_rate': 4e-05, 'epoch': 5.82}
{'train_runtime': 109.2684, 'train_samples_per_second': 5.033, 'train_steps_per_second': 0.092, 'train_loss': 5.825131559371949, 'epoch': 5.82}
[1;34mwandb[0m: üöÄ View run [33mllama2-7b-hf-chat_ZsRE_55[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/wlirnjrd[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_015319-wlirnjrd/logs[0m
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:09<00:09,  9.70s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:12<00:00,  5.93s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:12<00:00,  6.50s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_1/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_1/eval/adv_train/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_1/eval/adv_train/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_1/eval/adv_train/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_1/eval/adv_train/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_1/eval/adv_train/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_1/eval/adv_train/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_1/eval/adv_train/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_1/eval/adv_train/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_1/eval/adv_train/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_1/eval/adv_train/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_1/eval/adv_train/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_1/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_1/eval/GCG/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_1/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_1/eval/mix_eval_freeform_0811/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_1/eval/mix_eval_freeform_0811/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_1/eval/mix_eval_freeform_0811/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_1/eval/mix_eval_freeform_0811/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_1/eval/mix_eval_freeform_0811/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_1/eval/mix_eval_freeform_0811/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_1/eval/mix_eval_freeform_0811/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_1/eval/mix_eval_freeform_0811/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_1/eval/mix_eval_freeform_0811/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_1/eval/mix_eval_freeform_0811/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_1/eval/mix_eval_freeform_0811/results.json
550  to  600
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_1/eval/mix_eval_freeform_0811/results.json
600  to  650
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_1/eval/mix_eval_freeform_0811/results.json
650  to  700
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_1/eval/mix_eval_freeform_0811/results.json
700  to  750
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_1/eval/mix_eval_freeform_0811/results.json
750  to  800
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_1/eval/mix_eval_freeform_0811/results.json
800  to  850
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_1/eval/mix_eval_freeform_0811/results.json
850  to  900
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_1/eval/mix_eval_freeform_0811/results.json
900  to  950
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_1/eval/mix_eval_freeform_0811/results.json
950  to  1000
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_1/eval/mix_eval_freeform_0811/results.json
1000  to  1050
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_1/eval/mix_eval_freeform_0811/results.json
1050  to  1100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_1/eval/mix_eval_freeform_0811/results.json
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_1/wandb
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_1/README.md
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_1/special_tokens_map.json
Skipping directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_1/eval
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_1/tokenizer.model
Skipping file: ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_1/log.txt
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_1/tokenizer_config.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_1/adapter_model.safetensors
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_1/adapter_config.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_1/tmp_data.jsonl
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_1/checkpoint-10
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=1, data_source=ZsRE, data_size=60, random=False
Loading data from ../../data/edit_data/merged_data_part_1.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 60 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_1/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/llama2-7b-hf-chat
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_1
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: llama2-7b-hf-chat_ZsRE_60
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:06<00:06,  6.58s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  4.08s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  4.46s/it]
Using custom data configuration default-d4115ba46fb9d8d1
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-d4115ba46fb9d8d1/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2642.91it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 311.15it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-d4115ba46fb9d8d1/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 270.01it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7fd614f15280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 23,199,744 || all params: 6,761,615,360 || trainable%: 0.34310949033338684
0ex [00:00, ?ex/s]37ex [00:00, 367.47ex/s]60ex [00:00, 499.27ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_020110-4d2ko0vf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run llama2-7b-hf-chat_ZsRE_60
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/4d2ko0vf

Example:
<s> Who was the person who directed Bitter Apples? William Beaudine</s>


Example:
<s> What is the publisher of Smelly Old History? Harper</s>


Example:
<s> What label was responsible for Silence Is Easy? Elektra Records</s>


Example:
<s> What label was responsible for Into the Glorious? Columbia Records</s>


Example:
<s> What is the constellation that is made with NGC 4293? Virgo</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 60
})
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:11<01:39, 11.06s/it]                                               10%|‚ñà         | 1/10 [00:11<01:39, 11.06s/it] 20%|‚ñà‚ñà        | 2/10 [00:21<01:26, 10.79s/it]                                               20%|‚ñà‚ñà        | 2/10 [00:21<01:26, 10.79s/it] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:32<01:14, 10.70s/it]                                               30%|‚ñà‚ñà‚ñà       | 3/10 [00:32<01:14, 10.70s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:42<01:03, 10.63s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:42<01:03, 10.63s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:53<00:52, 10.60s/it]                                               50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:53<00:52, 10.60s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:03<00:42, 10.57s/it]                                               60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:03<00:42, 10.57s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:14<00:31, 10.55s/it]                                               70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:14<00:31, 10.55s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:24<00:21, 10.52s/it]                                               80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:24<00:21, 10.52s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:35<00:10, 10.52s/it]                                               90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:35<00:10, 10.52s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:45<00:00, 10.54s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:45<00:00, 10.54s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:47<00:00, 10.54s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:47<00:00, 10.76s/it]
{'loss': 6.4248, 'grad_norm': 4.0287933349609375, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.53}
{'loss': 5.5316, 'grad_norm': 3.9869751930236816, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.07}
{'loss': 6.3096, 'grad_norm': 4.120787143707275, 'learning_rate': 1.2e-05, 'epoch': 1.6}
{'loss': 6.0738, 'grad_norm': 3.9424543380737305, 'learning_rate': 1.6000000000000003e-05, 'epoch': 2.13}
{'loss': 5.6487, 'grad_norm': 3.964867353439331, 'learning_rate': 2e-05, 'epoch': 2.67}
{'loss': 5.8717, 'grad_norm': 4.2582197189331055, 'learning_rate': 2.4e-05, 'epoch': 3.2}
{'loss': 6.0687, 'grad_norm': 4.498089790344238, 'learning_rate': 2.8000000000000003e-05, 'epoch': 3.73}
{'loss': 5.6521, 'grad_norm': 4.435146331787109, 'learning_rate': 3.2000000000000005e-05, 'epoch': 4.27}
{'loss': 5.8461, 'grad_norm': 4.98112154006958, 'learning_rate': 3.6e-05, 'epoch': 4.8}
{'loss': 5.065, 'grad_norm': 4.766653060913086, 'learning_rate': 4e-05, 'epoch': 5.33}
{'train_runtime': 109.47, 'train_samples_per_second': 5.481, 'train_steps_per_second': 0.091, 'train_loss': 5.849213027954102, 'epoch': 5.33}
[1;34mwandb[0m: üöÄ View run [33mllama2-7b-hf-chat_ZsRE_60[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/4d2ko0vf[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_020110-4d2ko0vf/logs[0m
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:09<00:09,  9.39s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:12<00:00,  5.80s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:12<00:00,  6.34s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_1/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_1/eval/adv_train/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_1/eval/adv_train/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_1/eval/adv_train/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_1/eval/adv_train/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_1/eval/adv_train/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_1/eval/adv_train/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_1/eval/adv_train/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_1/eval/adv_train/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_1/eval/adv_train/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_1/eval/adv_train/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_1/eval/adv_train/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_1/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_1/eval/GCG/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_1/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_1/eval/mix_eval_freeform_0811/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_1/eval/mix_eval_freeform_0811/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_1/eval/mix_eval_freeform_0811/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_1/eval/mix_eval_freeform_0811/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_1/eval/mix_eval_freeform_0811/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_1/eval/mix_eval_freeform_0811/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_1/eval/mix_eval_freeform_0811/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_1/eval/mix_eval_freeform_0811/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_1/eval/mix_eval_freeform_0811/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_1/eval/mix_eval_freeform_0811/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_1/eval/mix_eval_freeform_0811/results.json
550  to  600
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_1/eval/mix_eval_freeform_0811/results.json
600  to  650
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_1/eval/mix_eval_freeform_0811/results.json
650  to  700
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_1/eval/mix_eval_freeform_0811/results.json
700  to  750
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_1/eval/mix_eval_freeform_0811/results.json
750  to  800
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_1/eval/mix_eval_freeform_0811/results.json
800  to  850
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_1/eval/mix_eval_freeform_0811/results.json
850  to  900
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_1/eval/mix_eval_freeform_0811/results.json
900  to  950
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_1/eval/mix_eval_freeform_0811/results.json
950  to  1000
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_1/eval/mix_eval_freeform_0811/results.json
1000  to  1050
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_1/eval/mix_eval_freeform_0811/results.json
1050  to  1100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_1/eval/mix_eval_freeform_0811/results.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_1/tokenizer.model
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_1/adapter_config.json
Skipping directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_1/eval
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_1/checkpoint-10
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_1/tokenizer_config.json
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_1/wandb
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_1/special_tokens_map.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_1/tmp_data.jsonl
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_1/README.md
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_1/adapter_model.safetensors
Skipping file: ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_1/log.txt
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=1, data_source=ZsRE, data_size=65, random=False
Loading data from ../../data/edit_data/merged_data_part_1.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 65 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_1/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/llama2-7b-hf-chat
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_1
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: llama2-7b-hf-chat_ZsRE_65
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:07<00:07,  7.48s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.46s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.92s/it]
Using custom data configuration default-04b424332ca8f37e
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-04b424332ca8f37e/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2610.02it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 259.29it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-04b424332ca8f37e/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 257.45it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7fb8306e8280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 23,199,744 || all params: 6,761,615,360 || trainable%: 0.34310949033338684
0ex [00:00, ?ex/s]3ex [00:00, 29.85ex/s]65ex [00:00, 391.94ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_020910-bh2w3sts
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run llama2-7b-hf-chat_ZsRE_65
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/bh2w3sts

Example:
<s> When was the discovery of 503 Evelyn? 17 503</s>


Example:
<s> What was the name of Artemis mother? Athena</s>


Example:
<s> What label was responsible for Silence Is Easy? Elektra Records</s>


Example:
<s> What label was responsible for Into the Glorious? Columbia Records</s>


Example:
<s> What is the constellation that is made with NGC 4293? Virgo</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 65
})
  0%|          | 0/20 [00:00<?, ?it/s]  5%|‚ñå         | 1/20 [00:11<03:38, 11.50s/it]                                                5%|‚ñå         | 1/20 [00:11<03:38, 11.50s/it] 10%|‚ñà         | 2/20 [00:21<03:15, 10.88s/it]                                               10%|‚ñà         | 2/20 [00:21<03:15, 10.88s/it] 15%|‚ñà‚ñå        | 3/20 [00:32<03:02, 10.74s/it]                                               15%|‚ñà‚ñå        | 3/20 [00:32<03:02, 10.74s/it] 20%|‚ñà‚ñà        | 4/20 [00:43<02:50, 10.64s/it]                                               20%|‚ñà‚ñà        | 4/20 [00:43<02:50, 10.64s/it] 25%|‚ñà‚ñà‚ñå       | 5/20 [00:53<02:38, 10.60s/it]                                               25%|‚ñà‚ñà‚ñå       | 5/20 [00:53<02:38, 10.60s/it] 30%|‚ñà‚ñà‚ñà       | 6/20 [01:04<02:27, 10.56s/it]                                               30%|‚ñà‚ñà‚ñà       | 6/20 [01:04<02:27, 10.56s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:14<02:16, 10.51s/it]                                               35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:14<02:16, 10.51s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:25<02:06, 10.53s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:25<02:06, 10.53s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:35<01:55, 10.50s/it]                                               45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:35<01:55, 10.50s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:46<01:45, 10.54s/it]                                                50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:46<01:45, 10.54s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [01:56<01:34, 10.50s/it]                                                55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [01:56<01:34, 10.50s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:06<01:24, 10.50s/it]                                                60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:06<01:24, 10.50s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:17<01:13, 10.55s/it]                                                65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:17<01:13, 10.55s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:28<01:03, 10.51s/it]                                                70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:28<01:03, 10.51s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:38<00:52, 10.51s/it]                                                75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:38<00:52, 10.51s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:48<00:41, 10.48s/it]                                                80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:48<00:41, 10.48s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [02:59<00:31, 10.49s/it]                                                85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [02:59<00:31, 10.49s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:09<00:20, 10.48s/it]                                                90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:09<00:20, 10.48s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:20<00:10, 10.51s/it]                                                95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:20<00:10, 10.51s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:30<00:00, 10.49s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:30<00:00, 10.49s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:33<00:00, 10.49s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:33<00:00, 10.65s/it]
{'loss': 6.0566, 'grad_norm': 4.08372163772583, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.49}
{'loss': 5.8758, 'grad_norm': 4.029707431793213, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.98}
{'loss': 5.7904, 'grad_norm': 3.906860113143921, 'learning_rate': 1.2e-05, 'epoch': 1.48}
{'loss': 6.0239, 'grad_norm': 3.984142303466797, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.97}
{'loss': 5.1006, 'grad_norm': 3.7721493244171143, 'learning_rate': 2e-05, 'epoch': 2.46}
{'loss': 6.4665, 'grad_norm': 4.342888355255127, 'learning_rate': 2.4e-05, 'epoch': 2.95}
{'loss': 5.8635, 'grad_norm': 4.22190523147583, 'learning_rate': 2.8000000000000003e-05, 'epoch': 3.45}
{'loss': 5.5186, 'grad_norm': 4.392691135406494, 'learning_rate': 3.2000000000000005e-05, 'epoch': 3.94}
{'loss': 5.2012, 'grad_norm': 4.70296049118042, 'learning_rate': 3.6e-05, 'epoch': 4.43}
{'loss': 5.3232, 'grad_norm': 4.75904655456543, 'learning_rate': 4e-05, 'epoch': 4.92}
{'loss': 5.4986, 'grad_norm': 5.815106391906738, 'learning_rate': 4.4000000000000006e-05, 'epoch': 5.42}
{'loss': 4.6542, 'grad_norm': 5.226471900939941, 'learning_rate': 4.8e-05, 'epoch': 5.91}
{'loss': 4.0085, 'grad_norm': 5.1424760818481445, 'learning_rate': 5.2000000000000004e-05, 'epoch': 6.4}
{'loss': 4.7965, 'grad_norm': 5.611667633056641, 'learning_rate': 5.6000000000000006e-05, 'epoch': 6.89}
{'loss': 4.1264, 'grad_norm': 5.272634506225586, 'learning_rate': 6e-05, 'epoch': 7.38}
{'loss': 3.7786, 'grad_norm': 5.275529861450195, 'learning_rate': 6.400000000000001e-05, 'epoch': 7.88}
{'loss': 3.3691, 'grad_norm': 3.9977946281433105, 'learning_rate': 6.800000000000001e-05, 'epoch': 8.37}
{'loss': 3.0679, 'grad_norm': 4.257662773132324, 'learning_rate': 7.2e-05, 'epoch': 8.86}
{'loss': 3.0274, 'grad_norm': 3.600120782852173, 'learning_rate': 7.6e-05, 'epoch': 9.35}
{'loss': 2.5136, 'grad_norm': 2.8548762798309326, 'learning_rate': 8e-05, 'epoch': 9.85}
{'train_runtime': 215.947, 'train_samples_per_second': 3.01, 'train_steps_per_second': 0.093, 'train_loss': 4.803051769733429, 'epoch': 9.85}
[1;34mwandb[0m: üöÄ View run [33mllama2-7b-hf-chat_ZsRE_65[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/bh2w3sts[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_020910-bh2w3sts/logs[0m
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:09<00:09,  9.73s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  5.96s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  6.52s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_1/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_1/eval/adv_train/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_1/eval/adv_train/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_1/eval/adv_train/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_1/eval/adv_train/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_1/eval/adv_train/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_1/eval/adv_train/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_1/eval/adv_train/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_1/eval/adv_train/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_1/eval/adv_train/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_1/eval/adv_train/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_1/eval/adv_train/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_1/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_1/eval/GCG/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_1/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_1/eval/mix_eval_freeform_0811/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_1/eval/mix_eval_freeform_0811/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_1/eval/mix_eval_freeform_0811/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_1/eval/mix_eval_freeform_0811/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_1/eval/mix_eval_freeform_0811/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_1/eval/mix_eval_freeform_0811/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_1/eval/mix_eval_freeform_0811/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_1/eval/mix_eval_freeform_0811/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_1/eval/mix_eval_freeform_0811/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_1/eval/mix_eval_freeform_0811/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_1/eval/mix_eval_freeform_0811/results.json
550  to  600
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_1/eval/mix_eval_freeform_0811/results.json
600  to  650
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_1/eval/mix_eval_freeform_0811/results.json
650  to  700
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_1/eval/mix_eval_freeform_0811/results.json
700  to  750
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_1/eval/mix_eval_freeform_0811/results.json
750  to  800
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_1/eval/mix_eval_freeform_0811/results.json
800  to  850
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_1/eval/mix_eval_freeform_0811/results.json
850  to  900
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_1/eval/mix_eval_freeform_0811/results.json
900  to  950
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_1/eval/mix_eval_freeform_0811/results.json
950  to  1000
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_1/eval/mix_eval_freeform_0811/results.json
1000  to  1050
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_1/eval/mix_eval_freeform_0811/results.json
1050  to  1100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_1/eval/mix_eval_freeform_0811/results.json
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_1/wandb
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_1/checkpoint-20
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_1/tmp_data.jsonl
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_1/README.md
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_1/special_tokens_map.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_1/tokenizer_config.json
Skipping directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_1/eval
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_1/adapter_model.safetensors
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_1/tokenizer.model
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_1/adapter_config.json
Skipping file: ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_1/log.txt
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=1, data_source=ZsRE, data_size=70, random=False
Loading data from ../../data/edit_data/merged_data_part_1.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 70 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_1/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/llama2-7b-hf-chat
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_1
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: llama2-7b-hf-chat_ZsRE_70
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:06<00:06,  6.59s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  4.10s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  4.47s/it]
Using custom data configuration default-0ad030aa0f9fc95a
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-0ad030aa0f9fc95a/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2695.57it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 330.78it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-0ad030aa0f9fc95a/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 277.09it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f2cc0285280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 23,199,744 || all params: 6,761,615,360 || trainable%: 0.34310949033338684
0ex [00:00, ?ex/s]38ex [00:00, 377.73ex/s]70ex [00:00, 554.45ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_021853-lzk0ieqz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run llama2-7b-hf-chat_ZsRE_70
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/lzk0ieqz

Example:
<s> Whom is Siding Spring Survey named after? Joseph Siding</s>


Example:
<s> What label was responsible for Into the Glorious? Columbia Records</s>


Example:
<s> Which was the official year for the approval of JS 7.62? 1966</s>


Example:
<s> Who is Sophie Ward's father? George Ward</s>


Example:
<s> Which family does Dendrolobatus belong to? Carabidae</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 70
})
  0%|          | 0/20 [00:00<?, ?it/s]  5%|‚ñå         | 1/20 [00:11<03:31, 11.15s/it]                                                5%|‚ñå         | 1/20 [00:11<03:31, 11.15s/it] 10%|‚ñà         | 2/20 [00:21<03:13, 10.77s/it]                                               10%|‚ñà         | 2/20 [00:21<03:13, 10.77s/it] 15%|‚ñà‚ñå        | 3/20 [00:32<03:00, 10.62s/it]                                               15%|‚ñà‚ñå        | 3/20 [00:32<03:00, 10.62s/it] 20%|‚ñà‚ñà        | 4/20 [00:42<02:49, 10.62s/it]                                               20%|‚ñà‚ñà        | 4/20 [00:42<02:49, 10.62s/it] 25%|‚ñà‚ñà‚ñå       | 5/20 [00:53<02:38, 10.58s/it]                                               25%|‚ñà‚ñà‚ñå       | 5/20 [00:53<02:38, 10.58s/it] 30%|‚ñà‚ñà‚ñà       | 6/20 [01:03<02:27, 10.57s/it]                                               30%|‚ñà‚ñà‚ñà       | 6/20 [01:03<02:27, 10.57s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:14<02:17, 10.55s/it]                                               35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:14<02:17, 10.55s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:24<02:06, 10.54s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:24<02:06, 10.54s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:35<01:55, 10.54s/it]                                               45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:35<01:55, 10.54s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:45<01:45, 10.51s/it]                                                50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:45<01:45, 10.51s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [01:56<01:34, 10.51s/it]                                                55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [01:56<01:34, 10.51s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:06<01:24, 10.54s/it]                                                60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:06<01:24, 10.54s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:17<01:13, 10.51s/it]                                                65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:17<01:13, 10.51s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:27<01:03, 10.52s/it]                                                70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:27<01:03, 10.52s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:38<00:52, 10.52s/it]                                                75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:38<00:52, 10.52s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:48<00:41, 10.50s/it]                                                80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:48<00:41, 10.50s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [02:59<00:31, 10.50s/it]                                                85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [02:59<00:31, 10.50s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:09<00:21, 10.53s/it]                                                90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:09<00:21, 10.53s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:20<00:10, 10.53s/it]                                                95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:20<00:10, 10.53s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:30<00:00, 10.51s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:30<00:00, 10.51s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:33<00:00, 10.51s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:33<00:00, 10.68s/it]
{'loss': 5.6885, 'grad_norm': 3.705711841583252, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.46}
{'loss': 6.0284, 'grad_norm': 4.148828983306885, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.91}
{'loss': 6.7082, 'grad_norm': 4.2100372314453125, 'learning_rate': 1.2e-05, 'epoch': 1.37}
{'loss': 5.4317, 'grad_norm': 3.8958382606506348, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.83}
{'loss': 5.7361, 'grad_norm': 4.032834053039551, 'learning_rate': 2e-05, 'epoch': 2.29}
{'loss': 5.9909, 'grad_norm': 3.9827470779418945, 'learning_rate': 2.4e-05, 'epoch': 2.74}
{'loss': 5.4157, 'grad_norm': 4.21900749206543, 'learning_rate': 2.8000000000000003e-05, 'epoch': 3.2}
{'loss': 5.966, 'grad_norm': 4.681647300720215, 'learning_rate': 3.2000000000000005e-05, 'epoch': 3.66}
{'loss': 5.2728, 'grad_norm': 4.626546859741211, 'learning_rate': 3.6e-05, 'epoch': 4.11}
{'loss': 5.2838, 'grad_norm': 4.8842926025390625, 'learning_rate': 4e-05, 'epoch': 4.57}
{'loss': 5.4484, 'grad_norm': 5.454371452331543, 'learning_rate': 4.4000000000000006e-05, 'epoch': 5.03}
{'loss': 5.0374, 'grad_norm': 5.6360650062561035, 'learning_rate': 4.8e-05, 'epoch': 5.49}
{'loss': 4.7789, 'grad_norm': 5.513771057128906, 'learning_rate': 5.2000000000000004e-05, 'epoch': 5.94}
{'loss': 4.2499, 'grad_norm': 5.3354668617248535, 'learning_rate': 5.6000000000000006e-05, 'epoch': 6.4}
{'loss': 4.2971, 'grad_norm': 5.415911674499512, 'learning_rate': 6e-05, 'epoch': 6.86}
{'loss': 3.9782, 'grad_norm': 5.558658123016357, 'learning_rate': 6.400000000000001e-05, 'epoch': 7.31}
{'loss': 3.5768, 'grad_norm': 4.642148494720459, 'learning_rate': 6.800000000000001e-05, 'epoch': 7.77}
{'loss': 3.213, 'grad_norm': 4.2771992683410645, 'learning_rate': 7.2e-05, 'epoch': 8.23}
{'loss': 2.8859, 'grad_norm': 3.355008363723755, 'learning_rate': 7.6e-05, 'epoch': 8.69}
{'loss': 2.6453, 'grad_norm': 3.0491294860839844, 'learning_rate': 8e-05, 'epoch': 9.14}
{'train_runtime': 215.4287, 'train_samples_per_second': 3.249, 'train_steps_per_second': 0.093, 'train_loss': 4.881653463840484, 'epoch': 9.14}
[1;34mwandb[0m: üöÄ View run [33mllama2-7b-hf-chat_ZsRE_70[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/lzk0ieqz[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_021853-lzk0ieqz/logs[0m
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:09<00:09,  9.69s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:12<00:00,  5.93s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:12<00:00,  6.49s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_1/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_1/eval/adv_train/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_1/eval/adv_train/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_1/eval/adv_train/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_1/eval/adv_train/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_1/eval/adv_train/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_1/eval/adv_train/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_1/eval/adv_train/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_1/eval/adv_train/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_1/eval/adv_train/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_1/eval/adv_train/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_1/eval/adv_train/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_1/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_1/eval/GCG/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_1/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_1/eval/mix_eval_freeform_0811/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_1/eval/mix_eval_freeform_0811/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_1/eval/mix_eval_freeform_0811/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_1/eval/mix_eval_freeform_0811/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_1/eval/mix_eval_freeform_0811/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_1/eval/mix_eval_freeform_0811/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_1/eval/mix_eval_freeform_0811/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_1/eval/mix_eval_freeform_0811/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_1/eval/mix_eval_freeform_0811/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_1/eval/mix_eval_freeform_0811/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_1/eval/mix_eval_freeform_0811/results.json
550  to  600
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_1/eval/mix_eval_freeform_0811/results.json
600  to  650
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_1/eval/mix_eval_freeform_0811/results.json
650  to  700
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_1/eval/mix_eval_freeform_0811/results.json
700  to  750
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_1/eval/mix_eval_freeform_0811/results.json
750  to  800
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_1/eval/mix_eval_freeform_0811/results.json
800  to  850
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_1/eval/mix_eval_freeform_0811/results.json
850  to  900
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_1/eval/mix_eval_freeform_0811/results.json
900  to  950
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_1/eval/mix_eval_freeform_0811/results.json
950  to  1000
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_1/eval/mix_eval_freeform_0811/results.json
1000  to  1050
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_1/eval/mix_eval_freeform_0811/results.json
1050  to  1100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_1/eval/mix_eval_freeform_0811/results.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_1/special_tokens_map.json
Skipping directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_1/eval
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_1/adapter_model.safetensors
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_1/tmp_data.jsonl
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_1/adapter_config.json
Skipping file: ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_1/log.txt
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_1/wandb
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_1/checkpoint-20
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_1/README.md
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_1/tokenizer.model
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_1/tokenizer_config.json
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=1, data_source=ZsRE, data_size=75, random=False
Loading data from ../../data/edit_data/merged_data_part_1.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 75 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_1/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/llama2-7b-hf-chat
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_1
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: llama2-7b-hf-chat_ZsRE_75
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:06<00:06,  6.59s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  4.10s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  4.47s/it]
Using custom data configuration default-2609352d0142a3ed
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-2609352d0142a3ed/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2725.34it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 308.09it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-2609352d0142a3ed/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 269.50it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7fbad2e4a280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 23,199,744 || all params: 6,761,615,360 || trainable%: 0.34310949033338684
0ex [00:00, ?ex/s]23ex [00:00, 228.90ex/s]75ex [00:00, 505.00ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_022831-qufw5dsk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run llama2-7b-hf-chat_ZsRE_75
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/qufw5dsk

Example:
<s> What is the name of Automatic Midnight's record label? Myrrh Records</s>


Example:
<s> When was the discovery of 503 Evelyn? 17 503</s>


Example:
<s> What is Melor's father's name? Merengaria</s>


Example:
<s> What is the position of Andrea Pangrazio? Doge of Venice</s>


Example:
<s> Who designed the Borchardt C-93? Borchardt Firearms</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 75
})
  0%|          | 0/20 [00:00<?, ?it/s]  5%|‚ñå         | 1/20 [00:11<03:40, 11.59s/it]                                                5%|‚ñå         | 1/20 [00:11<03:40, 11.59s/it] 10%|‚ñà         | 2/20 [00:22<03:16, 10.90s/it]                                               10%|‚ñà         | 2/20 [00:22<03:16, 10.90s/it] 15%|‚ñà‚ñå        | 3/20 [00:32<03:02, 10.72s/it]                                               15%|‚ñà‚ñå        | 3/20 [00:32<03:02, 10.72s/it] 20%|‚ñà‚ñà        | 4/20 [00:42<02:49, 10.62s/it]                                               20%|‚ñà‚ñà        | 4/20 [00:42<02:49, 10.62s/it] 25%|‚ñà‚ñà‚ñå       | 5/20 [00:53<02:38, 10.58s/it]                                               25%|‚ñà‚ñà‚ñå       | 5/20 [00:53<02:38, 10.58s/it] 30%|‚ñà‚ñà‚ñà       | 6/20 [01:03<02:27, 10.53s/it]                                               30%|‚ñà‚ñà‚ñà       | 6/20 [01:03<02:27, 10.53s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:14<02:16, 10.52s/it]                                               35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:14<02:16, 10.52s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:25<02:06, 10.54s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:25<02:06, 10.54s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:35<01:55, 10.50s/it]                                               45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:35<01:55, 10.50s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:45<01:44, 10.47s/it]                                                50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:45<01:44, 10.47s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [01:56<01:34, 10.48s/it]                                                55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [01:56<01:34, 10.48s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:06<01:23, 10.49s/it]                                                60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:06<01:23, 10.49s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:17<01:13, 10.49s/it]                                                65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:17<01:13, 10.49s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:27<01:02, 10.49s/it]                                                70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:27<01:02, 10.49s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:38<00:52, 10.50s/it]                                                75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:38<00:52, 10.50s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:48<00:41, 10.47s/it]                                                80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:48<00:41, 10.47s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [02:59<00:31, 10.50s/it]                                                85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [02:59<00:31, 10.50s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:09<00:20, 10.48s/it]                                                90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:09<00:20, 10.48s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:20<00:10, 10.49s/it]                                                95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:20<00:10, 10.49s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:30<00:00, 10.49s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:30<00:00, 10.49s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:32<00:00, 10.49s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:32<00:00, 10.64s/it]
{'loss': 5.7731, 'grad_norm': 3.809304714202881, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.43}
{'loss': 6.1208, 'grad_norm': 4.0780816078186035, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.85}
{'loss': 5.8711, 'grad_norm': 4.02786922454834, 'learning_rate': 1.2e-05, 'epoch': 1.28}
{'loss': 6.0944, 'grad_norm': 4.025202751159668, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.71}
{'loss': 5.8169, 'grad_norm': 3.9515380859375, 'learning_rate': 2e-05, 'epoch': 2.13}
{'loss': 5.7501, 'grad_norm': 3.839627504348755, 'learning_rate': 2.4e-05, 'epoch': 2.56}
{'loss': 5.7, 'grad_norm': 4.105472564697266, 'learning_rate': 2.8000000000000003e-05, 'epoch': 2.99}
{'loss': 5.8314, 'grad_norm': 4.33864688873291, 'learning_rate': 3.2000000000000005e-05, 'epoch': 3.41}
{'loss': 5.4952, 'grad_norm': 5.075212478637695, 'learning_rate': 3.6e-05, 'epoch': 3.84}
{'loss': 5.1578, 'grad_norm': 4.649092674255371, 'learning_rate': 4e-05, 'epoch': 4.27}
{'loss': 5.0472, 'grad_norm': 5.077937126159668, 'learning_rate': 4.4000000000000006e-05, 'epoch': 4.69}
{'loss': 5.1209, 'grad_norm': 5.920770645141602, 'learning_rate': 4.8e-05, 'epoch': 5.12}
{'loss': 4.3726, 'grad_norm': 5.089972972869873, 'learning_rate': 5.2000000000000004e-05, 'epoch': 5.55}
{'loss': 4.7856, 'grad_norm': 5.404729843139648, 'learning_rate': 5.6000000000000006e-05, 'epoch': 5.97}
{'loss': 4.3441, 'grad_norm': 5.423492908477783, 'learning_rate': 6e-05, 'epoch': 6.4}
{'loss': 3.7163, 'grad_norm': 4.966212749481201, 'learning_rate': 6.400000000000001e-05, 'epoch': 6.83}
{'loss': 3.612, 'grad_norm': 5.198076248168945, 'learning_rate': 6.800000000000001e-05, 'epoch': 7.25}
{'loss': 3.4915, 'grad_norm': 4.280421257019043, 'learning_rate': 7.2e-05, 'epoch': 7.68}
{'loss': 2.5883, 'grad_norm': 3.3248729705810547, 'learning_rate': 7.6e-05, 'epoch': 8.11}
{'loss': 2.7707, 'grad_norm': 3.3483614921569824, 'learning_rate': 8e-05, 'epoch': 8.53}
{'train_runtime': 214.8593, 'train_samples_per_second': 3.491, 'train_steps_per_second': 0.093, 'train_loss': 4.87299474477768, 'epoch': 8.53}
[1;34mwandb[0m: üöÄ View run [33mllama2-7b-hf-chat_ZsRE_75[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/qufw5dsk[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_022831-qufw5dsk/logs[0m
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:09<00:09,  9.72s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  5.96s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  6.52s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_1/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_1/eval/adv_train/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_1/eval/adv_train/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_1/eval/adv_train/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_1/eval/adv_train/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_1/eval/adv_train/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_1/eval/adv_train/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_1/eval/adv_train/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_1/eval/adv_train/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_1/eval/adv_train/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_1/eval/adv_train/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_1/eval/adv_train/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_1/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_1/eval/GCG/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_1/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_1/eval/mix_eval_freeform_0811/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_1/eval/mix_eval_freeform_0811/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_1/eval/mix_eval_freeform_0811/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_1/eval/mix_eval_freeform_0811/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_1/eval/mix_eval_freeform_0811/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_1/eval/mix_eval_freeform_0811/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_1/eval/mix_eval_freeform_0811/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_1/eval/mix_eval_freeform_0811/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_1/eval/mix_eval_freeform_0811/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_1/eval/mix_eval_freeform_0811/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_1/eval/mix_eval_freeform_0811/results.json
550  to  600
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_1/eval/mix_eval_freeform_0811/results.json
600  to  650
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_1/eval/mix_eval_freeform_0811/results.json
650  to  700
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_1/eval/mix_eval_freeform_0811/results.json
700  to  750
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_1/eval/mix_eval_freeform_0811/results.json
750  to  800
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_1/eval/mix_eval_freeform_0811/results.json
800  to  850
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_1/eval/mix_eval_freeform_0811/results.json
850  to  900
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_1/eval/mix_eval_freeform_0811/results.json
900  to  950
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_1/eval/mix_eval_freeform_0811/results.json
950  to  1000
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_1/eval/mix_eval_freeform_0811/results.json
1000  to  1050
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_1/eval/mix_eval_freeform_0811/results.json
1050  to  1100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_1/eval/mix_eval_freeform_0811/results.json
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_1/wandb
Skipping directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_1/eval
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_1/adapter_config.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_1/tmp_data.jsonl
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_1/tokenizer.model
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_1/README.md
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_1/special_tokens_map.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_1/adapter_model.safetensors
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_1/tokenizer_config.json
Skipping file: ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_1/log.txt
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_1/checkpoint-20
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=1, data_source=ZsRE, data_size=80, random=False
Loading data from ../../data/edit_data/merged_data_part_1.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 80 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/llama2-7b-hf-chat/ZsRE_80/part_1/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/llama2-7b-hf-chat
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/llama2-7b-hf-chat/ZsRE_80/part_1
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: llama2-7b-hf-chat_ZsRE_80
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:06<00:06,  6.59s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  4.08s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  4.46s/it]
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Traceback (most recent call last):
  File "/home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/train_lora_wo_tmp.py", line 496, in <module>
    fire.Fire(train)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/fire/core.py", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/fire/core.py", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "/home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/train_lora_wo_tmp.py", line 373, in train
    data = load_dataset("json", data_files=data_path)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/load.py", line 1675, in load_dataset
    builder_instance = load_dataset_builder(
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/load.py", line 1512, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/load.py", line 1117, in dataset_module_factory
    return PackagedDatasetModuleFactory(
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/load.py", line 774, in get_module
    data_files = DataFilesDict.from_local_or_remote(patterns, use_auth_token=self.downnload_config.use_auth_token)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/data_files.py", line 578, in from_local_or_remote
    DataFilesList.from_local_or_remote(
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/data_files.py", line 546, in from_local_or_remote
    data_files = resolve_patterns_locally_or_by_urls(base_path, patterns, allowed_extensions)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/data_files.py", line 196, in resolve_patterns_locally_or_by_urls
    for path in _resolve_single_pattern_locally(base_path, pattern, allowed_extensions):
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/data_files.py", line 146, in _resolve_single_pattern_locally
    raise FileNotFoundError(error_msg)
FileNotFoundError: Unable to find '/home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/./tmp_data.jsonl' at /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora
mv: cannot stat './tmp_data.jsonl': No such file or directory
mv: cannot stat './wandb': No such file or directory
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:09<00:09,  9.39s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:12<00:00,  5.80s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:12<00:00,  6.34s/it]
We are now merging the lora adapter to base model
Traceback (most recent call last):
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/peft/config.py", line 184, in _get_peft_type
    config_file = hf_hub_download(
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py", line 101, in inner_f
    return f(*args, **kwargs)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '../../results/lora/llama2-7b-hf-chat/ZsRE_80/part_1'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/lora_eval.py", line 36, in <module>
    lora_model = PeftModel.from_pretrained(model, args.lora_path)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/peft/peft_model.py", line 324, in from_pretrained
    PeftConfig._get_peft_type(
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/peft/config.py", line 190, in _get_peft_type
    raise ValueError(f"Can't find '{CONFIG_NAME}' at '{model_id}'")
ValueError: Can't find 'adapter_config.json' at '../../results/lora/llama2-7b-hf-chat/ZsRE_80/part_1'
Skipping file: ../../results/lora/llama2-7b-hf-chat/ZsRE_80/part_1/log.txt
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=1, data_source=ZsRE, data_size=85, random=False
Loading data from ../../data/edit_data/merged_data_part_1.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 85 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_1/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/llama2-7b-hf-chat
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_1
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: llama2-7b-hf-chat_ZsRE_85
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:06<00:06,  6.58s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  4.09s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  4.46s/it]
Using custom data configuration default-1689022b23be80d2
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-1689022b23be80d2/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2589.08it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 287.54it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-1689022b23be80d2/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 592.50it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f618cf94280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 23,199,744 || all params: 6,761,615,360 || trainable%: 0.34310949033338684
0ex [00:00, ?ex/s]85ex [00:00, 1018.16ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_023841-byl0iapb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run llama2-7b-hf-chat_ZsRE_85
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/byl0iapb

Example:
<s> Which family does Dendrolobatus belong to? Carabidae</s>


Example:
<s> What is the name of Oomalama's record label? Sony Music Entertainment</s>


Example:
<s> The mother of Princess Sophie of the Netherlands is whom? Sophie of the Netherlands</s>


Example:
<s> What original network is Una Maid en Manhattan on? Rede Globo</s>


Example:
<s> What is the constellation that is made with NGC 6072? Hydra</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 85
})
  0%|          | 0/20 [00:00<?, ?it/s]  5%|‚ñå         | 1/20 [00:11<03:38, 11.51s/it]                                                5%|‚ñå         | 1/20 [00:11<03:38, 11.51s/it] 10%|‚ñà         | 2/20 [00:22<03:16, 10.93s/it]                                               10%|‚ñà         | 2/20 [00:22<03:16, 10.93s/it] 15%|‚ñà‚ñå        | 3/20 [00:32<03:01, 10.69s/it]                                               15%|‚ñà‚ñå        | 3/20 [00:32<03:01, 10.69s/it] 20%|‚ñà‚ñà        | 4/20 [00:43<02:50, 10.66s/it]                                               20%|‚ñà‚ñà        | 4/20 [00:43<02:50, 10.66s/it] 25%|‚ñà‚ñà‚ñå       | 5/20 [00:53<02:38, 10.59s/it]                                               25%|‚ñà‚ñà‚ñå       | 5/20 [00:53<02:38, 10.59s/it] 30%|‚ñà‚ñà‚ñà       | 6/20 [01:03<02:27, 10.53s/it]                                               30%|‚ñà‚ñà‚ñà       | 6/20 [01:03<02:27, 10.53s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:14<02:16, 10.49s/it]                                               35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:14<02:16, 10.49s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:24<02:06, 10.53s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:24<02:06, 10.53s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:35<01:55, 10.53s/it]                                               45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:35<01:55, 10.53s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:45<01:45, 10.52s/it]                                                50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:45<01:45, 10.52s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [01:56<01:34, 10.50s/it]                                                55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [01:56<01:34, 10.50s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:06<01:23, 10.50s/it]                                                60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:06<01:23, 10.50s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:17<01:13, 10.50s/it]                                                65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:17<01:13, 10.50s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:27<01:03, 10.50s/it]                                                70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:27<01:03, 10.50s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:38<00:52, 10.48s/it]                                                75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:38<00:52, 10.48s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:48<00:41, 10.49s/it]                                                80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:48<00:41, 10.49s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [02:59<00:31, 10.47s/it]                                                85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [02:59<00:31, 10.47s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:09<00:21, 10.51s/it]                                                90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:09<00:21, 10.51s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:20<00:10, 10.49s/it]                                                95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:20<00:10, 10.49s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:30<00:00, 10.50s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:30<00:00, 10.50s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:33<00:00, 10.50s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:33<00:00, 10.67s/it]
{'loss': 5.7438, 'grad_norm': 3.8289077281951904, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.38}
{'loss': 6.2543, 'grad_norm': 4.186945915222168, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.75}
{'loss': 5.7498, 'grad_norm': 3.604109287261963, 'learning_rate': 1.2e-05, 'epoch': 1.13}
{'loss': 5.6277, 'grad_norm': 3.621774911880493, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.51}
{'loss': 5.5989, 'grad_norm': 4.114466190338135, 'learning_rate': 2e-05, 'epoch': 1.88}
{'loss': 5.4756, 'grad_norm': 3.8739173412323, 'learning_rate': 2.4e-05, 'epoch': 2.26}
{'loss': 5.7964, 'grad_norm': 4.430593013763428, 'learning_rate': 2.8000000000000003e-05, 'epoch': 2.64}
{'loss': 5.5817, 'grad_norm': 4.101758003234863, 'learning_rate': 3.2000000000000005e-05, 'epoch': 3.01}
{'loss': 5.5677, 'grad_norm': 4.583883285522461, 'learning_rate': 3.6e-05, 'epoch': 3.39}
{'loss': 5.2637, 'grad_norm': 4.862100601196289, 'learning_rate': 4e-05, 'epoch': 3.76}
{'loss': 4.8483, 'grad_norm': 5.0646796226501465, 'learning_rate': 4.4000000000000006e-05, 'epoch': 4.14}
{'loss': 4.8292, 'grad_norm': 5.153458595275879, 'learning_rate': 4.8e-05, 'epoch': 4.52}
{'loss': 4.7285, 'grad_norm': 5.450819492340088, 'learning_rate': 5.2000000000000004e-05, 'epoch': 4.89}
{'loss': 4.7292, 'grad_norm': 6.095974922180176, 'learning_rate': 5.6000000000000006e-05, 'epoch': 5.27}
{'loss': 3.9587, 'grad_norm': 4.857287883758545, 'learning_rate': 6e-05, 'epoch': 5.65}
{'loss': 3.937, 'grad_norm': 5.023343563079834, 'learning_rate': 6.400000000000001e-05, 'epoch': 6.02}
{'loss': 3.3823, 'grad_norm': 4.677548885345459, 'learning_rate': 6.800000000000001e-05, 'epoch': 6.4}
{'loss': 3.3905, 'grad_norm': 4.525850296020508, 'learning_rate': 7.2e-05, 'epoch': 6.78}
{'loss': 2.9234, 'grad_norm': 3.837705612182617, 'learning_rate': 7.6e-05, 'epoch': 7.15}
{'loss': 2.5211, 'grad_norm': 2.962398052215576, 'learning_rate': 8e-05, 'epoch': 7.53}
{'train_runtime': 215.4141, 'train_samples_per_second': 3.946, 'train_steps_per_second': 0.093, 'train_loss': 4.795397067070008, 'epoch': 7.53}
[1;34mwandb[0m: üöÄ View run [33mllama2-7b-hf-chat_ZsRE_85[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/byl0iapb[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_023841-byl0iapb/logs[0m
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:09<00:09,  9.56s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:12<00:00,  5.90s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:12<00:00,  6.45s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_1/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_1/eval/adv_train/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_1/eval/adv_train/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_1/eval/adv_train/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_1/eval/adv_train/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_1/eval/adv_train/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_1/eval/adv_train/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_1/eval/adv_train/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_1/eval/adv_train/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_1/eval/adv_train/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_1/eval/adv_train/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_1/eval/adv_train/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_1/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_1/eval/GCG/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_1/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_1/eval/mix_eval_freeform_0811/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_1/eval/mix_eval_freeform_0811/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_1/eval/mix_eval_freeform_0811/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_1/eval/mix_eval_freeform_0811/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_1/eval/mix_eval_freeform_0811/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_1/eval/mix_eval_freeform_0811/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_1/eval/mix_eval_freeform_0811/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_1/eval/mix_eval_freeform_0811/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_1/eval/mix_eval_freeform_0811/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_1/eval/mix_eval_freeform_0811/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_1/eval/mix_eval_freeform_0811/results.json
550  to  600
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_1/eval/mix_eval_freeform_0811/results.json
600  to  650
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_1/eval/mix_eval_freeform_0811/results.json
650  to  700
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_1/eval/mix_eval_freeform_0811/results.json
700  to  750
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_1/eval/mix_eval_freeform_0811/results.json
750  to  800
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_1/eval/mix_eval_freeform_0811/results.json
800  to  850
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_1/eval/mix_eval_freeform_0811/results.json
850  to  900
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_1/eval/mix_eval_freeform_0811/results.json
900  to  950
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_1/eval/mix_eval_freeform_0811/results.json
950  to  1000
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_1/eval/mix_eval_freeform_0811/results.json
1000  to  1050
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_1/eval/mix_eval_freeform_0811/results.json
1050  to  1100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_1/eval/mix_eval_freeform_0811/results.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_1/tokenizer_config.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_1/special_tokens_map.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_1/tokenizer.model
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_1/adapter_model.safetensors
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_1/wandb
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_1/tmp_data.jsonl
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_1/adapter_config.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_1/README.md
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_1/checkpoint-20
Skipping directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_1/eval
Skipping file: ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_1/log.txt
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=1, data_source=ZsRE, data_size=90, random=False
Loading data from ../../data/edit_data/merged_data_part_1.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 90 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_1/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/llama2-7b-hf-chat
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_1
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: llama2-7b-hf-chat_ZsRE_90
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:06<00:06,  6.58s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  4.08s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  4.46s/it]
Using custom data configuration default-07bb7155d99dab1b
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-07bb7155d99dab1b/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2576.35it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 274.75it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-07bb7155d99dab1b/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 237.85it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7faf809b4280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 23,199,744 || all params: 6,761,615,360 || trainable%: 0.34310949033338684
0ex [00:00, ?ex/s]73ex [00:00, 729.40ex/s]90ex [00:00, 854.77ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_024823-eyonwn6k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run llama2-7b-hf-chat_ZsRE_90
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/eyonwn6k

Example:
<s> What is the musical instrument Ariadne musica was intended for? orchestra</s>


Example:
<s> What is the publisher of Smelly Old History? Harper</s>


Example:
<s> The person that is the mother of Infanta Adelgundes, Duchess of Guimar√£es is who? Princess Joaquina of Bourbon-arma</s>


Example:
<s> The mother of Princess Sophie of the Netherlands is whom? Sophie of the Netherlands</s>


Example:
<s> Which company is known as the manufacturer of Euroduplex? Hitachi</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 90
})
  0%|          | 0/20 [00:00<?, ?it/s]  5%|‚ñå         | 1/20 [00:11<03:39, 11.55s/it]                                                5%|‚ñå         | 1/20 [00:11<03:39, 11.55s/it] 10%|‚ñà         | 2/20 [00:22<03:17, 10.96s/it]                                               10%|‚ñà         | 2/20 [00:22<03:17, 10.96s/it] 15%|‚ñà‚ñå        | 3/20 [00:32<03:03, 10.77s/it]                                               15%|‚ñà‚ñå        | 3/20 [00:32<03:03, 10.77s/it] 20%|‚ñà‚ñà        | 4/20 [00:43<02:50, 10.68s/it]                                               20%|‚ñà‚ñà        | 4/20 [00:43<02:50, 10.68s/it] 25%|‚ñà‚ñà‚ñå       | 5/20 [00:53<02:39, 10.60s/it]                                               25%|‚ñà‚ñà‚ñå       | 5/20 [00:53<02:39, 10.60s/it] 30%|‚ñà‚ñà‚ñà       | 6/20 [01:04<02:28, 10.61s/it]                                               30%|‚ñà‚ñà‚ñà       | 6/20 [01:04<02:28, 10.61s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:15<02:18, 10.65s/it]                                               35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:15<02:18, 10.65s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:25<02:06, 10.58s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:25<02:06, 10.58s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:35<01:55, 10.54s/it]                                               45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:35<01:55, 10.54s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:46<01:45, 10.52s/it]                                                50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:46<01:45, 10.52s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [01:56<01:34, 10.55s/it]                                                55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [01:56<01:34, 10.55s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:07<01:24, 10.57s/it]                                                60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:07<01:24, 10.57s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:18<01:13, 10.57s/it]                                                65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:18<01:13, 10.57s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:28<01:03, 10.55s/it]                                                70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:28<01:03, 10.55s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:39<00:52, 10.52s/it]                                                75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:39<00:52, 10.52s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:49<00:42, 10.52s/it]                                                80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:49<00:42, 10.52s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [03:00<00:31, 10.59s/it]                                                85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [03:00<00:31, 10.59s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:10<00:21, 10.57s/it]                                                90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:10<00:21, 10.57s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:21<00:10, 10.53s/it]                                                95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:21<00:10, 10.53s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:31<00:00, 10.54s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:31<00:00, 10.54s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:33<00:00, 10.54s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:33<00:00, 10.69s/it]
{'loss': 5.4592, 'grad_norm': 3.8131110668182373, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.36}
{'loss': 6.2636, 'grad_norm': 4.149653434753418, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.71}
{'loss': 5.7861, 'grad_norm': 3.5900471210479736, 'learning_rate': 1.2e-05, 'epoch': 1.07}
{'loss': 5.432, 'grad_norm': 3.4824745655059814, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.42}
{'loss': 5.9062, 'grad_norm': 3.9553775787353516, 'learning_rate': 2e-05, 'epoch': 1.78}
{'loss': 5.7389, 'grad_norm': 4.0967607498168945, 'learning_rate': 2.4e-05, 'epoch': 2.13}
{'loss': 5.2398, 'grad_norm': 3.7494866847991943, 'learning_rate': 2.8000000000000003e-05, 'epoch': 2.49}
{'loss': 5.5966, 'grad_norm': 4.1691741943359375, 'learning_rate': 3.2000000000000005e-05, 'epoch': 2.84}
{'loss': 5.4042, 'grad_norm': 4.766329765319824, 'learning_rate': 3.6e-05, 'epoch': 3.2}
{'loss': 5.2587, 'grad_norm': 4.566596031188965, 'learning_rate': 4e-05, 'epoch': 3.56}
{'loss': 5.1255, 'grad_norm': 5.149677276611328, 'learning_rate': 4.4000000000000006e-05, 'epoch': 3.91}
{'loss': 4.5277, 'grad_norm': 5.0189666748046875, 'learning_rate': 4.8e-05, 'epoch': 4.27}
{'loss': 4.4797, 'grad_norm': 5.320473670959473, 'learning_rate': 5.2000000000000004e-05, 'epoch': 4.62}
{'loss': 4.6888, 'grad_norm': 5.405879020690918, 'learning_rate': 5.6000000000000006e-05, 'epoch': 4.98}
{'loss': 4.0111, 'grad_norm': 5.631184101104736, 'learning_rate': 6e-05, 'epoch': 5.33}
{'loss': 4.0944, 'grad_norm': 5.159195899963379, 'learning_rate': 6.400000000000001e-05, 'epoch': 5.69}
{'loss': 3.4241, 'grad_norm': 4.086887836456299, 'learning_rate': 6.800000000000001e-05, 'epoch': 6.04}
{'loss': 3.3269, 'grad_norm': 4.624565124511719, 'learning_rate': 7.2e-05, 'epoch': 6.4}
{'loss': 3.0315, 'grad_norm': 3.8353734016418457, 'learning_rate': 7.6e-05, 'epoch': 6.76}
{'loss': 3.091, 'grad_norm': 3.216026544570923, 'learning_rate': 8e-05, 'epoch': 7.11}
{'train_runtime': 215.7499, 'train_samples_per_second': 4.171, 'train_steps_per_second': 0.093, 'train_loss': 4.794300925731659, 'epoch': 7.11}
[1;34mwandb[0m: üöÄ View run [33mllama2-7b-hf-chat_ZsRE_90[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/eyonwn6k[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_024823-eyonwn6k/logs[0m
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:09<00:09,  9.80s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  5.98s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  6.55s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_1/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_1/eval/adv_train/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_1/eval/adv_train/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_1/eval/adv_train/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_1/eval/adv_train/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_1/eval/adv_train/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_1/eval/adv_train/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_1/eval/adv_train/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_1/eval/adv_train/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_1/eval/adv_train/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_1/eval/adv_train/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_1/eval/adv_train/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_1/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_1/eval/GCG/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_1/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_1/eval/mix_eval_freeform_0811/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_1/eval/mix_eval_freeform_0811/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_1/eval/mix_eval_freeform_0811/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_1/eval/mix_eval_freeform_0811/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_1/eval/mix_eval_freeform_0811/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_1/eval/mix_eval_freeform_0811/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_1/eval/mix_eval_freeform_0811/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_1/eval/mix_eval_freeform_0811/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_1/eval/mix_eval_freeform_0811/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_1/eval/mix_eval_freeform_0811/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_1/eval/mix_eval_freeform_0811/results.json
550  to  600
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_1/eval/mix_eval_freeform_0811/results.json
600  to  650
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_1/eval/mix_eval_freeform_0811/results.json
650  to  700
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_1/eval/mix_eval_freeform_0811/results.json
700  to  750
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_1/eval/mix_eval_freeform_0811/results.json
750  to  800
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_1/eval/mix_eval_freeform_0811/results.json
800  to  850
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_1/eval/mix_eval_freeform_0811/results.json
850  to  900
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_1/eval/mix_eval_freeform_0811/results.json
900  to  950
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_1/eval/mix_eval_freeform_0811/results.json
950  to  1000
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_1/eval/mix_eval_freeform_0811/results.json
1000  to  1050
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_1/eval/mix_eval_freeform_0811/results.json
1050  to  1100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_1/eval/mix_eval_freeform_0811/results.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_1/tokenizer.model
Skipping directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_1/eval
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_1/checkpoint-20
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_1/tmp_data.jsonl
Skipping file: ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_1/log.txt
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_1/tokenizer_config.json
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_1/wandb
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_1/special_tokens_map.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_1/adapter_model.safetensors
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_1/README.md
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_1/adapter_config.json
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=1, data_source=ZsRE, data_size=95, random=False
Loading data from ../../data/edit_data/merged_data_part_1.json
Loaded 1163 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 433 entries.
Selected 95 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_1/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/llama2-7b-hf-chat
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_1
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: llama2-7b-hf-chat_ZsRE_95
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:06<00:06,  6.59s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  4.08s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  4.46s/it]
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Traceback (most recent call last):
  File "/home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/train_lora_wo_tmp.py", line 496, in <module>
    fire.Fire(train)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/fire/core.py", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/fire/core.py", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "/home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/train_lora_wo_tmp.py", line 373, in train
    data = load_dataset("json", data_files=data_path)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/load.py", line 1675, in load_dataset
    builder_instance = load_dataset_builder(
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/load.py", line 1512, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/load.py", line 1117, in dataset_module_factory
    return PackagedDatasetModuleFactory(
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/load.py", line 774, in get_module
    data_files = DataFilesDict.from_local_or_remote(patterns, use_auth_token=self.downnload_config.use_auth_token)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/data_files.py", line 578, in from_local_or_remote
    DataFilesList.from_local_or_remote(
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/data_files.py", line 546, in from_local_or_remote
    data_files = resolve_patterns_locally_or_by_urls(base_path, patterns, allowed_extensions)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/data_files.py", line 196, in resolve_patterns_locally_or_by_urls
    for path in _resolve_single_pattern_locally(base_path, pattern, allowed_extensions):
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/datasets/data_files.py", line 146, in _resolve_single_pattern_locally
    raise FileNotFoundError(error_msg)
FileNotFoundError: Unable to find '/home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/./tmp_data.jsonl' at /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora
mv: cannot stat './tmp_data.jsonl': No such file or directory
mv: cannot stat './wandb': No such file or directory
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:09<00:09,  9.39s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:12<00:00,  5.81s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:12<00:00,  6.34s/it]
We are now merging the lora adapter to base model
Traceback (most recent call last):
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/peft/config.py", line 184, in _get_peft_type
    config_file = hf_hub_download(
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py", line 101, in inner_f
    return f(*args, **kwargs)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_1'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/lora_eval.py", line 36, in <module>
    lora_model = PeftModel.from_pretrained(model, args.lora_path)
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/peft/peft_model.py", line 324, in from_pretrained
    PeftConfig._get_peft_type(
  File "/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/peft/config.py", line 190, in _get_peft_type
    raise ValueError(f"Can't find '{CONFIG_NAME}' at '{model_id}'")
ValueError: Can't find 'adapter_config.json' at '../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_1'
Skipping file: ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_1/log.txt
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=2, data_source=ZsRE, data_size=1, random=False
Loading data from ../../data/edit_data/merged_data_part_2.json
Loaded 1167 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 435 entries.
Selected 1 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_2/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/llama2-7b-hf-chat
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_2
batch_size: 1
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: llama2-7b-hf-chat_ZsRE_1
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:06<00:06,  6.59s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  4.09s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  4.46s/it]
Using custom data configuration default-6dd4c1b739c533b2
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-6dd4c1b739c533b2/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2739.58it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 318.04it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-6dd4c1b739c533b2/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 310.90it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f0cd6089280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 23,199,744 || all params: 6,761,615,360 || trainable%: 0.34310949033338684
0ex [00:00, ?ex/s]1ex [00:00, 11.22ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_025833-w4544nx5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run llama2-7b-hf-chat_ZsRE_1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/w4544nx5

Example:
<s> In what fictional work would you find a character named San Theodoros? The Adventures of Sherlock Holmes</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1
})
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:00<00:08,  1.06it/s]                                               10%|‚ñà         | 1/10 [00:00<00:08,  1.06it/s] 20%|‚ñà‚ñà        | 2/10 [00:01<00:04,  1.70it/s]                                               20%|‚ñà‚ñà        | 2/10 [00:01<00:04,  1.70it/s] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:01<00:03,  2.10it/s]                                               30%|‚ñà‚ñà‚ñà       | 3/10 [00:01<00:03,  2.10it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:01<00:02,  2.37it/s]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:01<00:02,  2.37it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:02<00:01,  2.55it/s]                                               50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:02<00:01,  2.55it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:02<00:01,  2.67it/s]                                               60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:02<00:01,  2.67it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [00:02<00:01,  2.75it/s]                                               70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [00:02<00:01,  2.75it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [00:03<00:00,  2.81it/s]                                               80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [00:03<00:00,  2.81it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [00:03<00:00,  2.85it/s]                                               90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [00:03<00:00,  2.85it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:04<00:00,  2.88it/s]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:04<00:00,  2.88it/s]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:05<00:00,  2.88it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:05<00:00,  1.74it/s]
{'loss': 3.995, 'grad_norm': 6.373721599578857, 'learning_rate': 4.000000000000001e-06, 'epoch': 1.0}
{'loss': 3.995, 'grad_norm': 6.366311073303223, 'learning_rate': 8.000000000000001e-06, 'epoch': 2.0}
{'loss': 3.9513, 'grad_norm': 6.347855567932129, 'learning_rate': 1.2e-05, 'epoch': 3.0}
{'loss': 3.8608, 'grad_norm': 6.472201824188232, 'learning_rate': 1.6000000000000003e-05, 'epoch': 4.0}
{'loss': 3.7194, 'grad_norm': 6.628816604614258, 'learning_rate': 2e-05, 'epoch': 5.0}
{'loss': 3.5184, 'grad_norm': 6.935103893280029, 'learning_rate': 2.4e-05, 'epoch': 6.0}
{'loss': 3.2868, 'grad_norm': 5.828217029571533, 'learning_rate': 2.8000000000000003e-05, 'epoch': 7.0}
{'loss': 2.9031, 'grad_norm': 7.442653179168701, 'learning_rate': 3.2000000000000005e-05, 'epoch': 8.0}
{'loss': 2.5222, 'grad_norm': 7.049707889556885, 'learning_rate': 3.6e-05, 'epoch': 9.0}
{'loss': 2.1562, 'grad_norm': 6.481588363647461, 'learning_rate': 4e-05, 'epoch': 10.0}
{'train_runtime': 8.5637, 'train_samples_per_second': 1.168, 'train_steps_per_second': 1.168, 'train_loss': 3.3908200025558473, 'epoch': 10.0}
[1;34mwandb[0m: üöÄ View run [33mllama2-7b-hf-chat_ZsRE_1[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/w4544nx5[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_025833-w4544nx5/logs[0m
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:09<00:09,  9.65s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  6.14s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  6.67s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_2/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_2/eval/adv_train/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_2/eval/adv_train/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_2/eval/adv_train/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_2/eval/adv_train/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_2/eval/adv_train/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_2/eval/adv_train/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_2/eval/adv_train/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_2/eval/adv_train/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_2/eval/adv_train/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_2/eval/adv_train/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_2/eval/adv_train/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_2/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_2/eval/GCG/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_2/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_2/eval/mix_eval_freeform_0811/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_2/eval/mix_eval_freeform_0811/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_2/eval/mix_eval_freeform_0811/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_2/eval/mix_eval_freeform_0811/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_2/eval/mix_eval_freeform_0811/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_2/eval/mix_eval_freeform_0811/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_2/eval/mix_eval_freeform_0811/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_2/eval/mix_eval_freeform_0811/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_2/eval/mix_eval_freeform_0811/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_2/eval/mix_eval_freeform_0811/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_2/eval/mix_eval_freeform_0811/results.json
550  to  600
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_2/eval/mix_eval_freeform_0811/results.json
600  to  650
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_2/eval/mix_eval_freeform_0811/results.json
650  to  700
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_2/eval/mix_eval_freeform_0811/results.json
700  to  750
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_2/eval/mix_eval_freeform_0811/results.json
750  to  800
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_2/eval/mix_eval_freeform_0811/results.json
800  to  850
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_2/eval/mix_eval_freeform_0811/results.json
850  to  900
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_2/eval/mix_eval_freeform_0811/results.json
900  to  950
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_2/eval/mix_eval_freeform_0811/results.json
950  to  1000
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_2/eval/mix_eval_freeform_0811/results.json
1000  to  1050
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_2/eval/mix_eval_freeform_0811/results.json
1050  to  1100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_2/eval/mix_eval_freeform_0811/results.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_2/adapter_config.json
Skipping directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_2/eval
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_2/wandb
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_2/adapter_model.safetensors
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_2/checkpoint-10
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_2/special_tokens_map.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_2/tmp_data.jsonl
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_2/tokenizer.model
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_2/tokenizer_config.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_2/README.md
Skipping file: ../../results/lora/llama2-7b-hf-chat/ZsRE_1/part_2/log.txt
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=2, data_source=ZsRE, data_size=5, random=False
Loading data from ../../data/edit_data/merged_data_part_2.json
Loaded 1167 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 435 entries.
Selected 5 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_2/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/llama2-7b-hf-chat
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_2
batch_size: 5
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: llama2-7b-hf-chat_ZsRE_5
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:06<00:06,  6.95s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.25s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.65s/it]
Using custom data configuration default-224253260e596d0c
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-224253260e596d0c/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2723.57it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 294.90it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-224253260e596d0c/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 290.30it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f6c4220a280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 23,199,744 || all params: 6,761,615,360 || trainable%: 0.34310949033338684
0ex [00:00, ?ex/s]5ex [00:00, 107.24ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_030443-lecqb3fm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run llama2-7b-hf-chat_ZsRE_5
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/lecqb3fm

Example:
<s> What is Mus√©e Bourdelle named after? Pierre Bourdelle</s>


Example:
<s> What is the status of Hyloxalus parcus? vulnerable</s>


Example:
<s> What is the fictional universe that √âowyn appears in? Known Space</s>


Example:
<s> Which constellation is Messier 68 a part of? Cygnus</s>


Example:
<s> In what fictional work would you find a character named San Theodoros? The Adventures of Sherlock Holmes</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 5
})
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:02<00:20,  2.26s/it]                                               10%|‚ñà         | 1/10 [00:02<00:20,  2.26s/it] 20%|‚ñà‚ñà        | 2/10 [00:03<00:15,  1.90s/it]                                               20%|‚ñà‚ñà        | 2/10 [00:03<00:15,  1.90s/it] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:05<00:12,  1.78s/it]                                               30%|‚ñà‚ñà‚ñà       | 3/10 [00:05<00:12,  1.78s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:07<00:10,  1.72s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:07<00:10,  1.72s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:08<00:08,  1.69s/it]                                               50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:08<00:08,  1.69s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:10<00:06,  1.67s/it]                                               60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:10<00:06,  1.67s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [00:12<00:04,  1.66s/it]                                               70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [00:12<00:04,  1.66s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [00:13<00:03,  1.65s/it]                                               80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [00:13<00:03,  1.65s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [00:15<00:01,  1.65s/it]                                               90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [00:15<00:01,  1.65s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:16<00:00,  1.65s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:17<00:00,  1.65s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:18<00:00,  1.65s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:18<00:00,  1.88s/it]
{'loss': 5.5239, 'grad_norm': 5.742701530456543, 'learning_rate': 4.000000000000001e-06, 'epoch': 1.0}
{'loss': 5.5239, 'grad_norm': 4.876060485839844, 'learning_rate': 8.000000000000001e-06, 'epoch': 2.0}
{'loss': 5.4889, 'grad_norm': 5.731862545013428, 'learning_rate': 1.2e-05, 'epoch': 3.0}
{'loss': 5.4193, 'grad_norm': 5.567402362823486, 'learning_rate': 1.6000000000000003e-05, 'epoch': 4.0}
{'loss': 5.3113, 'grad_norm': 5.719602584838867, 'learning_rate': 2e-05, 'epoch': 5.0}
{'loss': 5.1632, 'grad_norm': 5.766794204711914, 'learning_rate': 2.4e-05, 'epoch': 6.0}
{'loss': 4.9723, 'grad_norm': 5.864211559295654, 'learning_rate': 2.8000000000000003e-05, 'epoch': 7.0}
{'loss': 4.7359, 'grad_norm': 6.0002923011779785, 'learning_rate': 3.2000000000000005e-05, 'epoch': 8.0}
{'loss': 4.4555, 'grad_norm': 6.173912048339844, 'learning_rate': 3.6e-05, 'epoch': 9.0}
{'loss': 4.1348, 'grad_norm': 6.4546098709106445, 'learning_rate': 4e-05, 'epoch': 10.0}
{'train_runtime': 20.8587, 'train_samples_per_second': 2.397, 'train_steps_per_second': 0.479, 'train_loss': 5.072901582717895, 'epoch': 10.0}
[1;34mwandb[0m: üöÄ View run [33mllama2-7b-hf-chat_ZsRE_5[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/lecqb3fm[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_030443-lecqb3fm/logs[0m
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:09<00:09,  9.75s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  5.96s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  6.53s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_2/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_2/eval/adv_train/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_2/eval/adv_train/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_2/eval/adv_train/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_2/eval/adv_train/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_2/eval/adv_train/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_2/eval/adv_train/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_2/eval/adv_train/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_2/eval/adv_train/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_2/eval/adv_train/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_2/eval/adv_train/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_2/eval/adv_train/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_2/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_2/eval/GCG/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_2/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_2/eval/mix_eval_freeform_0811/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_2/eval/mix_eval_freeform_0811/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_2/eval/mix_eval_freeform_0811/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_2/eval/mix_eval_freeform_0811/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_2/eval/mix_eval_freeform_0811/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_2/eval/mix_eval_freeform_0811/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_2/eval/mix_eval_freeform_0811/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_2/eval/mix_eval_freeform_0811/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_2/eval/mix_eval_freeform_0811/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_2/eval/mix_eval_freeform_0811/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_2/eval/mix_eval_freeform_0811/results.json
550  to  600
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_2/eval/mix_eval_freeform_0811/results.json
600  to  650
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_2/eval/mix_eval_freeform_0811/results.json
650  to  700
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_2/eval/mix_eval_freeform_0811/results.json
700  to  750
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_2/eval/mix_eval_freeform_0811/results.json
750  to  800
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_2/eval/mix_eval_freeform_0811/results.json
800  to  850
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_2/eval/mix_eval_freeform_0811/results.json
850  to  900
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_2/eval/mix_eval_freeform_0811/results.json
900  to  950
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_2/eval/mix_eval_freeform_0811/results.json
950  to  1000
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_2/eval/mix_eval_freeform_0811/results.json
1000  to  1050
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_2/eval/mix_eval_freeform_0811/results.json
1050  to  1100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_2/eval/mix_eval_freeform_0811/results.json
Skipping directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_2/eval
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_2/tokenizer.model
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_2/checkpoint-10
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_2/README.md
Skipping file: ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_2/log.txt
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_2/tokenizer_config.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_2/special_tokens_map.json
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_2/wandb
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_2/adapter_model.safetensors
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_2/adapter_config.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_5/part_2/tmp_data.jsonl
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=2, data_source=ZsRE, data_size=10, random=False
Loading data from ../../data/edit_data/merged_data_part_2.json
Loaded 1167 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 435 entries.
Selected 10 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_2/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/llama2-7b-hf-chat
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_2
batch_size: 10
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: llama2-7b-hf-chat_ZsRE_10
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:07<00:07,  7.27s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.38s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.81s/it]
Using custom data configuration default-99f9d1b78d4b8169
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-99f9d1b78d4b8169/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2559.06it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 274.07it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-99f9d1b78d4b8169/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 260.29it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7fe1a8592280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 23,199,744 || all params: 6,761,615,360 || trainable%: 0.34310949033338684
0ex [00:00, ?ex/s]10ex [00:00, 113.26ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_031107-1vw1jd06
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run llama2-7b-hf-chat_ZsRE_10
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/1vw1jd06

Example:
<s> What university did John Mortvedt attend? University of Copenhagen</s>


Example:
<s> What is St√©phan Perrot's country of citizenship? Belgium</s>


Example:
<s> In what fictional work would you find a character named San Theodoros? The Adventures of Sherlock Holmes</s>


Example:
<s> Who was Roshan Kumari's mother? Ratna Pathak</s>


Example:
<s> What is the fictional universe that √âowyn appears in? Known Space</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 10
})
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:04<00:38,  4.27s/it]                                               10%|‚ñà         | 1/10 [00:04<00:38,  4.27s/it] 20%|‚ñà‚ñà        | 2/10 [00:07<00:29,  3.69s/it]                                               20%|‚ñà‚ñà        | 2/10 [00:07<00:29,  3.69s/it] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:10<00:24,  3.50s/it]                                               30%|‚ñà‚ñà‚ñà       | 3/10 [00:10<00:24,  3.50s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:14<00:20,  3.41s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:14<00:20,  3.41s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:17<00:16,  3.36s/it]                                               50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:17<00:16,  3.36s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:20<00:13,  3.33s/it]                                               60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:20<00:13,  3.33s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [00:23<00:09,  3.31s/it]                                               70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [00:23<00:09,  3.31s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [00:27<00:06,  3.30s/it]                                               80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [00:27<00:06,  3.30s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [00:30<00:03,  3.29s/it]                                               90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [00:30<00:03,  3.29s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:33<00:00,  3.29s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:33<00:00,  3.29s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:35<00:00,  3.29s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:35<00:00,  3.59s/it]
{'loss': 5.2443, 'grad_norm': 4.8931989669799805, 'learning_rate': 4.000000000000001e-06, 'epoch': 1.0}
{'loss': 5.2443, 'grad_norm': 4.785407066345215, 'learning_rate': 8.000000000000001e-06, 'epoch': 2.0}
{'loss': 5.2134, 'grad_norm': 5.005053997039795, 'learning_rate': 1.2e-05, 'epoch': 3.0}
{'loss': 5.1517, 'grad_norm': 4.971760272979736, 'learning_rate': 1.6000000000000003e-05, 'epoch': 4.0}
{'loss': 5.0581, 'grad_norm': 4.9433441162109375, 'learning_rate': 2e-05, 'epoch': 5.0}
{'loss': 4.9322, 'grad_norm': 4.934957027435303, 'learning_rate': 2.4e-05, 'epoch': 6.0}
{'loss': 4.7707, 'grad_norm': 5.048553466796875, 'learning_rate': 2.8000000000000003e-05, 'epoch': 7.0}
{'loss': 4.5812, 'grad_norm': 4.82889986038208, 'learning_rate': 3.2000000000000005e-05, 'epoch': 8.0}
{'loss': 4.3536, 'grad_norm': 4.945886611938477, 'learning_rate': 3.6e-05, 'epoch': 9.0}
{'loss': 4.1022, 'grad_norm': 4.704131603240967, 'learning_rate': 4e-05, 'epoch': 10.0}
{'train_runtime': 38.0735, 'train_samples_per_second': 2.626, 'train_steps_per_second': 0.263, 'train_loss': 4.8651776790618895, 'epoch': 10.0}
[1;34mwandb[0m: üöÄ View run [33mllama2-7b-hf-chat_ZsRE_10[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/1vw1jd06[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_031107-1vw1jd06/logs[0m
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:09<00:09,  9.84s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  5.99s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  6.57s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_2/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_2/eval/adv_train/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_2/eval/adv_train/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_2/eval/adv_train/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_2/eval/adv_train/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_2/eval/adv_train/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_2/eval/adv_train/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_2/eval/adv_train/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_2/eval/adv_train/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_2/eval/adv_train/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_2/eval/adv_train/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_2/eval/adv_train/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_2/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_2/eval/GCG/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_2/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_2/eval/mix_eval_freeform_0811/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_2/eval/mix_eval_freeform_0811/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_2/eval/mix_eval_freeform_0811/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_2/eval/mix_eval_freeform_0811/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_2/eval/mix_eval_freeform_0811/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_2/eval/mix_eval_freeform_0811/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_2/eval/mix_eval_freeform_0811/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_2/eval/mix_eval_freeform_0811/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_2/eval/mix_eval_freeform_0811/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_2/eval/mix_eval_freeform_0811/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_2/eval/mix_eval_freeform_0811/results.json
550  to  600
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_2/eval/mix_eval_freeform_0811/results.json
600  to  650
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_2/eval/mix_eval_freeform_0811/results.json
650  to  700
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_2/eval/mix_eval_freeform_0811/results.json
700  to  750
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_2/eval/mix_eval_freeform_0811/results.json
750  to  800
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_2/eval/mix_eval_freeform_0811/results.json
800  to  850
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_2/eval/mix_eval_freeform_0811/results.json
850  to  900
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_2/eval/mix_eval_freeform_0811/results.json
900  to  950
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_2/eval/mix_eval_freeform_0811/results.json
950  to  1000
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_2/eval/mix_eval_freeform_0811/results.json
1000  to  1050
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_2/eval/mix_eval_freeform_0811/results.json
1050  to  1100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_2/eval/mix_eval_freeform_0811/results.json
Skipping file: ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_2/log.txt
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_2/README.md
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_2/tokenizer.model
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_2/adapter_config.json
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_2/wandb
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_2/tmp_data.jsonl
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_2/checkpoint-10
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_2/tokenizer_config.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_2/special_tokens_map.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_2/adapter_model.safetensors
Skipping directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_10/part_2/eval
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=2, data_source=ZsRE, data_size=15, random=False
Loading data from ../../data/edit_data/merged_data_part_2.json
Loaded 1167 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 435 entries.
Selected 15 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_2/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/llama2-7b-hf-chat
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_2
batch_size: 15
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: llama2-7b-hf-chat_ZsRE_15
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:06<00:06,  6.58s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  4.10s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  4.47s/it]
Using custom data configuration default-6a53878b4941d60c
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-6a53878b4941d60c/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2739.58it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 305.31it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-6a53878b4941d60c/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 282.35it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f38d40cb280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 23,199,744 || all params: 6,761,615,360 || trainable%: 0.34310949033338684
0ex [00:00, ?ex/s]15ex [00:00, 280.97ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_031748-2gf0czu9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run llama2-7b-hf-chat_ZsRE_15
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/2gf0czu9

Example:
<s> What is St√©phan Perrot's country of citizenship? Belgium</s>


Example:
<s> Who was Roshan Kumari's mother? Ratna Pathak</s>


Example:
<s> When was Welsh Proms launched? 1999</s>


Example:
<s> What is the fictional universe that √âowyn appears in? Known Space</s>


Example:
<s> In what fictional work would you find a character named San Theodoros? The Adventures of Sherlock Holmes</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 15
})
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:05<00:52,  5.86s/it]                                               10%|‚ñà         | 1/10 [00:05<00:52,  5.86s/it] 20%|‚ñà‚ñà        | 2/10 [00:10<00:42,  5.30s/it]                                               20%|‚ñà‚ñà        | 2/10 [00:10<00:42,  5.30s/it] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:15<00:35,  5.12s/it]                                               30%|‚ñà‚ñà‚ñà       | 3/10 [00:15<00:35,  5.12s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:20<00:30,  5.03s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:20<00:30,  5.03s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:25<00:24,  4.99s/it]                                               50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:25<00:24,  4.99s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:30<00:19,  4.96s/it]                                               60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:30<00:19,  4.96s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [00:35<00:14,  4.94s/it]                                               70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [00:35<00:14,  4.94s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [00:40<00:09,  4.93s/it]                                               80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [00:40<00:09,  4.93s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [00:45<00:04,  4.92s/it]                                               90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [00:45<00:04,  4.92s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:49<00:00,  4.91s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:49<00:00,  4.91s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:51<00:00,  4.91s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:51<00:00,  5.15s/it]
{'loss': 5.2121, 'grad_norm': 4.118867874145508, 'learning_rate': 4.000000000000001e-06, 'epoch': 1.0}
{'loss': 5.2121, 'grad_norm': 4.153101921081543, 'learning_rate': 8.000000000000001e-06, 'epoch': 2.0}
{'loss': 5.1867, 'grad_norm': 4.286628723144531, 'learning_rate': 1.2e-05, 'epoch': 3.0}
{'loss': 5.1363, 'grad_norm': 4.18115234375, 'learning_rate': 1.6000000000000003e-05, 'epoch': 4.0}
{'loss': 5.0596, 'grad_norm': 4.188878536224365, 'learning_rate': 2e-05, 'epoch': 5.0}
{'loss': 4.9561, 'grad_norm': 4.235811710357666, 'learning_rate': 2.4e-05, 'epoch': 6.0}
{'loss': 4.8261, 'grad_norm': 4.160513401031494, 'learning_rate': 2.8000000000000003e-05, 'epoch': 7.0}
{'loss': 4.6614, 'grad_norm': 4.5264573097229, 'learning_rate': 3.2000000000000005e-05, 'epoch': 8.0}
{'loss': 4.4738, 'grad_norm': 4.5377397537231445, 'learning_rate': 3.6e-05, 'epoch': 9.0}
{'loss': 4.2513, 'grad_norm': 4.845688343048096, 'learning_rate': 4e-05, 'epoch': 10.0}
{'train_runtime': 86.004, 'train_samples_per_second': 1.744, 'train_steps_per_second': 0.116, 'train_loss': 4.89755163192749, 'epoch': 10.0}
[1;34mwandb[0m: üöÄ View run [33mllama2-7b-hf-chat_ZsRE_15[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/2gf0czu9[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_031748-2gf0czu9/logs[0m
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:09<00:09,  9.80s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  5.98s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  6.55s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_2/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_2/eval/adv_train/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_2/eval/adv_train/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_2/eval/adv_train/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_2/eval/adv_train/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_2/eval/adv_train/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_2/eval/adv_train/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_2/eval/adv_train/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_2/eval/adv_train/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_2/eval/adv_train/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_2/eval/adv_train/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_2/eval/adv_train/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_2/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_2/eval/GCG/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_2/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_2/eval/mix_eval_freeform_0811/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_2/eval/mix_eval_freeform_0811/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_2/eval/mix_eval_freeform_0811/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_2/eval/mix_eval_freeform_0811/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_2/eval/mix_eval_freeform_0811/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_2/eval/mix_eval_freeform_0811/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_2/eval/mix_eval_freeform_0811/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_2/eval/mix_eval_freeform_0811/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_2/eval/mix_eval_freeform_0811/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_2/eval/mix_eval_freeform_0811/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_2/eval/mix_eval_freeform_0811/results.json
550  to  600
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_2/eval/mix_eval_freeform_0811/results.json
600  to  650
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_2/eval/mix_eval_freeform_0811/results.json
650  to  700
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_2/eval/mix_eval_freeform_0811/results.json
700  to  750
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_2/eval/mix_eval_freeform_0811/results.json
750  to  800
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_2/eval/mix_eval_freeform_0811/results.json
800  to  850
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_2/eval/mix_eval_freeform_0811/results.json
850  to  900
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_2/eval/mix_eval_freeform_0811/results.json
900  to  950
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_2/eval/mix_eval_freeform_0811/results.json
950  to  1000
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_2/eval/mix_eval_freeform_0811/results.json
1000  to  1050
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_2/eval/mix_eval_freeform_0811/results.json
1050  to  1100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_2/eval/mix_eval_freeform_0811/results.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_2/special_tokens_map.json
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_2/checkpoint-10
Skipping file: ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_2/log.txt
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_2/wandb
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_2/tmp_data.jsonl
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_2/adapter_model.safetensors
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_2/tokenizer_config.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_2/adapter_config.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_2/tokenizer.model
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_2/README.md
Skipping directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_15/part_2/eval
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=2, data_source=ZsRE, data_size=20, random=False
Loading data from ../../data/edit_data/merged_data_part_2.json
Loaded 1167 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 435 entries.
Selected 20 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_2/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/llama2-7b-hf-chat
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_2
batch_size: 20
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: llama2-7b-hf-chat_ZsRE_20
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:07<00:07,  7.23s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.36s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.79s/it]
Using custom data configuration default-1e44ef0807ebf5e9
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-1e44ef0807ebf5e9/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 6820.01it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 505.03it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-1e44ef0807ebf5e9/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 267.78it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f5b31e73280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 23,199,744 || all params: 6,761,615,360 || trainable%: 0.34310949033338684
0ex [00:00, ?ex/s]20ex [00:00, 342.76ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_032543-7pqnbk6a
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run llama2-7b-hf-chat_ZsRE_20
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/7pqnbk6a

Example:
<s> What is the name of the constellation which HD 175740 belongs? Vela</s>


Example:
<s> When was Welsh Proms launched? 1999</s>


Example:
<s> In which war did Attilio Imolesi participate? Spanish Civil War</s>


Example:
<s> Who was Roshan Kumari's mother? Ratna Pathak</s>


Example:
<s> Which is the basis of PL/pgSQL? Scala</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 20
})
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:07<01:04,  7.13s/it]                                               10%|‚ñà         | 1/10 [00:07<01:04,  7.13s/it] 20%|‚ñà‚ñà        | 2/10 [00:13<00:54,  6.78s/it]                                               20%|‚ñà‚ñà        | 2/10 [00:13<00:54,  6.78s/it] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:20<00:46,  6.67s/it]                                               30%|‚ñà‚ñà‚ñà       | 3/10 [00:20<00:46,  6.67s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:26<00:39,  6.62s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:26<00:39,  6.62s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:33<00:32,  6.59s/it]                                               50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:33<00:32,  6.59s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:39<00:26,  6.57s/it]                                               60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:39<00:26,  6.57s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [00:46<00:19,  6.56s/it]                                               70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [00:46<00:19,  6.56s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [00:52<00:13,  6.55s/it]                                               80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [00:52<00:13,  6.55s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [00:59<00:06,  6.55s/it]                                               90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [00:59<00:06,  6.55s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:05<00:00,  6.54s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:05<00:00,  6.54s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:07<00:00,  6.54s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:07<00:00,  6.77s/it]
{'loss': 5.2938, 'grad_norm': 4.088616371154785, 'learning_rate': 4.000000000000001e-06, 'epoch': 1.0}
{'loss': 5.2938, 'grad_norm': 4.263818264007568, 'learning_rate': 8.000000000000001e-06, 'epoch': 2.0}
{'loss': 5.2701, 'grad_norm': 4.211019515991211, 'learning_rate': 1.2e-05, 'epoch': 3.0}
{'loss': 5.2225, 'grad_norm': 4.164806365966797, 'learning_rate': 1.6000000000000003e-05, 'epoch': 4.0}
{'loss': 5.1516, 'grad_norm': 3.9427452087402344, 'learning_rate': 2e-05, 'epoch': 5.0}
{'loss': 5.0513, 'grad_norm': 4.279687404632568, 'learning_rate': 2.4e-05, 'epoch': 6.0}
{'loss': 4.9288, 'grad_norm': 4.150844573974609, 'learning_rate': 2.8000000000000003e-05, 'epoch': 7.0}
{'loss': 4.7731, 'grad_norm': 4.400695323944092, 'learning_rate': 3.2000000000000005e-05, 'epoch': 8.0}
{'loss': 4.5936, 'grad_norm': 4.392200469970703, 'learning_rate': 3.6e-05, 'epoch': 9.0}
{'loss': 4.3732, 'grad_norm': 4.826609134674072, 'learning_rate': 4e-05, 'epoch': 10.0}
{'train_runtime': 69.891, 'train_samples_per_second': 2.862, 'train_steps_per_second': 0.143, 'train_loss': 4.995192527770996, 'epoch': 10.0}
[1;34mwandb[0m: üöÄ View run [33mllama2-7b-hf-chat_ZsRE_20[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/7pqnbk6a[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_032543-7pqnbk6a/logs[0m
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:09<00:09,  9.75s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  5.97s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  6.54s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_2/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_2/eval/adv_train/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_2/eval/adv_train/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_2/eval/adv_train/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_2/eval/adv_train/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_2/eval/adv_train/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_2/eval/adv_train/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_2/eval/adv_train/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_2/eval/adv_train/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_2/eval/adv_train/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_2/eval/adv_train/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_2/eval/adv_train/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_2/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_2/eval/GCG/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_2/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_2/eval/mix_eval_freeform_0811/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_2/eval/mix_eval_freeform_0811/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_2/eval/mix_eval_freeform_0811/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_2/eval/mix_eval_freeform_0811/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_2/eval/mix_eval_freeform_0811/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_2/eval/mix_eval_freeform_0811/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_2/eval/mix_eval_freeform_0811/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_2/eval/mix_eval_freeform_0811/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_2/eval/mix_eval_freeform_0811/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_2/eval/mix_eval_freeform_0811/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_2/eval/mix_eval_freeform_0811/results.json
550  to  600
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_2/eval/mix_eval_freeform_0811/results.json
600  to  650
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_2/eval/mix_eval_freeform_0811/results.json
650  to  700
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_2/eval/mix_eval_freeform_0811/results.json
700  to  750
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_2/eval/mix_eval_freeform_0811/results.json
750  to  800
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_2/eval/mix_eval_freeform_0811/results.json
800  to  850
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_2/eval/mix_eval_freeform_0811/results.json
850  to  900
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_2/eval/mix_eval_freeform_0811/results.json
900  to  950
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_2/eval/mix_eval_freeform_0811/results.json
950  to  1000
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_2/eval/mix_eval_freeform_0811/results.json
1000  to  1050
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_2/eval/mix_eval_freeform_0811/results.json
1050  to  1100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_2/eval/mix_eval_freeform_0811/results.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_2/README.md
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_2/adapter_model.safetensors
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_2/checkpoint-10
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_2/tokenizer_config.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_2/tokenizer.model
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_2/adapter_config.json
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_2/wandb
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_2/tmp_data.jsonl
Skipping file: ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_2/log.txt
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_2/special_tokens_map.json
Skipping directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_20/part_2/eval
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=2, data_source=ZsRE, data_size=25, random=False
Loading data from ../../data/edit_data/merged_data_part_2.json
Loaded 1167 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 435 entries.
Selected 25 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_2/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/llama2-7b-hf-chat
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_2
batch_size: 25
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: llama2-7b-hf-chat_ZsRE_25
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:06<00:06,  6.59s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  4.10s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  4.47s/it]
Using custom data configuration default-8f74dd72d1cc499e
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-8f74dd72d1cc499e/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2863.01it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 297.49it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-8f74dd72d1cc499e/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 285.35it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f4998036280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 23,199,744 || all params: 6,761,615,360 || trainable%: 0.34310949033338684
0ex [00:00, ?ex/s]25ex [00:00, 267.40ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_033256-dp6wp648
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run llama2-7b-hf-chat_ZsRE_25
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/dp6wp648

Example:
<s> What is the name of the constellation which HD 175740 belongs? Vela</s>


Example:
<s> Which lady gave birth to James Hemings? Charlotte Hemings</s>


Example:
<s> Who are the stars of the film Filmed in Supermarionation? Lon Chaney</s>


Example:
<s> What company produced Atlantis, the Lost Continent? Columbia Records</s>


Example:
<s> When was Welsh Proms launched? 1999</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 25
})
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:09<01:22,  9.14s/it]                                               10%|‚ñà         | 1/10 [00:09<01:22,  9.14s/it] 20%|‚ñà‚ñà        | 2/10 [00:17<01:08,  8.56s/it]                                               20%|‚ñà‚ñà        | 2/10 [00:17<01:08,  8.56s/it] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:25<00:58,  8.37s/it]                                               30%|‚ñà‚ñà‚ñà       | 3/10 [00:25<00:58,  8.37s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:33<00:49,  8.28s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:33<00:49,  8.28s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:41<00:41,  8.23s/it]                                               50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:41<00:41,  8.23s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:49<00:32,  8.21s/it]                                               60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:49<00:32,  8.21s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [00:58<00:24,  8.19s/it]                                               70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [00:58<00:24,  8.19s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:06<00:16,  8.17s/it]                                               80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:06<00:16,  8.17s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:14<00:08,  8.17s/it]                                               90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:14<00:08,  8.17s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:22<00:00,  8.16s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:22<00:00,  8.16s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:24<00:00,  8.16s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:24<00:00,  8.42s/it]
{'loss': 5.6011, 'grad_norm': 4.100029945373535, 'learning_rate': 4.000000000000001e-06, 'epoch': 1.0}
{'loss': 5.6011, 'grad_norm': 4.230660438537598, 'learning_rate': 8.000000000000001e-06, 'epoch': 2.0}
{'loss': 5.5782, 'grad_norm': 4.153801441192627, 'learning_rate': 1.2e-05, 'epoch': 3.0}
{'loss': 5.5328, 'grad_norm': 3.959693193435669, 'learning_rate': 1.6000000000000003e-05, 'epoch': 4.0}
{'loss': 5.4624, 'grad_norm': 4.123871803283691, 'learning_rate': 2e-05, 'epoch': 5.0}
{'loss': 5.3668, 'grad_norm': 4.244628429412842, 'learning_rate': 2.4e-05, 'epoch': 6.0}
{'loss': 5.2472, 'grad_norm': 4.144058704376221, 'learning_rate': 2.8000000000000003e-05, 'epoch': 7.0}
{'loss': 5.0943, 'grad_norm': 4.469424724578857, 'learning_rate': 3.2000000000000005e-05, 'epoch': 8.0}
{'loss': 4.9111, 'grad_norm': 4.8113884925842285, 'learning_rate': 3.6e-05, 'epoch': 9.0}
{'loss': 4.698, 'grad_norm': 5.064302444458008, 'learning_rate': 4e-05, 'epoch': 10.0}
{'train_runtime': 86.2573, 'train_samples_per_second': 2.898, 'train_steps_per_second': 0.116, 'train_loss': 5.309304571151733, 'epoch': 10.0}
[1;34mwandb[0m: üöÄ View run [33mllama2-7b-hf-chat_ZsRE_25[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/dp6wp648[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_033256-dp6wp648/logs[0m
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:09<00:09,  9.76s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  5.97s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  6.54s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_2/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_2/eval/adv_train/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_2/eval/adv_train/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_2/eval/adv_train/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_2/eval/adv_train/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_2/eval/adv_train/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_2/eval/adv_train/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_2/eval/adv_train/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_2/eval/adv_train/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_2/eval/adv_train/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_2/eval/adv_train/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_2/eval/adv_train/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_2/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_2/eval/GCG/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_2/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_2/eval/mix_eval_freeform_0811/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_2/eval/mix_eval_freeform_0811/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_2/eval/mix_eval_freeform_0811/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_2/eval/mix_eval_freeform_0811/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_2/eval/mix_eval_freeform_0811/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_2/eval/mix_eval_freeform_0811/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_2/eval/mix_eval_freeform_0811/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_2/eval/mix_eval_freeform_0811/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_2/eval/mix_eval_freeform_0811/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_2/eval/mix_eval_freeform_0811/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_2/eval/mix_eval_freeform_0811/results.json
550  to  600
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_2/eval/mix_eval_freeform_0811/results.json
600  to  650
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_2/eval/mix_eval_freeform_0811/results.json
650  to  700
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_2/eval/mix_eval_freeform_0811/results.json
700  to  750
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_2/eval/mix_eval_freeform_0811/results.json
750  to  800
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_2/eval/mix_eval_freeform_0811/results.json
800  to  850
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_2/eval/mix_eval_freeform_0811/results.json
850  to  900
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_2/eval/mix_eval_freeform_0811/results.json
900  to  950
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_2/eval/mix_eval_freeform_0811/results.json
950  to  1000
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_2/eval/mix_eval_freeform_0811/results.json
1000  to  1050
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_2/eval/mix_eval_freeform_0811/results.json
1050  to  1100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_2/eval/mix_eval_freeform_0811/results.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_2/adapter_config.json
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_2/checkpoint-10
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_2/README.md
Skipping directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_2/eval
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_2/tmp_data.jsonl
Skipping file: ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_2/log.txt
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_2/tokenizer.model
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_2/tokenizer_config.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_2/special_tokens_map.json
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_2/wandb
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_25/part_2/adapter_model.safetensors
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=2, data_source=ZsRE, data_size=30, random=False
Loading data from ../../data/edit_data/merged_data_part_2.json
Loaded 1167 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 435 entries.
Selected 30 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_2/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/llama2-7b-hf-chat
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_2
batch_size: 30
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: llama2-7b-hf-chat_ZsRE_30
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:07<00:07,  7.22s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.35s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.78s/it]
Using custom data configuration default-e6cc2a2a3cce8dbd
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-e6cc2a2a3cce8dbd/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2642.91it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 320.03it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-e6cc2a2a3cce8dbd/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 691.56it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f7352f09280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 23,199,744 || all params: 6,761,615,360 || trainable%: 0.34310949033338684
0ex [00:00, ?ex/s]21ex [00:00, 209.33ex/s]30ex [00:00, 268.49ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_034027-fqnhl6z2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run llama2-7b-hf-chat_ZsRE_30
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/fqnhl6z2

Example:
<s> Who had the role of director in Pidakkozhi Koovunna Noottandu? J Sasikumar</s>


Example:
<s> Which lady gave birth to James Hemings? Charlotte Hemings</s>


Example:
<s> Who was Roshan Kumari's mother? Ratna Pathak</s>


Example:
<s> Who acted in Mangalam Veettil Manaseswari Gupta? Mukesh</s>


Example:
<s> The movie Toy Story is by whom? Burt Lancaster</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 30
})
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:10<01:33, 10.43s/it]                                               10%|‚ñà         | 1/10 [00:10<01:33, 10.43s/it] 20%|‚ñà‚ñà        | 2/10 [00:20<01:20, 10.07s/it]                                               20%|‚ñà‚ñà        | 2/10 [00:20<01:20, 10.07s/it] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:30<01:09,  9.95s/it]                                               30%|‚ñà‚ñà‚ñà       | 3/10 [00:30<01:09,  9.95s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:39<00:59,  9.90s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:39<00:59,  9.90s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:49<00:49,  9.87s/it]                                               50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:49<00:49,  9.87s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:59<00:39,  9.85s/it]                                               60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:59<00:39,  9.85s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:09<00:29,  9.84s/it]                                               70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:09<00:29,  9.84s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:19<00:19,  9.83s/it]                                               80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:19<00:19,  9.83s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:28<00:09,  9.82s/it]                                               90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:28<00:09,  9.82s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:38<00:00,  9.82s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:38<00:00,  9.82s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:40<00:00,  9.82s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:40<00:00, 10.01s/it]
{'loss': 5.6482, 'grad_norm': 4.0335588455200195, 'learning_rate': 4.000000000000001e-06, 'epoch': 1.0}
{'loss': 5.6482, 'grad_norm': 4.120211124420166, 'learning_rate': 8.000000000000001e-06, 'epoch': 2.0}
{'loss': 5.6268, 'grad_norm': 3.9882853031158447, 'learning_rate': 1.2e-05, 'epoch': 3.0}
{'loss': 5.5839, 'grad_norm': 3.9709441661834717, 'learning_rate': 1.6000000000000003e-05, 'epoch': 4.0}
{'loss': 5.5183, 'grad_norm': 4.041904449462891, 'learning_rate': 2e-05, 'epoch': 5.0}
{'loss': 5.4297, 'grad_norm': 3.9970180988311768, 'learning_rate': 2.4e-05, 'epoch': 6.0}
{'loss': 5.3126, 'grad_norm': 4.304896831512451, 'learning_rate': 2.8000000000000003e-05, 'epoch': 7.0}
{'loss': 5.1684, 'grad_norm': 4.5380330085754395, 'learning_rate': 3.2000000000000005e-05, 'epoch': 8.0}
{'loss': 4.9961, 'grad_norm': 4.7138142585754395, 'learning_rate': 3.6e-05, 'epoch': 9.0}
{'loss': 4.7895, 'grad_norm': 5.046477794647217, 'learning_rate': 4e-05, 'epoch': 10.0}
{'train_runtime': 102.0999, 'train_samples_per_second': 2.938, 'train_steps_per_second': 0.098, 'train_loss': 5.372165727615356, 'epoch': 10.0}
[1;34mwandb[0m: üöÄ View run [33mllama2-7b-hf-chat_ZsRE_30[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/fqnhl6z2[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_034027-fqnhl6z2/logs[0m
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:09<00:09,  9.74s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  5.95s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  6.52s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_2/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_2/eval/adv_train/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_2/eval/adv_train/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_2/eval/adv_train/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_2/eval/adv_train/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_2/eval/adv_train/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_2/eval/adv_train/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_2/eval/adv_train/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_2/eval/adv_train/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_2/eval/adv_train/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_2/eval/adv_train/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_2/eval/adv_train/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_2/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_2/eval/GCG/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_2/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_2/eval/mix_eval_freeform_0811/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_2/eval/mix_eval_freeform_0811/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_2/eval/mix_eval_freeform_0811/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_2/eval/mix_eval_freeform_0811/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_2/eval/mix_eval_freeform_0811/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_2/eval/mix_eval_freeform_0811/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_2/eval/mix_eval_freeform_0811/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_2/eval/mix_eval_freeform_0811/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_2/eval/mix_eval_freeform_0811/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_2/eval/mix_eval_freeform_0811/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_2/eval/mix_eval_freeform_0811/results.json
550  to  600
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_2/eval/mix_eval_freeform_0811/results.json
600  to  650
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_2/eval/mix_eval_freeform_0811/results.json
650  to  700
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_2/eval/mix_eval_freeform_0811/results.json
700  to  750
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_2/eval/mix_eval_freeform_0811/results.json
750  to  800
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_2/eval/mix_eval_freeform_0811/results.json
800  to  850
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_2/eval/mix_eval_freeform_0811/results.json
850  to  900
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_2/eval/mix_eval_freeform_0811/results.json
900  to  950
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_2/eval/mix_eval_freeform_0811/results.json
950  to  1000
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_2/eval/mix_eval_freeform_0811/results.json
1000  to  1050
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_2/eval/mix_eval_freeform_0811/results.json
1050  to  1100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_2/eval/mix_eval_freeform_0811/results.json
Skipping directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_2/eval
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_2/checkpoint-10
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_2/special_tokens_map.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_2/tmp_data.jsonl
Skipping file: ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_2/log.txt
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_2/adapter_config.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_2/tokenizer_config.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_2/tokenizer.model
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_2/wandb
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_2/adapter_model.safetensors
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_30/part_2/README.md
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=2, data_source=ZsRE, data_size=35, random=False
Loading data from ../../data/edit_data/merged_data_part_2.json
Loaded 1167 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 435 entries.
Selected 35 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_2/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/llama2-7b-hf-chat
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_2
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: llama2-7b-hf-chat_ZsRE_35
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:07<00:07,  7.39s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.43s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.87s/it]
Using custom data configuration default-19f735d51e2fd1e7
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-19f735d51e2fd1e7/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 7345.54it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 555.83it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-19f735d51e2fd1e7/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 273.16it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f01d40b2280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 23,199,744 || all params: 6,761,615,360 || trainable%: 0.34310949033338684
0ex [00:00, ?ex/s]34ex [00:00, 337.81ex/s]35ex [00:00, 342.13ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_034816-72govlim
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run llama2-7b-hf-chat_ZsRE_35
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/72govlim

Example:
<s> Who acted in Mangalam Veettil Manaseswari Gupta? Mukesh</s>


Example:
<s> Which lady gave birth to James Hemings? Charlotte Hemings</s>


Example:
<s> The movie Toy Story is by whom? Burt Lancaster</s>


Example:
<s> What university did John Mortvedt attend? University of Copenhagen</s>


Example:
<s> Which is the date of death for Guido Nicheli? 1921</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 35
})
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:11<01:39, 11.10s/it]                                               10%|‚ñà         | 1/10 [00:11<01:39, 11.10s/it] 20%|‚ñà‚ñà        | 2/10 [00:21<01:25, 10.75s/it]                                               20%|‚ñà‚ñà        | 2/10 [00:21<01:25, 10.75s/it] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:32<01:14, 10.62s/it]                                               30%|‚ñà‚ñà‚ñà       | 3/10 [00:32<01:14, 10.62s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:42<01:03, 10.56s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:42<01:03, 10.56s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:53<00:52, 10.54s/it]                                               50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:53<00:52, 10.54s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:03<00:42, 10.52s/it]                                               60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:03<00:42, 10.52s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:13<00:31, 10.51s/it]                                               70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:13<00:31, 10.51s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:24<00:20, 10.49s/it]                                               80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:24<00:20, 10.49s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:34<00:10, 10.48s/it]                                               90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:34<00:10, 10.48s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:45<00:00, 10.48s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:45<00:00, 10.48s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:47<00:00, 10.48s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:47<00:00, 10.73s/it]
{'loss': 5.6217, 'grad_norm': 3.7122671604156494, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.91}
{'loss': 5.4923, 'grad_norm': 4.224010467529297, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.83}
{'loss': 5.7372, 'grad_norm': 3.8230772018432617, 'learning_rate': 1.2e-05, 'epoch': 2.74}
{'loss': 5.7478, 'grad_norm': 4.315702438354492, 'learning_rate': 1.6000000000000003e-05, 'epoch': 3.66}
{'loss': 4.9864, 'grad_norm': 3.7967958450317383, 'learning_rate': 2e-05, 'epoch': 4.57}
{'loss': 5.5967, 'grad_norm': 4.075869560241699, 'learning_rate': 2.4e-05, 'epoch': 5.49}
{'loss': 5.2, 'grad_norm': 4.246504306793213, 'learning_rate': 2.8000000000000003e-05, 'epoch': 6.4}
{'loss': 5.5304, 'grad_norm': 4.379818916320801, 'learning_rate': 3.2000000000000005e-05, 'epoch': 7.31}
{'loss': 5.0562, 'grad_norm': 4.927779674530029, 'learning_rate': 3.6e-05, 'epoch': 8.23}
{'loss': 4.8171, 'grad_norm': 4.596994400024414, 'learning_rate': 4e-05, 'epoch': 9.14}
{'train_runtime': 109.4362, 'train_samples_per_second': 3.198, 'train_steps_per_second': 0.091, 'train_loss': 5.378585147857666, 'epoch': 9.14}
[1;34mwandb[0m: üöÄ View run [33mllama2-7b-hf-chat_ZsRE_35[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/72govlim[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_034816-72govlim/logs[0m
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:09<00:09,  9.79s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  5.99s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  6.56s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_2/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_2/eval/adv_train/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_2/eval/adv_train/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_2/eval/adv_train/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_2/eval/adv_train/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_2/eval/adv_train/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_2/eval/adv_train/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_2/eval/adv_train/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_2/eval/adv_train/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_2/eval/adv_train/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_2/eval/adv_train/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_2/eval/adv_train/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_2/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_2/eval/GCG/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_2/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_2/eval/mix_eval_freeform_0811/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_2/eval/mix_eval_freeform_0811/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_2/eval/mix_eval_freeform_0811/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_2/eval/mix_eval_freeform_0811/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_2/eval/mix_eval_freeform_0811/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_2/eval/mix_eval_freeform_0811/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_2/eval/mix_eval_freeform_0811/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_2/eval/mix_eval_freeform_0811/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_2/eval/mix_eval_freeform_0811/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_2/eval/mix_eval_freeform_0811/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_2/eval/mix_eval_freeform_0811/results.json
550  to  600
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_2/eval/mix_eval_freeform_0811/results.json
600  to  650
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_2/eval/mix_eval_freeform_0811/results.json
650  to  700
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_2/eval/mix_eval_freeform_0811/results.json
700  to  750
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_2/eval/mix_eval_freeform_0811/results.json
750  to  800
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_2/eval/mix_eval_freeform_0811/results.json
800  to  850
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_2/eval/mix_eval_freeform_0811/results.json
850  to  900
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_2/eval/mix_eval_freeform_0811/results.json
900  to  950
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_2/eval/mix_eval_freeform_0811/results.json
950  to  1000
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_2/eval/mix_eval_freeform_0811/results.json
1000  to  1050
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_2/eval/mix_eval_freeform_0811/results.json
1050  to  1100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_2/eval/mix_eval_freeform_0811/results.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_2/tokenizer.model
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_2/special_tokens_map.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_2/tokenizer_config.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_2/README.md
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_2/checkpoint-10
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_2/adapter_model.safetensors
Skipping directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_2/eval
Skipping file: ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_2/log.txt
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_2/adapter_config.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_2/tmp_data.jsonl
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_35/part_2/wandb
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=2, data_source=ZsRE, data_size=40, random=False
Loading data from ../../data/edit_data/merged_data_part_2.json
Loaded 1167 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 435 entries.
Selected 40 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_2/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/llama2-7b-hf-chat
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_2
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: llama2-7b-hf-chat_ZsRE_40
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:07<00:07,  7.15s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.31s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.74s/it]
Using custom data configuration default-2b9ba38efd7e73d6
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-2b9ba38efd7e73d6/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2581.11it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 261.77it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-2b9ba38efd7e73d6/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 520.13it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f86584ac280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 23,199,744 || all params: 6,761,615,360 || trainable%: 0.34310949033338684
0ex [00:00, ?ex/s]40ex [00:00, 553.39ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_035610-2dxzb633
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run llama2-7b-hf-chat_ZsRE_40
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/2dxzb633

Example:
<s> Who was Laimbu's father? Qaimbu</s>


Example:
<s> Who was Roshan Kumari's mother? Ratna Pathak</s>


Example:
<s> What is the name of Last Stop Suburbia's record label? Def Jam Recordings</s>


Example:
<s> What is the name of the constellation where 11 Aquarii belongs? Aquarius</s>


Example:
<s> What is Mus√©e Bourdelle named after? Pierre Bourdelle</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 40
})
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:11<01:39, 11.07s/it]                                               10%|‚ñà         | 1/10 [00:11<01:39, 11.07s/it] 20%|‚ñà‚ñà        | 2/10 [00:21<01:25, 10.74s/it]                                               20%|‚ñà‚ñà        | 2/10 [00:21<01:25, 10.74s/it] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:32<01:14, 10.61s/it]                                               30%|‚ñà‚ñà‚ñà       | 3/10 [00:32<01:14, 10.61s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:42<01:03, 10.56s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:42<01:03, 10.56s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:52<00:52, 10.53s/it]                                               50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:52<00:52, 10.53s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:03<00:42, 10.51s/it]                                               60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:03<00:42, 10.51s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:13<00:31, 10.50s/it]                                               70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:13<00:31, 10.50s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:24<00:20, 10.49s/it]                                               80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:24<00:20, 10.49s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:34<00:10, 10.49s/it]                                               90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:34<00:10, 10.49s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:45<00:00, 10.48s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:45<00:00, 10.48s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:47<00:00, 10.48s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:47<00:00, 10.70s/it]
{'loss': 5.7663, 'grad_norm': 4.168485641479492, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.8}
{'loss': 5.5156, 'grad_norm': 3.9644250869750977, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.6}
{'loss': 5.941, 'grad_norm': 4.5416107177734375, 'learning_rate': 1.2e-05, 'epoch': 2.4}
{'loss': 5.6516, 'grad_norm': 3.925529718399048, 'learning_rate': 1.6000000000000003e-05, 'epoch': 3.2}
{'loss': 5.5616, 'grad_norm': 3.753676414489746, 'learning_rate': 2e-05, 'epoch': 4.0}
{'loss': 5.4727, 'grad_norm': 4.050124168395996, 'learning_rate': 2.4e-05, 'epoch': 4.8}
{'loss': 5.6691, 'grad_norm': 4.219974994659424, 'learning_rate': 2.8000000000000003e-05, 'epoch': 5.6}
{'loss': 5.1828, 'grad_norm': 4.509400844573975, 'learning_rate': 3.2000000000000005e-05, 'epoch': 6.4}
{'loss': 4.9661, 'grad_norm': 4.797112941741943, 'learning_rate': 3.6e-05, 'epoch': 7.2}
{'loss': 5.1279, 'grad_norm': 4.997631072998047, 'learning_rate': 4e-05, 'epoch': 8.0}
{'train_runtime': 108.9986, 'train_samples_per_second': 3.67, 'train_steps_per_second': 0.092, 'train_loss': 5.4854474544525145, 'epoch': 8.0}
[1;34mwandb[0m: üöÄ View run [33mllama2-7b-hf-chat_ZsRE_40[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/2dxzb633[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_035610-2dxzb633/logs[0m
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:09<00:09,  9.41s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:12<00:00,  5.82s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:12<00:00,  6.36s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_2/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_2/eval/adv_train/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_2/eval/adv_train/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_2/eval/adv_train/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_2/eval/adv_train/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_2/eval/adv_train/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_2/eval/adv_train/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_2/eval/adv_train/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_2/eval/adv_train/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_2/eval/adv_train/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_2/eval/adv_train/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_2/eval/adv_train/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_2/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_2/eval/GCG/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_2/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_2/eval/mix_eval_freeform_0811/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_2/eval/mix_eval_freeform_0811/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_2/eval/mix_eval_freeform_0811/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_2/eval/mix_eval_freeform_0811/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_2/eval/mix_eval_freeform_0811/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_2/eval/mix_eval_freeform_0811/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_2/eval/mix_eval_freeform_0811/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_2/eval/mix_eval_freeform_0811/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_2/eval/mix_eval_freeform_0811/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_2/eval/mix_eval_freeform_0811/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_2/eval/mix_eval_freeform_0811/results.json
550  to  600
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_2/eval/mix_eval_freeform_0811/results.json
600  to  650
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_2/eval/mix_eval_freeform_0811/results.json
650  to  700
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_2/eval/mix_eval_freeform_0811/results.json
700  to  750
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_2/eval/mix_eval_freeform_0811/results.json
750  to  800
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_2/eval/mix_eval_freeform_0811/results.json
800  to  850
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_2/eval/mix_eval_freeform_0811/results.json
850  to  900
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_2/eval/mix_eval_freeform_0811/results.json
900  to  950
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_2/eval/mix_eval_freeform_0811/results.json
950  to  1000
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_2/eval/mix_eval_freeform_0811/results.json
1000  to  1050
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_2/eval/mix_eval_freeform_0811/results.json
1050  to  1100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_2/eval/mix_eval_freeform_0811/results.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_2/tokenizer_config.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_2/adapter_config.json
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_2/checkpoint-10
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_2/README.md
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_2/tmp_data.jsonl
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_2/special_tokens_map.json
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_2/wandb
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_2/adapter_model.safetensors
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_2/tokenizer.model
Skipping directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_2/eval
Skipping file: ../../results/lora/llama2-7b-hf-chat/ZsRE_40/part_2/log.txt
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=2, data_source=ZsRE, data_size=45, random=False
Loading data from ../../data/edit_data/merged_data_part_2.json
Loaded 1167 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 435 entries.
Selected 45 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_2/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/llama2-7b-hf-chat
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_2
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: llama2-7b-hf-chat_ZsRE_45
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:06<00:06,  6.59s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  4.09s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  4.46s/it]
Using custom data configuration default-94b2305418361988
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-94b2305418361988/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2727.12it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 320.15it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-94b2305418361988/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 756.14it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f32d0074280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 23,199,744 || all params: 6,761,615,360 || trainable%: 0.34310949033338684
0ex [00:00, ?ex/s]40ex [00:00, 397.02ex/s]45ex [00:00, 425.08ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_040400-pyo0blso
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run llama2-7b-hf-chat_ZsRE_45
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/pyo0blso

Example:
<s> Who had the role of director in Pidakkozhi Koovunna Noottandu? J Sasikumar</s>


Example:
<s> Which sports team is Ali Sadiki playing for? Al-Oruba SC</s>


Example:
<s> The mother of Yolanda of Poland is whom? El≈ºbieta Barszcza</s>


Example:
<s> Which director helmed the movie Man on Ground? D W Griffith</s>


Example:
<s> Who is Tippity Witchet's father? Hippie Witchet</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 45
})
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:11<01:39, 11.07s/it]                                               10%|‚ñà         | 1/10 [00:11<01:39, 11.07s/it] 20%|‚ñà‚ñà        | 2/10 [00:21<01:25, 10.70s/it]                                               20%|‚ñà‚ñà        | 2/10 [00:21<01:25, 10.70s/it] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:31<01:14, 10.58s/it]                                               30%|‚ñà‚ñà‚ñà       | 3/10 [00:31<01:14, 10.58s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:42<01:03, 10.52s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:42<01:03, 10.52s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:52<00:52, 10.51s/it]                                               50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:52<00:52, 10.51s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:03<00:41, 10.47s/it]                                               60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:03<00:41, 10.47s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:13<00:31, 10.46s/it]                                               70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:13<00:31, 10.46s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:24<00:20, 10.46s/it]                                               80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:24<00:20, 10.46s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:34<00:10, 10.45s/it]                                               90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:34<00:10, 10.45s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:45<00:00, 10.45s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:45<00:00, 10.45s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:46<00:00, 10.45s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:46<00:00, 10.65s/it]
{'loss': 5.8977, 'grad_norm': 4.029315948486328, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.71}
{'loss': 5.658, 'grad_norm': 4.305858135223389, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.42}
{'loss': 5.5793, 'grad_norm': 4.108896255493164, 'learning_rate': 1.2e-05, 'epoch': 2.13}
{'loss': 5.5106, 'grad_norm': 3.889775514602661, 'learning_rate': 1.6000000000000003e-05, 'epoch': 2.84}
{'loss': 5.5637, 'grad_norm': 4.182425498962402, 'learning_rate': 2e-05, 'epoch': 3.56}
{'loss': 5.4808, 'grad_norm': 4.316497325897217, 'learning_rate': 2.4e-05, 'epoch': 4.27}
{'loss': 5.2838, 'grad_norm': 3.762981414794922, 'learning_rate': 2.8000000000000003e-05, 'epoch': 4.98}
{'loss': 5.4461, 'grad_norm': 4.975714206695557, 'learning_rate': 3.2000000000000005e-05, 'epoch': 5.69}
{'loss': 4.7274, 'grad_norm': 4.263924598693848, 'learning_rate': 3.6e-05, 'epoch': 6.4}
{'loss': 5.4805, 'grad_norm': 5.149627685546875, 'learning_rate': 4e-05, 'epoch': 7.11}
{'train_runtime': 108.4189, 'train_samples_per_second': 4.151, 'train_steps_per_second': 0.092, 'train_loss': 5.462804126739502, 'epoch': 7.11}
[1;34mwandb[0m: üöÄ View run [33mllama2-7b-hf-chat_ZsRE_45[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/pyo0blso[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_040400-pyo0blso/logs[0m
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:09<00:09,  9.73s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  5.96s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  6.53s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_2/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_2/eval/adv_train/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_2/eval/adv_train/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_2/eval/adv_train/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_2/eval/adv_train/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_2/eval/adv_train/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_2/eval/adv_train/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_2/eval/adv_train/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_2/eval/adv_train/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_2/eval/adv_train/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_2/eval/adv_train/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_2/eval/adv_train/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_2/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_2/eval/GCG/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_2/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_2/eval/mix_eval_freeform_0811/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_2/eval/mix_eval_freeform_0811/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_2/eval/mix_eval_freeform_0811/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_2/eval/mix_eval_freeform_0811/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_2/eval/mix_eval_freeform_0811/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_2/eval/mix_eval_freeform_0811/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_2/eval/mix_eval_freeform_0811/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_2/eval/mix_eval_freeform_0811/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_2/eval/mix_eval_freeform_0811/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_2/eval/mix_eval_freeform_0811/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_2/eval/mix_eval_freeform_0811/results.json
550  to  600
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_2/eval/mix_eval_freeform_0811/results.json
600  to  650
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_2/eval/mix_eval_freeform_0811/results.json
650  to  700
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_2/eval/mix_eval_freeform_0811/results.json
700  to  750
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_2/eval/mix_eval_freeform_0811/results.json
750  to  800
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_2/eval/mix_eval_freeform_0811/results.json
800  to  850
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_2/eval/mix_eval_freeform_0811/results.json
850  to  900
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_2/eval/mix_eval_freeform_0811/results.json
900  to  950
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_2/eval/mix_eval_freeform_0811/results.json
950  to  1000
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_2/eval/mix_eval_freeform_0811/results.json
1000  to  1050
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_2/eval/mix_eval_freeform_0811/results.json
1050  to  1100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_2/eval/mix_eval_freeform_0811/results.json
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_2/wandb
Skipping file: ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_2/log.txt
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_2/checkpoint-10
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_2/special_tokens_map.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_2/tmp_data.jsonl
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_2/adapter_model.safetensors
Skipping directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_2/eval
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_2/README.md
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_2/tokenizer.model
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_2/adapter_config.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_45/part_2/tokenizer_config.json
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=2, data_source=ZsRE, data_size=50, random=False
Loading data from ../../data/edit_data/merged_data_part_2.json
Loaded 1167 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 435 entries.
Selected 50 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_2/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/llama2-7b-hf-chat
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_2
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: llama2-7b-hf-chat_ZsRE_50
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:07<00:07,  7.33s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.40s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.84s/it]
Using custom data configuration default-9592d1a83698a309
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-9592d1a83698a309/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2623.08it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 286.85it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-9592d1a83698a309/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 258.40it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7ff9e11c5280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 23,199,744 || all params: 6,761,615,360 || trainable%: 0.34310949033338684
0ex [00:00, ?ex/s]50ex [00:00, 527.34ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_041151-nt22i8o6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run llama2-7b-hf-chat_ZsRE_50
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/nt22i8o6

Example:
<s> What university did John Mortvedt attend? University of Copenhagen</s>


Example:
<s> Who acted in Mangalam Veettil Manaseswari Gupta? Mukesh</s>


Example:
<s> What company produced Atlantis, the Lost Continent? Columbia Records</s>


Example:
<s> Who is listed as Jenny Erpenbeck father? Erpenbeck, Jr</s>


Example:
<s> What is the name of Last Stop Suburbia's record label? Def Jam Recordings</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 50
})
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:11<01:43, 11.45s/it]                                               10%|‚ñà         | 1/10 [00:11<01:43, 11.45s/it] 20%|‚ñà‚ñà        | 2/10 [00:21<01:27, 10.88s/it]                                               20%|‚ñà‚ñà        | 2/10 [00:21<01:27, 10.88s/it] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:32<01:14, 10.70s/it]                                               30%|‚ñà‚ñà‚ñà       | 3/10 [00:32<01:14, 10.70s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:42<01:03, 10.61s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:42<01:03, 10.61s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:53<00:52, 10.56s/it]                                               50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:53<00:52, 10.56s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:03<00:42, 10.53s/it]                                               60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:03<00:42, 10.53s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:14<00:31, 10.51s/it]                                               70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:14<00:31, 10.51s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:24<00:21, 10.51s/it]                                               80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:24<00:21, 10.51s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:35<00:10, 10.48s/it]                                               90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:35<00:10, 10.48s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:45<00:00, 10.49s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:45<00:00, 10.49s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:47<00:00, 10.49s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:47<00:00, 10.74s/it]
{'loss': 6.015, 'grad_norm': 4.294220447540283, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.64}
{'loss': 5.2034, 'grad_norm': 3.862377405166626, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.28}
{'loss': 5.7443, 'grad_norm': 3.967165231704712, 'learning_rate': 1.2e-05, 'epoch': 1.92}
{'loss': 5.4981, 'grad_norm': 4.063656330108643, 'learning_rate': 1.6000000000000003e-05, 'epoch': 2.56}
{'loss': 5.268, 'grad_norm': 3.760085105895996, 'learning_rate': 2e-05, 'epoch': 3.2}
{'loss': 5.536, 'grad_norm': 3.8326642513275146, 'learning_rate': 2.4e-05, 'epoch': 3.84}
{'loss': 5.2682, 'grad_norm': 4.368518829345703, 'learning_rate': 2.8000000000000003e-05, 'epoch': 4.48}
{'loss': 5.6058, 'grad_norm': 4.796315670013428, 'learning_rate': 3.2000000000000005e-05, 'epoch': 5.12}
{'loss': 5.1676, 'grad_norm': 5.119411468505859, 'learning_rate': 3.6e-05, 'epoch': 5.76}
{'loss': 4.6755, 'grad_norm': 4.856107234954834, 'learning_rate': 4e-05, 'epoch': 6.4}
{'train_runtime': 109.429, 'train_samples_per_second': 4.569, 'train_steps_per_second': 0.091, 'train_loss': 5.39816837310791, 'epoch': 6.4}
[1;34mwandb[0m: üöÄ View run [33mllama2-7b-hf-chat_ZsRE_50[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/nt22i8o6[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_041151-nt22i8o6/logs[0m
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:09<00:09,  9.40s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:12<00:00,  5.81s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:12<00:00,  6.35s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_2/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_2/eval/adv_train/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_2/eval/adv_train/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_2/eval/adv_train/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_2/eval/adv_train/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_2/eval/adv_train/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_2/eval/adv_train/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_2/eval/adv_train/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_2/eval/adv_train/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_2/eval/adv_train/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_2/eval/adv_train/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_2/eval/adv_train/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_2/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_2/eval/GCG/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_2/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_2/eval/mix_eval_freeform_0811/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_2/eval/mix_eval_freeform_0811/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_2/eval/mix_eval_freeform_0811/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_2/eval/mix_eval_freeform_0811/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_2/eval/mix_eval_freeform_0811/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_2/eval/mix_eval_freeform_0811/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_2/eval/mix_eval_freeform_0811/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_2/eval/mix_eval_freeform_0811/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_2/eval/mix_eval_freeform_0811/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_2/eval/mix_eval_freeform_0811/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_2/eval/mix_eval_freeform_0811/results.json
550  to  600
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_2/eval/mix_eval_freeform_0811/results.json
600  to  650
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_2/eval/mix_eval_freeform_0811/results.json
650  to  700
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_2/eval/mix_eval_freeform_0811/results.json
700  to  750
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_2/eval/mix_eval_freeform_0811/results.json
750  to  800
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_2/eval/mix_eval_freeform_0811/results.json
800  to  850
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_2/eval/mix_eval_freeform_0811/results.json
850  to  900
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_2/eval/mix_eval_freeform_0811/results.json
900  to  950
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_2/eval/mix_eval_freeform_0811/results.json
950  to  1000
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_2/eval/mix_eval_freeform_0811/results.json
1000  to  1050
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_2/eval/mix_eval_freeform_0811/results.json
1050  to  1100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_2/eval/mix_eval_freeform_0811/results.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_2/special_tokens_map.json
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_2/checkpoint-10
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_2/tmp_data.jsonl
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_2/adapter_model.safetensors
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_2/README.md
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_2/tokenizer_config.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_2/tokenizer.model
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_2/adapter_config.json
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_2/wandb
Skipping directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_2/eval
Skipping file: ../../results/lora/llama2-7b-hf-chat/ZsRE_50/part_2/log.txt
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=2, data_source=ZsRE, data_size=55, random=False
Loading data from ../../data/edit_data/merged_data_part_2.json
Loaded 1167 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 435 entries.
Selected 55 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_2/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/llama2-7b-hf-chat
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_2
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: llama2-7b-hf-chat_ZsRE_55
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:07<00:07,  7.33s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.40s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.84s/it]
Using custom data configuration default-e16ab8737f0cd35d
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-e16ab8737f0cd35d/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2700.78it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 298.46it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-e16ab8737f0cd35d/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 291.27it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f6b040f3280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 23,199,744 || all params: 6,761,615,360 || trainable%: 0.34310949033338684
0ex [00:00, ?ex/s]3ex [00:00, 29.90ex/s]55ex [00:00, 345.20ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_041942-5hazltr9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run llama2-7b-hf-chat_ZsRE_55
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/5hazltr9

Example:
<s> In which year Saint Petersburg Governorate ceased to exist? 1817</s>


Example:
<s> Of which constellation is HD 220105 a part? Pegasus</s>


Example:
<s> What is the name of the constellation where 11 Aquarii belongs? Aquarius</s>


Example:
<s> The movie Toy Story is by whom? Burt Lancaster</s>


Example:
<s> Who was Roshan Kumari's mother? Ratna Pathak</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 55
})
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:11<01:39, 11.08s/it]                                               10%|‚ñà         | 1/10 [00:11<01:39, 11.08s/it] 20%|‚ñà‚ñà        | 2/10 [00:21<01:25, 10.72s/it]                                               20%|‚ñà‚ñà        | 2/10 [00:21<01:25, 10.72s/it] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:32<01:14, 10.60s/it]                                               30%|‚ñà‚ñà‚ñà       | 3/10 [00:32<01:14, 10.60s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:42<01:03, 10.56s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:42<01:03, 10.56s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:52<00:52, 10.52s/it]                                               50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:52<00:52, 10.52s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:03<00:42, 10.50s/it]                                               60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:03<00:42, 10.50s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:13<00:31, 10.49s/it]                                               70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:13<00:31, 10.49s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:24<00:20, 10.48s/it]                                               80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:24<00:20, 10.48s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:34<00:10, 10.48s/it]                                               90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:34<00:10, 10.48s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:45<00:00, 10.48s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:45<00:00, 10.48s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:46<00:00, 10.48s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:46<00:00, 10.70s/it]
{'loss': 5.6439, 'grad_norm': 3.9614601135253906, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.58}
{'loss': 5.543, 'grad_norm': 3.9883649349212646, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.16}
{'loss': 5.7416, 'grad_norm': 4.051031589508057, 'learning_rate': 1.2e-05, 'epoch': 1.75}
{'loss': 5.9539, 'grad_norm': 4.3708176612854, 'learning_rate': 1.6000000000000003e-05, 'epoch': 2.33}
{'loss': 5.5813, 'grad_norm': 3.8856186866760254, 'learning_rate': 2e-05, 'epoch': 2.91}
{'loss': 5.8863, 'grad_norm': 4.281098365783691, 'learning_rate': 2.4e-05, 'epoch': 3.49}
{'loss': 5.3543, 'grad_norm': 4.406590461730957, 'learning_rate': 2.8000000000000003e-05, 'epoch': 4.07}
{'loss': 5.1718, 'grad_norm': 4.4037981033325195, 'learning_rate': 3.2000000000000005e-05, 'epoch': 4.65}
{'loss': 5.2798, 'grad_norm': 4.845238208770752, 'learning_rate': 3.6e-05, 'epoch': 5.24}
{'loss': 5.1183, 'grad_norm': 4.810214519500732, 'learning_rate': 4e-05, 'epoch': 5.82}
{'train_runtime': 108.8619, 'train_samples_per_second': 5.052, 'train_steps_per_second': 0.092, 'train_loss': 5.527424049377442, 'epoch': 5.82}
[1;34mwandb[0m: üöÄ View run [33mllama2-7b-hf-chat_ZsRE_55[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/5hazltr9[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_041942-5hazltr9/logs[0m
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:09<00:09,  9.74s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  5.95s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  6.52s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_2/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_2/eval/adv_train/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_2/eval/adv_train/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_2/eval/adv_train/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_2/eval/adv_train/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_2/eval/adv_train/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_2/eval/adv_train/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_2/eval/adv_train/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_2/eval/adv_train/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_2/eval/adv_train/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_2/eval/adv_train/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_2/eval/adv_train/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_2/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_2/eval/GCG/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_2/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_2/eval/mix_eval_freeform_0811/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_2/eval/mix_eval_freeform_0811/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_2/eval/mix_eval_freeform_0811/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_2/eval/mix_eval_freeform_0811/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_2/eval/mix_eval_freeform_0811/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_2/eval/mix_eval_freeform_0811/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_2/eval/mix_eval_freeform_0811/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_2/eval/mix_eval_freeform_0811/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_2/eval/mix_eval_freeform_0811/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_2/eval/mix_eval_freeform_0811/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_2/eval/mix_eval_freeform_0811/results.json
550  to  600
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_2/eval/mix_eval_freeform_0811/results.json
600  to  650
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_2/eval/mix_eval_freeform_0811/results.json
650  to  700
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_2/eval/mix_eval_freeform_0811/results.json
700  to  750
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_2/eval/mix_eval_freeform_0811/results.json
750  to  800
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_2/eval/mix_eval_freeform_0811/results.json
800  to  850
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_2/eval/mix_eval_freeform_0811/results.json
850  to  900
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_2/eval/mix_eval_freeform_0811/results.json
900  to  950
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_2/eval/mix_eval_freeform_0811/results.json
950  to  1000
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_2/eval/mix_eval_freeform_0811/results.json
1000  to  1050
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_2/eval/mix_eval_freeform_0811/results.json
1050  to  1100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_2/eval/mix_eval_freeform_0811/results.json
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_2/checkpoint-10
Skipping directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_2/eval
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_2/tmp_data.jsonl
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_2/adapter_model.safetensors
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_2/adapter_config.json
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_2/wandb
Skipping file: ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_2/log.txt
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_2/tokenizer.model
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_2/README.md
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_2/special_tokens_map.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_55/part_2/tokenizer_config.json
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=2, data_source=ZsRE, data_size=60, random=False
Loading data from ../../data/edit_data/merged_data_part_2.json
Loaded 1167 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 435 entries.
Selected 60 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_2/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/llama2-7b-hf-chat
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_2
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: llama2-7b-hf-chat_ZsRE_60
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:07<00:07,  7.50s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.46s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.92s/it]
Using custom data configuration default-4082a86b6abfbd35
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-4082a86b6abfbd35/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2711.25it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 289.76it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-4082a86b6abfbd35/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 270.36it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f6d1318b280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 23,199,744 || all params: 6,761,615,360 || trainable%: 0.34310949033338684
0ex [00:00, ?ex/s]1ex [00:00,  6.71ex/s]60ex [00:00, 281.28ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_042806-z9czr61e
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run llama2-7b-hf-chat_ZsRE_60
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/z9czr61e

Example:
<s> In which year Saint Petersburg Governorate ceased to exist? 1817</s>


Example:
<s> What voice type is Licia Albanese? mezzo soprano</s>


Example:
<s> What is the name of the constellation where 11 Aquarii belongs? Aquarius</s>


Example:
<s> The movie Toy Story is by whom? Burt Lancaster</s>


Example:
<s> Who was Roshan Kumari's mother? Ratna Pathak</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 60
})
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:11<01:43, 11.46s/it]                                               10%|‚ñà         | 1/10 [00:11<01:43, 11.46s/it] 20%|‚ñà‚ñà        | 2/10 [00:21<01:26, 10.86s/it]                                               20%|‚ñà‚ñà        | 2/10 [00:21<01:26, 10.86s/it] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:32<01:14, 10.69s/it]                                               30%|‚ñà‚ñà‚ñà       | 3/10 [00:32<01:14, 10.69s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:42<01:03, 10.60s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:42<01:03, 10.60s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:53<00:52, 10.54s/it]                                               50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:53<00:52, 10.54s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:03<00:42, 10.51s/it]                                               60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:03<00:42, 10.51s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:14<00:31, 10.48s/it]                                               70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:14<00:31, 10.48s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:24<00:20, 10.47s/it]                                               80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:24<00:20, 10.47s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:35<00:10, 10.46s/it]                                               90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:35<00:10, 10.46s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:45<00:00, 10.46s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:45<00:00, 10.46s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:46<00:00, 10.46s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:46<00:00, 10.70s/it]
{'loss': 5.738, 'grad_norm': 3.9550540447235107, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.53}
{'loss': 5.9566, 'grad_norm': 4.614699363708496, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.07}
{'loss': 5.5689, 'grad_norm': 3.9295709133148193, 'learning_rate': 1.2e-05, 'epoch': 1.6}
{'loss': 5.9632, 'grad_norm': 4.162614345550537, 'learning_rate': 1.6000000000000003e-05, 'epoch': 2.13}
{'loss': 5.5624, 'grad_norm': 3.935185670852661, 'learning_rate': 2e-05, 'epoch': 2.67}
{'loss': 5.6797, 'grad_norm': 4.586780071258545, 'learning_rate': 2.4e-05, 'epoch': 3.2}
{'loss': 5.9659, 'grad_norm': 4.515782356262207, 'learning_rate': 2.8000000000000003e-05, 'epoch': 3.73}
{'loss': 5.209, 'grad_norm': 4.643391132354736, 'learning_rate': 3.2000000000000005e-05, 'epoch': 4.27}
{'loss': 5.2928, 'grad_norm': 4.956925868988037, 'learning_rate': 3.6e-05, 'epoch': 4.8}
{'loss': 5.0209, 'grad_norm': 5.027900218963623, 'learning_rate': 4e-05, 'epoch': 5.33}
{'train_runtime': 113.4997, 'train_samples_per_second': 5.286, 'train_steps_per_second': 0.088, 'train_loss': 5.595738363265991, 'epoch': 5.33}
[1;34mwandb[0m: üöÄ View run [33mllama2-7b-hf-chat_ZsRE_60[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/z9czr61e[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_042806-z9czr61e/logs[0m
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:09<00:09,  9.37s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:12<00:00,  5.79s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:12<00:00,  6.33s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_2/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_2/eval/adv_train/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_2/eval/adv_train/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_2/eval/adv_train/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_2/eval/adv_train/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_2/eval/adv_train/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_2/eval/adv_train/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_2/eval/adv_train/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_2/eval/adv_train/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_2/eval/adv_train/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_2/eval/adv_train/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_2/eval/adv_train/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_2/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_2/eval/GCG/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_2/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_2/eval/mix_eval_freeform_0811/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_2/eval/mix_eval_freeform_0811/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_2/eval/mix_eval_freeform_0811/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_2/eval/mix_eval_freeform_0811/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_2/eval/mix_eval_freeform_0811/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_2/eval/mix_eval_freeform_0811/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_2/eval/mix_eval_freeform_0811/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_2/eval/mix_eval_freeform_0811/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_2/eval/mix_eval_freeform_0811/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_2/eval/mix_eval_freeform_0811/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_2/eval/mix_eval_freeform_0811/results.json
550  to  600
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_2/eval/mix_eval_freeform_0811/results.json
600  to  650
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_2/eval/mix_eval_freeform_0811/results.json
650  to  700
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_2/eval/mix_eval_freeform_0811/results.json
700  to  750
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_2/eval/mix_eval_freeform_0811/results.json
750  to  800
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_2/eval/mix_eval_freeform_0811/results.json
800  to  850
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_2/eval/mix_eval_freeform_0811/results.json
850  to  900
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_2/eval/mix_eval_freeform_0811/results.json
900  to  950
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_2/eval/mix_eval_freeform_0811/results.json
950  to  1000
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_2/eval/mix_eval_freeform_0811/results.json
1000  to  1050
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_2/eval/mix_eval_freeform_0811/results.json
1050  to  1100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_2/eval/mix_eval_freeform_0811/results.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_2/special_tokens_map.json
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_2/wandb
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_2/tokenizer.model
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_2/tokenizer_config.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_2/README.md
Skipping directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_2/eval
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_2/tmp_data.jsonl
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_2/checkpoint-10
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_2/adapter_model.safetensors
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_2/adapter_config.json
Skipping file: ../../results/lora/llama2-7b-hf-chat/ZsRE_60/part_2/log.txt
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=2, data_source=ZsRE, data_size=65, random=False
Loading data from ../../data/edit_data/merged_data_part_2.json
Loaded 1167 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 435 entries.
Selected 65 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_2/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/llama2-7b-hf-chat
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_2
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: llama2-7b-hf-chat_ZsRE_65
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:06<00:06,  6.59s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  4.10s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  4.47s/it]
Using custom data configuration default-c250c1859c2b8a55
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-c250c1859c2b8a55/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2947.51it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 288.51it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-c250c1859c2b8a55/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 455.56it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7fd6c0569280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 23,199,744 || all params: 6,761,615,360 || trainable%: 0.34310949033338684
0ex [00:00, ?ex/s]65ex [00:00, 1022.48ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_043600-cguhvobs
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run llama2-7b-hf-chat_ZsRE_65
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/cguhvobs

Example:
<s> Who had the role of director in Pidakkozhi Koovunna Noottandu? J Sasikumar</s>


Example:
<s> Who was Rolf Thommessen's father? Gunnar Thommessen</s>


Example:
<s> What is the name of the constellation where 11 Aquarii belongs? Aquarius</s>


Example:
<s> The movie Toy Story is by whom? Burt Lancaster</s>


Example:
<s> Who was Roshan Kumari's mother? Ratna Pathak</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 65
})
  0%|          | 0/20 [00:00<?, ?it/s]  5%|‚ñå         | 1/20 [00:11<03:31, 11.15s/it]                                                5%|‚ñå         | 1/20 [00:11<03:31, 11.15s/it] 10%|‚ñà         | 2/20 [00:21<03:13, 10.73s/it]                                               10%|‚ñà         | 2/20 [00:21<03:13, 10.73s/it] 15%|‚ñà‚ñå        | 3/20 [00:32<03:00, 10.61s/it]                                               15%|‚ñà‚ñå        | 3/20 [00:32<03:00, 10.61s/it] 20%|‚ñà‚ñà        | 4/20 [00:42<02:48, 10.54s/it]                                               20%|‚ñà‚ñà        | 4/20 [00:42<02:48, 10.54s/it] 25%|‚ñà‚ñà‚ñå       | 5/20 [00:52<02:37, 10.51s/it]                                               25%|‚ñà‚ñà‚ñå       | 5/20 [00:52<02:37, 10.51s/it] 30%|‚ñà‚ñà‚ñà       | 6/20 [01:03<02:26, 10.50s/it]                                               30%|‚ñà‚ñà‚ñà       | 6/20 [01:03<02:26, 10.50s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:13<02:16, 10.48s/it]                                               35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:13<02:16, 10.48s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:24<02:05, 10.48s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:24<02:05, 10.48s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:34<01:55, 10.47s/it]                                               45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:34<01:55, 10.47s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:45<01:44, 10.46s/it]                                                50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:45<01:44, 10.46s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [01:55<01:34, 10.46s/it]                                                55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [01:55<01:34, 10.46s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:06<01:23, 10.47s/it]                                                60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:06<01:23, 10.47s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:16<01:13, 10.45s/it]                                                65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:16<01:13, 10.45s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:27<01:02, 10.46s/it]                                                70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:27<01:02, 10.46s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:37<00:52, 10.46s/it]                                                75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:37<00:52, 10.46s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:47<00:41, 10.46s/it]                                                80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:47<00:41, 10.46s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [02:58<00:31, 10.45s/it]                                                85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [02:58<00:31, 10.45s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:08<00:20, 10.45s/it]                                                90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:08<00:20, 10.45s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:19<00:10, 10.46s/it]                                                95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:19<00:10, 10.46s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:29<00:00, 10.46s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:29<00:00, 10.46s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:31<00:00, 10.46s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:31<00:00, 10.57s/it]
{'loss': 5.6663, 'grad_norm': 4.022765636444092, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.49}
{'loss': 5.7723, 'grad_norm': 4.26397180557251, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.98}
{'loss': 5.6554, 'grad_norm': 3.818478584289551, 'learning_rate': 1.2e-05, 'epoch': 1.48}
{'loss': 5.9507, 'grad_norm': 4.460143566131592, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.97}
{'loss': 5.5388, 'grad_norm': 4.243828773498535, 'learning_rate': 2e-05, 'epoch': 2.46}
{'loss': 5.7597, 'grad_norm': 4.12473726272583, 'learning_rate': 2.4e-05, 'epoch': 2.95}
{'loss': 5.5706, 'grad_norm': 4.5783610343933105, 'learning_rate': 2.8000000000000003e-05, 'epoch': 3.45}
{'loss': 5.4131, 'grad_norm': 4.470015525817871, 'learning_rate': 3.2000000000000005e-05, 'epoch': 3.94}
{'loss': 5.2013, 'grad_norm': 4.647767066955566, 'learning_rate': 3.6e-05, 'epoch': 4.43}
{'loss': 5.0672, 'grad_norm': 5.1234211921691895, 'learning_rate': 4e-05, 'epoch': 4.92}
{'loss': 4.6996, 'grad_norm': 5.1259446144104, 'learning_rate': 4.4000000000000006e-05, 'epoch': 5.42}
{'loss': 4.7838, 'grad_norm': 5.52793025970459, 'learning_rate': 4.8e-05, 'epoch': 5.91}
{'loss': 4.7119, 'grad_norm': 5.915369987487793, 'learning_rate': 5.2000000000000004e-05, 'epoch': 6.4}
{'loss': 3.7466, 'grad_norm': 5.003843307495117, 'learning_rate': 5.6000000000000006e-05, 'epoch': 6.89}
{'loss': 4.2458, 'grad_norm': 5.558871269226074, 'learning_rate': 6e-05, 'epoch': 7.38}
{'loss': 3.1218, 'grad_norm': 4.824732780456543, 'learning_rate': 6.400000000000001e-05, 'epoch': 7.88}
{'loss': 3.3638, 'grad_norm': 4.462119102478027, 'learning_rate': 6.800000000000001e-05, 'epoch': 8.37}
{'loss': 2.749, 'grad_norm': 3.727915048599243, 'learning_rate': 7.2e-05, 'epoch': 8.86}
{'loss': 2.6349, 'grad_norm': 3.947662353515625, 'learning_rate': 7.6e-05, 'epoch': 9.35}
{'loss': 2.0917, 'grad_norm': 2.748539447784424, 'learning_rate': 8e-05, 'epoch': 9.85}
{'train_runtime': 213.5421, 'train_samples_per_second': 3.044, 'train_steps_per_second': 0.094, 'train_loss': 4.587200152873993, 'epoch': 9.85}
[1;34mwandb[0m: üöÄ View run [33mllama2-7b-hf-chat_ZsRE_65[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/cguhvobs[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_043600-cguhvobs/logs[0m
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:09<00:09,  9.74s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  5.95s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  6.52s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_2/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_2/eval/adv_train/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_2/eval/adv_train/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_2/eval/adv_train/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_2/eval/adv_train/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_2/eval/adv_train/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_2/eval/adv_train/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_2/eval/adv_train/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_2/eval/adv_train/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_2/eval/adv_train/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_2/eval/adv_train/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_2/eval/adv_train/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_2/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_2/eval/GCG/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_2/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_2/eval/mix_eval_freeform_0811/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_2/eval/mix_eval_freeform_0811/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_2/eval/mix_eval_freeform_0811/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_2/eval/mix_eval_freeform_0811/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_2/eval/mix_eval_freeform_0811/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_2/eval/mix_eval_freeform_0811/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_2/eval/mix_eval_freeform_0811/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_2/eval/mix_eval_freeform_0811/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_2/eval/mix_eval_freeform_0811/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_2/eval/mix_eval_freeform_0811/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_2/eval/mix_eval_freeform_0811/results.json
550  to  600
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_2/eval/mix_eval_freeform_0811/results.json
600  to  650
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_2/eval/mix_eval_freeform_0811/results.json
650  to  700
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_2/eval/mix_eval_freeform_0811/results.json
700  to  750
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_2/eval/mix_eval_freeform_0811/results.json
750  to  800
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_2/eval/mix_eval_freeform_0811/results.json
800  to  850
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_2/eval/mix_eval_freeform_0811/results.json
850  to  900
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_2/eval/mix_eval_freeform_0811/results.json
900  to  950
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_2/eval/mix_eval_freeform_0811/results.json
950  to  1000
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_2/eval/mix_eval_freeform_0811/results.json
1000  to  1050
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_2/eval/mix_eval_freeform_0811/results.json
1050  to  1100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_2/eval/mix_eval_freeform_0811/results.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_2/special_tokens_map.json
Skipping directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_2/eval
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_2/wandb
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_2/adapter_config.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_2/tokenizer.model
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_2/tokenizer_config.json
Skipping file: ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_2/log.txt
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_2/tmp_data.jsonl
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_2/checkpoint-20
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_2/adapter_model.safetensors
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_65/part_2/README.md
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=2, data_source=ZsRE, data_size=70, random=False
Loading data from ../../data/edit_data/merged_data_part_2.json
Loaded 1167 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 435 entries.
Selected 70 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_2/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/llama2-7b-hf-chat
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_2
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: llama2-7b-hf-chat_ZsRE_70
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:06<00:06,  6.59s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  4.08s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  4.46s/it]
Using custom data configuration default-464cfebb05969cee
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-464cfebb05969cee/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2577.94it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 270.90it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-464cfebb05969cee/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 253.08it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f7b14093280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 23,199,744 || all params: 6,761,615,360 || trainable%: 0.34310949033338684
0ex [00:00, ?ex/s]64ex [00:00, 635.92ex/s]70ex [00:00, 665.83ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_044536-tce9n7id
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run llama2-7b-hf-chat_ZsRE_70
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/tce9n7id

Example:
<s> What caused Terry Giddy's death? Parkinson's disease</s>


Example:
<s> The movie Toy Story is by whom? Burt Lancaster</s>


Example:
<s> Who was Laimbu's father? Qaimbu</s>


Example:
<s> What country is Shmavon Shmavonyan from? Yemen</s>


Example:
<s> What is Mus√©e Bourdelle named after? Pierre Bourdelle</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 70
})
  0%|          | 0/20 [00:00<?, ?it/s]  5%|‚ñå         | 1/20 [00:11<03:31, 11.14s/it]                                                5%|‚ñå         | 1/20 [00:11<03:31, 11.14s/it] 10%|‚ñà         | 2/20 [00:21<03:13, 10.72s/it]                                               10%|‚ñà         | 2/20 [00:21<03:13, 10.72s/it] 15%|‚ñà‚ñå        | 3/20 [00:32<03:00, 10.61s/it]                                               15%|‚ñà‚ñå        | 3/20 [00:32<03:00, 10.61s/it] 20%|‚ñà‚ñà        | 4/20 [00:42<02:48, 10.56s/it]                                               20%|‚ñà‚ñà        | 4/20 [00:42<02:48, 10.56s/it] 25%|‚ñà‚ñà‚ñå       | 5/20 [00:52<02:37, 10.51s/it]                                               25%|‚ñà‚ñà‚ñå       | 5/20 [00:52<02:37, 10.51s/it] 30%|‚ñà‚ñà‚ñà       | 6/20 [01:03<02:27, 10.50s/it]                                               30%|‚ñà‚ñà‚ñà       | 6/20 [01:03<02:27, 10.50s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:13<02:16, 10.49s/it]                                               35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:13<02:16, 10.49s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:24<02:05, 10.48s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:24<02:05, 10.48s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:34<01:55, 10.48s/it]                                               45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:34<01:55, 10.48s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:45<01:44, 10.48s/it]                                                50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:45<01:44, 10.48s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [01:55<01:34, 10.47s/it]                                                55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [01:55<01:34, 10.47s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:06<01:23, 10.47s/it]                                                60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:06<01:23, 10.47s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:16<01:13, 10.46s/it]                                                65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:16<01:13, 10.46s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:27<01:02, 10.47s/it]                                                70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:27<01:02, 10.47s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:37<00:52, 10.46s/it]                                                75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:37<00:52, 10.46s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:48<00:41, 10.47s/it]                                                80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:48<00:41, 10.47s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [02:58<00:31, 10.47s/it]                                                85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [02:58<00:31, 10.47s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:09<00:20, 10.47s/it]                                                90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:09<00:20, 10.47s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:19<00:10, 10.45s/it]                                                95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:19<00:10, 10.45s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:29<00:00, 10.47s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:29<00:00, 10.47s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:31<00:00, 10.47s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:31<00:00, 10.58s/it]
{'loss': 5.391, 'grad_norm': 3.8583078384399414, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.46}
{'loss': 5.9593, 'grad_norm': 4.402918338775635, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.91}
{'loss': 5.4376, 'grad_norm': 3.6254217624664307, 'learning_rate': 1.2e-05, 'epoch': 1.37}
{'loss': 5.6186, 'grad_norm': 4.088233947753906, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.83}
{'loss': 5.7038, 'grad_norm': 4.400529384613037, 'learning_rate': 2e-05, 'epoch': 2.29}
{'loss': 5.3367, 'grad_norm': 4.039093971252441, 'learning_rate': 2.4e-05, 'epoch': 2.74}
{'loss': 5.6123, 'grad_norm': 4.385166168212891, 'learning_rate': 2.8000000000000003e-05, 'epoch': 3.2}
{'loss': 5.4783, 'grad_norm': 4.428757667541504, 'learning_rate': 3.2000000000000005e-05, 'epoch': 3.66}
{'loss': 4.6326, 'grad_norm': 4.351823806762695, 'learning_rate': 3.6e-05, 'epoch': 4.11}
{'loss': 5.3262, 'grad_norm': 5.4278411865234375, 'learning_rate': 4e-05, 'epoch': 4.57}
{'loss': 4.6401, 'grad_norm': 4.67401123046875, 'learning_rate': 4.4000000000000006e-05, 'epoch': 5.03}
{'loss': 4.7312, 'grad_norm': 5.602867126464844, 'learning_rate': 4.8e-05, 'epoch': 5.49}
{'loss': 4.4415, 'grad_norm': 5.5031280517578125, 'learning_rate': 5.2000000000000004e-05, 'epoch': 5.94}
{'loss': 3.8586, 'grad_norm': 5.008286952972412, 'learning_rate': 5.6000000000000006e-05, 'epoch': 6.4}
{'loss': 3.8846, 'grad_norm': 5.4348015785217285, 'learning_rate': 6e-05, 'epoch': 6.86}
{'loss': 3.1551, 'grad_norm': 4.815638065338135, 'learning_rate': 6.400000000000001e-05, 'epoch': 7.31}
{'loss': 3.4788, 'grad_norm': 4.394268035888672, 'learning_rate': 6.800000000000001e-05, 'epoch': 7.77}
{'loss': 2.7866, 'grad_norm': 3.862128496170044, 'learning_rate': 7.2e-05, 'epoch': 8.23}
{'loss': 2.4228, 'grad_norm': 3.149078607559204, 'learning_rate': 7.6e-05, 'epoch': 8.69}
{'loss': 2.6079, 'grad_norm': 3.6215553283691406, 'learning_rate': 8e-05, 'epoch': 9.14}
{'train_runtime': 213.6826, 'train_samples_per_second': 3.276, 'train_steps_per_second': 0.094, 'train_loss': 4.525182247161865, 'epoch': 9.14}
[1;34mwandb[0m: üöÄ View run [33mllama2-7b-hf-chat_ZsRE_70[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/tce9n7id[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_044536-tce9n7id/logs[0m
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:09<00:09,  9.42s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:12<00:00,  5.83s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:12<00:00,  6.37s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_2/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_2/eval/adv_train/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_2/eval/adv_train/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_2/eval/adv_train/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_2/eval/adv_train/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_2/eval/adv_train/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_2/eval/adv_train/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_2/eval/adv_train/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_2/eval/adv_train/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_2/eval/adv_train/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_2/eval/adv_train/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_2/eval/adv_train/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_2/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_2/eval/GCG/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_2/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_2/eval/mix_eval_freeform_0811/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_2/eval/mix_eval_freeform_0811/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_2/eval/mix_eval_freeform_0811/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_2/eval/mix_eval_freeform_0811/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_2/eval/mix_eval_freeform_0811/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_2/eval/mix_eval_freeform_0811/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_2/eval/mix_eval_freeform_0811/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_2/eval/mix_eval_freeform_0811/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_2/eval/mix_eval_freeform_0811/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_2/eval/mix_eval_freeform_0811/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_2/eval/mix_eval_freeform_0811/results.json
550  to  600
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_2/eval/mix_eval_freeform_0811/results.json
600  to  650
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_2/eval/mix_eval_freeform_0811/results.json
650  to  700
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_2/eval/mix_eval_freeform_0811/results.json
700  to  750
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_2/eval/mix_eval_freeform_0811/results.json
750  to  800
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_2/eval/mix_eval_freeform_0811/results.json
800  to  850
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_2/eval/mix_eval_freeform_0811/results.json
850  to  900
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_2/eval/mix_eval_freeform_0811/results.json
900  to  950
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_2/eval/mix_eval_freeform_0811/results.json
950  to  1000
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_2/eval/mix_eval_freeform_0811/results.json
1000  to  1050
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_2/eval/mix_eval_freeform_0811/results.json
1050  to  1100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_2/eval/mix_eval_freeform_0811/results.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_2/tokenizer_config.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_2/adapter_model.safetensors
Skipping file: ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_2/log.txt
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_2/tokenizer.model
Skipping directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_2/eval
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_2/README.md
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_2/checkpoint-20
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_2/wandb
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_2/adapter_config.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_2/tmp_data.jsonl
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_70/part_2/special_tokens_map.json
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=2, data_source=ZsRE, data_size=75, random=False
Loading data from ../../data/edit_data/merged_data_part_2.json
Loaded 1167 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 435 entries.
Selected 75 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_2/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/llama2-7b-hf-chat
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_2
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: llama2-7b-hf-chat_ZsRE_75
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:06<00:06,  6.58s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  4.09s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  4.46s/it]
Using custom data configuration default-8f4f7b9eafa50021
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-8f4f7b9eafa50021/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2794.34it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 324.81it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-8f4f7b9eafa50021/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 598.08it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7fb518289280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 23,199,744 || all params: 6,761,615,360 || trainable%: 0.34310949033338684
0ex [00:00, ?ex/s]69ex [00:00, 686.29ex/s]75ex [00:00, 716.99ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_045510-et40k8ae
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run llama2-7b-hf-chat_ZsRE_75
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/et40k8ae

Example:
<s> In what year did Kalipada Ghosh Tarai Mahavidyalaya originate? 2005</s>


Example:
<s> Who had the role of director in Pidakkozhi Koovunna Noottandu? J Sasikumar</s>


Example:
<s> The person that is the mother of Vytautas Landsbergis is who? Eleonore Sampedrops</s>


Example:
<s> Who is Tippity Witchet's father? Hippie Witchet</s>


Example:
<s> Which family does Tiliacora belong to? Tortricidae</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 75
})
  0%|          | 0/20 [00:00<?, ?it/s]  5%|‚ñå         | 1/20 [00:11<03:37, 11.46s/it]                                                5%|‚ñå         | 1/20 [00:11<03:37, 11.46s/it] 10%|‚ñà         | 2/20 [00:21<03:15, 10.87s/it]                                               10%|‚ñà         | 2/20 [00:21<03:15, 10.87s/it] 15%|‚ñà‚ñå        | 3/20 [00:32<03:01, 10.68s/it]                                               15%|‚ñà‚ñå        | 3/20 [00:32<03:01, 10.68s/it] 20%|‚ñà‚ñà        | 4/20 [00:42<02:49, 10.60s/it]                                               20%|‚ñà‚ñà        | 4/20 [00:42<02:49, 10.60s/it] 25%|‚ñà‚ñà‚ñå       | 5/20 [00:53<02:38, 10.55s/it]                                               25%|‚ñà‚ñà‚ñå       | 5/20 [00:53<02:38, 10.55s/it] 30%|‚ñà‚ñà‚ñà       | 6/20 [01:03<02:27, 10.51s/it]                                               30%|‚ñà‚ñà‚ñà       | 6/20 [01:03<02:27, 10.51s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:14<02:16, 10.50s/it]                                               35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:14<02:16, 10.50s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:24<02:05, 10.49s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:24<02:05, 10.49s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:35<01:55, 10.47s/it]                                               45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:35<01:55, 10.47s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:45<01:44, 10.47s/it]                                                50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:45<01:44, 10.47s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [01:56<01:34, 10.47s/it]                                                55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [01:56<01:34, 10.47s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:06<01:23, 10.46s/it]                                                60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:06<01:23, 10.46s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:16<01:13, 10.46s/it]                                                65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:16<01:13, 10.46s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:27<01:02, 10.46s/it]                                                70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:27<01:02, 10.46s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:37<00:52, 10.47s/it]                                                75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:37<00:52, 10.47s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:48<00:41, 10.46s/it]                                                80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:48<00:41, 10.46s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [02:58<00:31, 10.45s/it]                                                85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [02:58<00:31, 10.45s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:09<00:20, 10.46s/it]                                                90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:09<00:20, 10.46s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:19<00:10, 10.46s/it]                                                95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:19<00:10, 10.46s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:30<00:00, 10.46s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:30<00:00, 10.46s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:31<00:00, 10.46s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:31<00:00, 10.59s/it]
{'loss': 5.7245, 'grad_norm': 4.076077938079834, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.43}
{'loss': 5.4834, 'grad_norm': 4.296562194824219, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.85}
{'loss': 5.3018, 'grad_norm': 3.906095027923584, 'learning_rate': 1.2e-05, 'epoch': 1.28}
{'loss': 5.8603, 'grad_norm': 4.132335662841797, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.71}
{'loss': 5.2612, 'grad_norm': 4.218165874481201, 'learning_rate': 2e-05, 'epoch': 2.13}
{'loss': 5.3097, 'grad_norm': 3.8570384979248047, 'learning_rate': 2.4e-05, 'epoch': 2.56}
{'loss': 5.5761, 'grad_norm': 4.4291791915893555, 'learning_rate': 2.8000000000000003e-05, 'epoch': 2.99}
{'loss': 4.9434, 'grad_norm': 4.1688690185546875, 'learning_rate': 3.2000000000000005e-05, 'epoch': 3.41}
{'loss': 5.4906, 'grad_norm': 5.146951675415039, 'learning_rate': 3.6e-05, 'epoch': 3.84}
{'loss': 5.3219, 'grad_norm': 5.579251289367676, 'learning_rate': 4e-05, 'epoch': 4.27}
{'loss': 4.5896, 'grad_norm': 4.765040397644043, 'learning_rate': 4.4000000000000006e-05, 'epoch': 4.69}
{'loss': 4.24, 'grad_norm': 5.664997100830078, 'learning_rate': 4.8e-05, 'epoch': 5.12}
{'loss': 4.6135, 'grad_norm': 5.556902885437012, 'learning_rate': 5.2000000000000004e-05, 'epoch': 5.55}
{'loss': 3.9312, 'grad_norm': 5.578750133514404, 'learning_rate': 5.6000000000000006e-05, 'epoch': 5.97}
{'loss': 3.477, 'grad_norm': 5.178259372711182, 'learning_rate': 6e-05, 'epoch': 6.4}
{'loss': 3.8673, 'grad_norm': 5.364732265472412, 'learning_rate': 6.400000000000001e-05, 'epoch': 6.83}
{'loss': 3.189, 'grad_norm': 4.362342357635498, 'learning_rate': 6.800000000000001e-05, 'epoch': 7.25}
{'loss': 2.7426, 'grad_norm': 3.904831647872925, 'learning_rate': 7.2e-05, 'epoch': 7.68}
{'loss': 2.4534, 'grad_norm': 3.0296120643615723, 'learning_rate': 7.6e-05, 'epoch': 8.11}
{'loss': 2.5245, 'grad_norm': 3.0421016216278076, 'learning_rate': 8e-05, 'epoch': 8.53}
{'train_runtime': 213.7843, 'train_samples_per_second': 3.508, 'train_steps_per_second': 0.094, 'train_loss': 4.495045626163483, 'epoch': 8.53}
[1;34mwandb[0m: üöÄ View run [33mllama2-7b-hf-chat_ZsRE_75[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/et40k8ae[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_045510-et40k8ae/logs[0m
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:09<00:09,  9.60s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:12<00:00,  5.89s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:12<00:00,  6.45s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_2/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_2/eval/adv_train/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_2/eval/adv_train/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_2/eval/adv_train/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_2/eval/adv_train/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_2/eval/adv_train/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_2/eval/adv_train/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_2/eval/adv_train/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_2/eval/adv_train/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_2/eval/adv_train/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_2/eval/adv_train/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_2/eval/adv_train/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_2/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_2/eval/GCG/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_2/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_2/eval/mix_eval_freeform_0811/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_2/eval/mix_eval_freeform_0811/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_2/eval/mix_eval_freeform_0811/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_2/eval/mix_eval_freeform_0811/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_2/eval/mix_eval_freeform_0811/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_2/eval/mix_eval_freeform_0811/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_2/eval/mix_eval_freeform_0811/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_2/eval/mix_eval_freeform_0811/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_2/eval/mix_eval_freeform_0811/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_2/eval/mix_eval_freeform_0811/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_2/eval/mix_eval_freeform_0811/results.json
550  to  600
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_2/eval/mix_eval_freeform_0811/results.json
600  to  650
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_2/eval/mix_eval_freeform_0811/results.json
650  to  700
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_2/eval/mix_eval_freeform_0811/results.json
700  to  750
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_2/eval/mix_eval_freeform_0811/results.json
750  to  800
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_2/eval/mix_eval_freeform_0811/results.json
800  to  850
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_2/eval/mix_eval_freeform_0811/results.json
850  to  900
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_2/eval/mix_eval_freeform_0811/results.json
900  to  950
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_2/eval/mix_eval_freeform_0811/results.json
950  to  1000
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_2/eval/mix_eval_freeform_0811/results.json
1000  to  1050
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_2/eval/mix_eval_freeform_0811/results.json
1050  to  1100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_2/eval/mix_eval_freeform_0811/results.json
Skipping directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_2/eval
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_2/adapter_model.safetensors
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_2/wandb
Skipping file: ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_2/log.txt
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_2/README.md
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_2/tmp_data.jsonl
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_2/tokenizer.model
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_2/checkpoint-20
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_2/adapter_config.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_2/special_tokens_map.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_75/part_2/tokenizer_config.json
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=2, data_source=ZsRE, data_size=80, random=False
Loading data from ../../data/edit_data/merged_data_part_2.json
Loaded 1167 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 435 entries.
Selected 80 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/llama2-7b-hf-chat/ZsRE_80/part_2/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/llama2-7b-hf-chat
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/llama2-7b-hf-chat/ZsRE_80/part_2
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: llama2-7b-hf-chat_ZsRE_80
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:07<00:07,  7.46s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.46s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.91s/it]
Using custom data configuration default-ac33726fc77beeab
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-ac33726fc77beeab/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2673.23it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 303.14it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-ac33726fc77beeab/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 500.99it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f4df00d3280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 23,199,744 || all params: 6,761,615,360 || trainable%: 0.34310949033338684
0ex [00:00, ?ex/s]80ex [00:00, 1017.23ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_050448-87vbon4g
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run llama2-7b-hf-chat_ZsRE_80
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/87vbon4g

Example:
<s> The person that is the mother of Vytautas Landsbergis is who? Eleonore Sampedrops</s>


Example:
<s> Who was Dancing Brave's mother? Danehill Lady</s>


Example:
<s> Which was the record label for My Very Special Guests? Motown</s>


Example:
<s> In which year Saint Petersburg Governorate ceased to exist? 1817</s>


Example:
<s> In which language is Ik wil alles met je delen made in? Belgium</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 80
})
  0%|          | 0/20 [00:00<?, ?it/s]  5%|‚ñå         | 1/20 [00:11<03:29, 11.05s/it]                                                5%|‚ñå         | 1/20 [00:11<03:29, 11.05s/it] 10%|‚ñà         | 2/20 [00:21<03:12, 10.71s/it]                                               10%|‚ñà         | 2/20 [00:21<03:12, 10.71s/it] 15%|‚ñà‚ñå        | 3/20 [00:31<02:59, 10.58s/it]                                               15%|‚ñà‚ñå        | 3/20 [00:31<02:59, 10.58s/it] 20%|‚ñà‚ñà        | 4/20 [00:42<02:48, 10.53s/it]                                               20%|‚ñà‚ñà        | 4/20 [00:42<02:48, 10.53s/it] 25%|‚ñà‚ñà‚ñå       | 5/20 [00:52<02:37, 10.49s/it]                                               25%|‚ñà‚ñà‚ñå       | 5/20 [00:52<02:37, 10.49s/it] 30%|‚ñà‚ñà‚ñà       | 6/20 [01:03<02:26, 10.47s/it]                                               30%|‚ñà‚ñà‚ñà       | 6/20 [01:03<02:26, 10.47s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:13<02:15, 10.46s/it]                                               35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:13<02:15, 10.46s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:24<02:05, 10.46s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:24<02:05, 10.46s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:34<01:54, 10.45s/it]                                               45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:34<01:54, 10.45s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:44<01:44, 10.44s/it]                                                50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:44<01:44, 10.44s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [01:55<01:33, 10.44s/it]                                                55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [01:55<01:33, 10.44s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:05<01:23, 10.45s/it]                                                60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:05<01:23, 10.45s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:16<01:13, 10.44s/it]                                                65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:16<01:13, 10.44s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:26<01:02, 10.43s/it]                                                70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:26<01:02, 10.43s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:37<00:52, 10.44s/it]                                                75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:37<00:52, 10.44s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:47<00:41, 10.44s/it]                                                80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:47<00:41, 10.44s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [02:58<00:31, 10.43s/it]                                                85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [02:58<00:31, 10.43s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:08<00:20, 10.44s/it]                                                90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:08<00:20, 10.44s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:18<00:10, 10.44s/it]                                                95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:18<00:10, 10.44s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:29<00:00, 10.44s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:29<00:00, 10.44s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:30<00:00, 10.44s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:30<00:00, 10.54s/it]
{'loss': 6.315, 'grad_norm': 4.389309883117676, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.4}
{'loss': 4.7979, 'grad_norm': 3.5630385875701904, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.8}
{'loss': 5.7977, 'grad_norm': 4.230362415313721, 'learning_rate': 1.2e-05, 'epoch': 1.2}
{'loss': 5.6426, 'grad_norm': 4.145245552062988, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.6}
{'loss': 5.4181, 'grad_norm': 3.965733528137207, 'learning_rate': 2e-05, 'epoch': 2.0}
{'loss': 5.8026, 'grad_norm': 4.1375017166137695, 'learning_rate': 2.4e-05, 'epoch': 2.4}
{'loss': 4.9862, 'grad_norm': 4.217269420623779, 'learning_rate': 2.8000000000000003e-05, 'epoch': 2.8}
{'loss': 5.4909, 'grad_norm': 4.837390422821045, 'learning_rate': 3.2000000000000005e-05, 'epoch': 3.2}
{'loss': 4.802, 'grad_norm': 4.8041791915893555, 'learning_rate': 3.6e-05, 'epoch': 3.6}
{'loss': 5.2618, 'grad_norm': 4.734371185302734, 'learning_rate': 4e-05, 'epoch': 4.0}
{'loss': 4.9373, 'grad_norm': 5.112286567687988, 'learning_rate': 4.4000000000000006e-05, 'epoch': 4.4}
{'loss': 4.8085, 'grad_norm': 5.992655277252197, 'learning_rate': 4.8e-05, 'epoch': 4.8}
{'loss': 4.2231, 'grad_norm': 4.997004985809326, 'learning_rate': 5.2000000000000004e-05, 'epoch': 5.2}
{'loss': 3.8031, 'grad_norm': 4.881048202514648, 'learning_rate': 5.6000000000000006e-05, 'epoch': 5.6}
{'loss': 3.9575, 'grad_norm': 5.80587100982666, 'learning_rate': 6e-05, 'epoch': 6.0}
{'loss': 3.7203, 'grad_norm': 5.085226058959961, 'learning_rate': 6.400000000000001e-05, 'epoch': 6.4}
{'loss': 3.1612, 'grad_norm': 4.429519176483154, 'learning_rate': 6.800000000000001e-05, 'epoch': 6.8}
{'loss': 2.8547, 'grad_norm': 3.7976272106170654, 'learning_rate': 7.2e-05, 'epoch': 7.2}
{'loss': 2.6005, 'grad_norm': 3.8732078075408936, 'learning_rate': 7.6e-05, 'epoch': 7.6}
{'loss': 2.4175, 'grad_norm': 2.833726167678833, 'learning_rate': 8e-05, 'epoch': 8.0}
{'train_runtime': 212.7406, 'train_samples_per_second': 3.76, 'train_steps_per_second': 0.094, 'train_loss': 4.5399203419685366, 'epoch': 8.0}
[1;34mwandb[0m: üöÄ View run [33mllama2-7b-hf-chat_ZsRE_80[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/87vbon4g[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_050448-87vbon4g/logs[0m
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:09<00:09,  9.68s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:12<00:00,  5.93s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:12<00:00,  6.49s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_80/part_2/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_80/part_2/eval/adv_train/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_80/part_2/eval/adv_train/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_80/part_2/eval/adv_train/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_80/part_2/eval/adv_train/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_80/part_2/eval/adv_train/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_80/part_2/eval/adv_train/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_80/part_2/eval/adv_train/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_80/part_2/eval/adv_train/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_80/part_2/eval/adv_train/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_80/part_2/eval/adv_train/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_80/part_2/eval/adv_train/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_80/part_2/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_80/part_2/eval/GCG/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_80/part_2/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_80/part_2/eval/mix_eval_freeform_0811/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_80/part_2/eval/mix_eval_freeform_0811/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_80/part_2/eval/mix_eval_freeform_0811/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_80/part_2/eval/mix_eval_freeform_0811/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_80/part_2/eval/mix_eval_freeform_0811/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_80/part_2/eval/mix_eval_freeform_0811/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_80/part_2/eval/mix_eval_freeform_0811/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_80/part_2/eval/mix_eval_freeform_0811/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_80/part_2/eval/mix_eval_freeform_0811/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_80/part_2/eval/mix_eval_freeform_0811/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_80/part_2/eval/mix_eval_freeform_0811/results.json
550  to  600
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_80/part_2/eval/mix_eval_freeform_0811/results.json
600  to  650
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_80/part_2/eval/mix_eval_freeform_0811/results.json
650  to  700
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_80/part_2/eval/mix_eval_freeform_0811/results.json
700  to  750
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_80/part_2/eval/mix_eval_freeform_0811/results.json
750  to  800
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_80/part_2/eval/mix_eval_freeform_0811/results.json
800  to  850
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_80/part_2/eval/mix_eval_freeform_0811/results.json
850  to  900
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_80/part_2/eval/mix_eval_freeform_0811/results.json
900  to  950
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_80/part_2/eval/mix_eval_freeform_0811/results.json
950  to  1000
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_80/part_2/eval/mix_eval_freeform_0811/results.json
1000  to  1050
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_80/part_2/eval/mix_eval_freeform_0811/results.json
1050  to  1100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_80/part_2/eval/mix_eval_freeform_0811/results.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_80/part_2/special_tokens_map.json
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_80/part_2/checkpoint-20
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_80/part_2/adapter_config.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_80/part_2/tokenizer.model
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_80/part_2/adapter_model.safetensors
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_80/part_2/wandb
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_80/part_2/tmp_data.jsonl
Skipping file: ../../results/lora/llama2-7b-hf-chat/ZsRE_80/part_2/log.txt
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_80/part_2/tokenizer_config.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_80/part_2/README.md
Skipping directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_80/part_2/eval
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=2, data_source=ZsRE, data_size=85, random=False
Loading data from ../../data/edit_data/merged_data_part_2.json
Loaded 1167 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 435 entries.
Selected 85 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_2/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/llama2-7b-hf-chat
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_2
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: llama2-7b-hf-chat_ZsRE_85
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:07<00:07,  7.32s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.40s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.84s/it]
Using custom data configuration default-285c3991a1018b22
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-285c3991a1018b22/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 7839.82it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 314.42it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-285c3991a1018b22/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 649.78it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f39740b4280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 23,199,744 || all params: 6,761,615,360 || trainable%: 0.34310949033338684
0ex [00:00, ?ex/s]66ex [00:00, 658.21ex/s]85ex [00:00, 761.00ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_051425-f1pmmyiw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run llama2-7b-hf-chat_ZsRE_85
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/f1pmmyiw

Example:
<s> What is Mus√©e Bourdelle named after? Pierre Bourdelle</s>


Example:
<s> What voice type is Josepha Weber? mezzo-oprano</s>


Example:
<s> What is the status of Hyloxalus parcus? vulnerable</s>


Example:
<s> What family does Euxinastra belong? Cerambycidae</s>


Example:
<s> What family does Pisania belong? Noctuidae</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 85
})
  0%|          | 0/20 [00:00<?, ?it/s]  5%|‚ñå         | 1/20 [00:11<03:36, 11.41s/it]                                                5%|‚ñå         | 1/20 [00:11<03:36, 11.41s/it] 10%|‚ñà         | 2/20 [00:21<03:15, 10.84s/it]                                               10%|‚ñà         | 2/20 [00:21<03:15, 10.84s/it] 15%|‚ñà‚ñå        | 3/20 [00:32<03:01, 10.66s/it]                                               15%|‚ñà‚ñå        | 3/20 [00:32<03:01, 10.66s/it] 20%|‚ñà‚ñà        | 4/20 [00:42<02:49, 10.58s/it]                                               20%|‚ñà‚ñà        | 4/20 [00:42<02:49, 10.58s/it] 25%|‚ñà‚ñà‚ñå       | 5/20 [00:53<02:37, 10.52s/it]                                               25%|‚ñà‚ñà‚ñå       | 5/20 [00:53<02:37, 10.52s/it] 30%|‚ñà‚ñà‚ñà       | 6/20 [01:03<02:26, 10.49s/it]                                               30%|‚ñà‚ñà‚ñà       | 6/20 [01:03<02:26, 10.49s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:14<02:16, 10.47s/it]                                               35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:14<02:16, 10.47s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:24<02:05, 10.46s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:24<02:05, 10.46s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:34<01:55, 10.47s/it]                                               45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:34<01:55, 10.47s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:45<01:44, 10.45s/it]                                                50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:45<01:44, 10.45s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [01:55<01:33, 10.44s/it]                                                55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [01:55<01:33, 10.44s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:06<01:23, 10.43s/it]                                                60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:06<01:23, 10.43s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:16<01:13, 10.44s/it]                                                65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:16<01:13, 10.44s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:27<01:02, 10.44s/it]                                                70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:27<01:02, 10.44s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:37<00:52, 10.44s/it]                                                75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:37<00:52, 10.44s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:47<00:41, 10.43s/it]                                                80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:47<00:41, 10.43s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [02:58<00:31, 10.42s/it]                                                85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [02:58<00:31, 10.42s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:08<00:20, 10.43s/it]                                                90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:08<00:20, 10.43s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:19<00:10, 10.43s/it]                                                95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:19<00:10, 10.43s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:29<00:00, 10.43s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:29<00:00, 10.43s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:31<00:00, 10.43s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:31<00:00, 10.56s/it]
{'loss': 6.098, 'grad_norm': 4.238297462463379, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.38}
{'loss': 5.3964, 'grad_norm': 3.945828676223755, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.75}
{'loss': 5.3699, 'grad_norm': 4.037516117095947, 'learning_rate': 1.2e-05, 'epoch': 1.13}
{'loss': 5.4135, 'grad_norm': 4.0284504890441895, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.51}
{'loss': 5.3939, 'grad_norm': 3.7682411670684814, 'learning_rate': 2e-05, 'epoch': 1.88}
{'loss': 5.6074, 'grad_norm': 4.110699653625488, 'learning_rate': 2.4e-05, 'epoch': 2.26}
{'loss': 5.5638, 'grad_norm': 4.070244789123535, 'learning_rate': 2.8000000000000003e-05, 'epoch': 2.64}
{'loss': 5.1217, 'grad_norm': 4.688042163848877, 'learning_rate': 3.2000000000000005e-05, 'epoch': 3.01}
{'loss': 4.9955, 'grad_norm': 4.682807922363281, 'learning_rate': 3.6e-05, 'epoch': 3.39}
{'loss': 5.123, 'grad_norm': 4.98316764831543, 'learning_rate': 4e-05, 'epoch': 3.76}
{'loss': 4.8217, 'grad_norm': 4.796319961547852, 'learning_rate': 4.4000000000000006e-05, 'epoch': 4.14}
{'loss': 4.5927, 'grad_norm': 5.590182781219482, 'learning_rate': 4.8e-05, 'epoch': 4.52}
{'loss': 4.4791, 'grad_norm': 5.397355556488037, 'learning_rate': 5.2000000000000004e-05, 'epoch': 4.89}
{'loss': 4.2579, 'grad_norm': 5.197486400604248, 'learning_rate': 5.6000000000000006e-05, 'epoch': 5.27}
{'loss': 3.2139, 'grad_norm': 5.027170658111572, 'learning_rate': 6e-05, 'epoch': 5.65}
{'loss': 4.1088, 'grad_norm': 5.199533462524414, 'learning_rate': 6.400000000000001e-05, 'epoch': 6.02}
{'loss': 3.2764, 'grad_norm': 4.823991775512695, 'learning_rate': 6.800000000000001e-05, 'epoch': 6.4}
{'loss': 2.6284, 'grad_norm': 3.198786973953247, 'learning_rate': 7.2e-05, 'epoch': 6.78}
{'loss': 2.8721, 'grad_norm': 3.6798272132873535, 'learning_rate': 7.6e-05, 'epoch': 7.15}
{'loss': 2.3119, 'grad_norm': 3.153724193572998, 'learning_rate': 8e-05, 'epoch': 7.53}
{'train_runtime': 213.2283, 'train_samples_per_second': 3.986, 'train_steps_per_second': 0.094, 'train_loss': 4.532307648658753, 'epoch': 7.53}
[1;34mwandb[0m: üöÄ View run [33mllama2-7b-hf-chat_ZsRE_85[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/f1pmmyiw[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_051425-f1pmmyiw/logs[0m
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:09<00:09,  9.80s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  5.99s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  6.56s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_2/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_2/eval/adv_train/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_2/eval/adv_train/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_2/eval/adv_train/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_2/eval/adv_train/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_2/eval/adv_train/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_2/eval/adv_train/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_2/eval/adv_train/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_2/eval/adv_train/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_2/eval/adv_train/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_2/eval/adv_train/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_2/eval/adv_train/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_2/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_2/eval/GCG/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_2/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_2/eval/mix_eval_freeform_0811/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_2/eval/mix_eval_freeform_0811/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_2/eval/mix_eval_freeform_0811/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_2/eval/mix_eval_freeform_0811/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_2/eval/mix_eval_freeform_0811/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_2/eval/mix_eval_freeform_0811/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_2/eval/mix_eval_freeform_0811/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_2/eval/mix_eval_freeform_0811/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_2/eval/mix_eval_freeform_0811/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_2/eval/mix_eval_freeform_0811/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_2/eval/mix_eval_freeform_0811/results.json
550  to  600
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_2/eval/mix_eval_freeform_0811/results.json
600  to  650
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_2/eval/mix_eval_freeform_0811/results.json
650  to  700
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_2/eval/mix_eval_freeform_0811/results.json
700  to  750
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_2/eval/mix_eval_freeform_0811/results.json
750  to  800
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_2/eval/mix_eval_freeform_0811/results.json
800  to  850
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_2/eval/mix_eval_freeform_0811/results.json
850  to  900
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_2/eval/mix_eval_freeform_0811/results.json
900  to  950
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_2/eval/mix_eval_freeform_0811/results.json
950  to  1000
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_2/eval/mix_eval_freeform_0811/results.json
1000  to  1050
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_2/eval/mix_eval_freeform_0811/results.json
1050  to  1100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_2/eval/mix_eval_freeform_0811/results.json
Skipping file: ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_2/log.txt
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_2/adapter_config.json
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_2/wandb
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_2/README.md
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_2/checkpoint-20
Skipping directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_2/eval
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_2/adapter_model.safetensors
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_2/special_tokens_map.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_2/tmp_data.jsonl
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_2/tokenizer_config.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_85/part_2/tokenizer.model
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=2, data_source=ZsRE, data_size=90, random=False
Loading data from ../../data/edit_data/merged_data_part_2.json
Loaded 1167 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 435 entries.
Selected 90 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_2/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/llama2-7b-hf-chat
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_2
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: llama2-7b-hf-chat_ZsRE_90
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:06<00:06,  6.84s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.20s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.60s/it]
Using custom data configuration default-11ab931c94692025
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-11ab931c94692025/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 4804.47it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 392.95it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-11ab931c94692025/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 537.94it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f52f4152280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 23,199,744 || all params: 6,761,615,360 || trainable%: 0.34310949033338684
0ex [00:00, ?ex/s]90ex [00:00, 1983.53ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_052401-2p2fjj9j
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run llama2-7b-hf-chat_ZsRE_90
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/2p2fjj9j

Example:
<s> What voice type is Piero de Palma? soprano</s>


Example:
<s> What voice type is Licia Albanese? mezzo soprano</s>


Example:
<s> Which country's citizenship does Javier Beltr√°n hold? Paraguay</s>


Example:
<s> What is the status of Hyloxalus parcus? vulnerable</s>


Example:
<s> What is the operating system used with Brain Fuck Scheduler? Android</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 90
})
  0%|          | 0/20 [00:00<?, ?it/s]  5%|‚ñå         | 1/20 [00:11<03:30, 11.08s/it]                                                5%|‚ñå         | 1/20 [00:11<03:30, 11.08s/it] 10%|‚ñà         | 2/20 [00:21<03:12, 10.69s/it]                                               10%|‚ñà         | 2/20 [00:21<03:12, 10.69s/it] 15%|‚ñà‚ñå        | 3/20 [00:31<02:59, 10.57s/it]                                               15%|‚ñà‚ñå        | 3/20 [00:31<02:59, 10.57s/it] 20%|‚ñà‚ñà        | 4/20 [00:42<02:48, 10.52s/it]                                               20%|‚ñà‚ñà        | 4/20 [00:42<02:48, 10.52s/it] 25%|‚ñà‚ñà‚ñå       | 5/20 [00:52<02:37, 10.48s/it]                                               25%|‚ñà‚ñà‚ñå       | 5/20 [00:52<02:37, 10.48s/it] 30%|‚ñà‚ñà‚ñà       | 6/20 [01:03<02:26, 10.47s/it]                                               30%|‚ñà‚ñà‚ñà       | 6/20 [01:03<02:26, 10.47s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:13<02:15, 10.45s/it]                                               35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:13<02:15, 10.45s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:24<02:05, 10.45s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:24<02:05, 10.45s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:34<01:54, 10.44s/it]                                               45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:34<01:54, 10.44s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:44<01:44, 10.44s/it]                                                50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:44<01:44, 10.44s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [01:55<01:33, 10.44s/it]                                                55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [01:55<01:33, 10.44s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:05<01:23, 10.43s/it]                                                60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:05<01:23, 10.43s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:16<01:13, 10.44s/it]                                                65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:16<01:13, 10.44s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:26<01:02, 10.43s/it]                                                70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:26<01:02, 10.43s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:37<00:52, 10.43s/it]                                                75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:37<00:52, 10.43s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:47<00:41, 10.43s/it]                                                80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:47<00:41, 10.43s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [02:57<00:31, 10.44s/it]                                                85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [02:57<00:31, 10.44s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:08<00:20, 10.44s/it]                                                90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:08<00:20, 10.44s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:18<00:10, 10.44s/it]                                                95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:18<00:10, 10.44s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:29<00:00, 10.43s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:29<00:00, 10.43s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:30<00:00, 10.43s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:30<00:00, 10.54s/it]
{'loss': 5.5489, 'grad_norm': 3.8063483238220215, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.36}
{'loss': 5.8381, 'grad_norm': 4.219547271728516, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.71}
{'loss': 5.3308, 'grad_norm': 3.773726463317871, 'learning_rate': 1.2e-05, 'epoch': 1.07}
{'loss': 5.467, 'grad_norm': 4.204000473022461, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.42}
{'loss': 5.5608, 'grad_norm': 3.9664316177368164, 'learning_rate': 2e-05, 'epoch': 1.78}
{'loss': 5.7398, 'grad_norm': 4.065417289733887, 'learning_rate': 2.4e-05, 'epoch': 2.13}
{'loss': 6.0893, 'grad_norm': 4.586862087249756, 'learning_rate': 2.8000000000000003e-05, 'epoch': 2.49}
{'loss': 4.8148, 'grad_norm': 4.058540344238281, 'learning_rate': 3.2000000000000005e-05, 'epoch': 2.84}
{'loss': 4.74, 'grad_norm': 4.456271648406982, 'learning_rate': 3.6e-05, 'epoch': 3.2}
{'loss': 4.8314, 'grad_norm': 5.079740047454834, 'learning_rate': 4e-05, 'epoch': 3.56}
{'loss': 5.3873, 'grad_norm': 5.29828405380249, 'learning_rate': 4.4000000000000006e-05, 'epoch': 3.91}
{'loss': 4.9458, 'grad_norm': 5.600948810577393, 'learning_rate': 4.8e-05, 'epoch': 4.27}
{'loss': 4.1242, 'grad_norm': 4.742436408996582, 'learning_rate': 5.2000000000000004e-05, 'epoch': 4.62}
{'loss': 4.3242, 'grad_norm': 5.795664310455322, 'learning_rate': 5.6000000000000006e-05, 'epoch': 4.98}
{'loss': 4.0479, 'grad_norm': 5.8519463539123535, 'learning_rate': 6e-05, 'epoch': 5.33}
{'loss': 3.5013, 'grad_norm': 4.625606536865234, 'learning_rate': 6.400000000000001e-05, 'epoch': 5.69}
{'loss': 3.2862, 'grad_norm': 3.7375237941741943, 'learning_rate': 6.800000000000001e-05, 'epoch': 6.04}
{'loss': 3.0482, 'grad_norm': 3.5045318603515625, 'learning_rate': 7.2e-05, 'epoch': 6.4}
{'loss': 2.736, 'grad_norm': 3.3512930870056152, 'learning_rate': 7.6e-05, 'epoch': 6.76}
{'loss': 2.5851, 'grad_norm': 3.563145875930786, 'learning_rate': 8e-05, 'epoch': 7.11}
{'train_runtime': 212.7981, 'train_samples_per_second': 4.229, 'train_steps_per_second': 0.094, 'train_loss': 4.597351324558258, 'epoch': 7.11}
[1;34mwandb[0m: üöÄ View run [33mllama2-7b-hf-chat_ZsRE_90[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/2p2fjj9j[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_052401-2p2fjj9j/logs[0m
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:09<00:09,  9.75s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  5.98s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  6.55s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_2/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_2/eval/adv_train/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_2/eval/adv_train/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_2/eval/adv_train/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_2/eval/adv_train/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_2/eval/adv_train/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_2/eval/adv_train/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_2/eval/adv_train/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_2/eval/adv_train/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_2/eval/adv_train/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_2/eval/adv_train/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_2/eval/adv_train/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_2/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_2/eval/GCG/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_2/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_2/eval/mix_eval_freeform_0811/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_2/eval/mix_eval_freeform_0811/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_2/eval/mix_eval_freeform_0811/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_2/eval/mix_eval_freeform_0811/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_2/eval/mix_eval_freeform_0811/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_2/eval/mix_eval_freeform_0811/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_2/eval/mix_eval_freeform_0811/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_2/eval/mix_eval_freeform_0811/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_2/eval/mix_eval_freeform_0811/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_2/eval/mix_eval_freeform_0811/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_2/eval/mix_eval_freeform_0811/results.json
550  to  600
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_2/eval/mix_eval_freeform_0811/results.json
600  to  650
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_2/eval/mix_eval_freeform_0811/results.json
650  to  700
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_2/eval/mix_eval_freeform_0811/results.json
700  to  750
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_2/eval/mix_eval_freeform_0811/results.json
750  to  800
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_2/eval/mix_eval_freeform_0811/results.json
800  to  850
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_2/eval/mix_eval_freeform_0811/results.json
850  to  900
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_2/eval/mix_eval_freeform_0811/results.json
900  to  950
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_2/eval/mix_eval_freeform_0811/results.json
950  to  1000
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_2/eval/mix_eval_freeform_0811/results.json
1000  to  1050
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_2/eval/mix_eval_freeform_0811/results.json
1050  to  1100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_2/eval/mix_eval_freeform_0811/results.json
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_2/wandb
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_2/checkpoint-20
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_2/adapter_model.safetensors
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_2/README.md
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_2/special_tokens_map.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_2/tmp_data.jsonl
Skipping file: ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_2/log.txt
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_2/adapter_config.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_2/tokenizer.model
Skipping directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_2/eval
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_90/part_2/tokenizer_config.json
Cleanup completed.
W&B online. Running your script from this directory will now sync to the cloud.
Arguments received: data_part=2, data_source=ZsRE, data_size=95, random=False
Loading data from ../../data/edit_data/merged_data_part_2.json
Loaded 1167 entries from the data file.
Filtered data based on source 'ZsRE', resulting in 435 entries.
Selected 95 entries based on data size.
Saving formatted data to tmp_data.jsonl
Data successfully saved to tmp_data.jsonl
rm: cannot remove '../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_2/tmp_data.jsonl': No such file or directory
Params using prompt template alpaca:
base_model: ../../.hf_cache/llama2-7b-hf-chat
data_path: ./tmp_data.jsonl
output_dir: ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_2
batch_size: 32
micro_batch_size: 1
num_epochs: 10
learning_rate: 0.0004
cutoff_len: 4096
val_set_size: 0
lr_scheduler: cosine
warmup_steps: 100
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['gate_proj', 'down_proj', 'up_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: llm-edit
wandb_run_name: llama2-7b-hf-chat_ZsRE_95
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt_format: instruction
p_to_be_unnatural: 0

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:06<00:06,  6.58s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  4.09s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  4.46s/it]
Using custom data configuration default-341a2f1a31f4d907
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1 2 None
Not using system message
Downloading and preparing dataset json/default to /home/k/kduan/.cache/huggingface/datasets/json/default-341a2f1a31f4d907/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 8905.10it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 299.59it/s]
Dataset json downloaded and prepared to /home/k/kduan/.cache/huggingface/datasets/json/default-341a2f1a31f4d907/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 503.34it/s]
Parameter 'function'=<function train.<locals>.<lambda> at 0x7f0c88336280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
trainable params: 23,199,744 || all params: 6,761,615,360 || trainable%: 0.34310949033338684
0ex [00:00, ?ex/s]42ex [00:00, 418.16ex/s]95ex [00:00, 701.97ex/s]
/home/k/kduan/miniconda3/envs/ee/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: 1641225799szn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /home/k/kduan/szn_workspace/Safety_Eval_Over_Edited_LLM/experiment/Qlora/wandb/run-20241105_053335-mva4xuhe
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run llama2-7b-hf-chat_ZsRE_95
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1641225799szn/llm-edit
wandb: üöÄ View run at https://wandb.ai/1641225799szn/llm-edit/runs/mva4xuhe

Example:
<s> Who was Dancing Brave's mother? Danehill Lady</s>


Example:
<s> What type of voice does Deborah York have? mezzo-oprano</s>


Example:
<s> What is the name of Last Stop Suburbia's record label? Def Jam Recordings</s>


Example:
<s> The mother of Maria Antonia Ferdinanda of Spain is whom? Maria Christina of Austria</s>


Example:
<s> What is the endangered status of Javan surili? critically threatened</s>

num_epochs was set to 10
Dataset({
    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 95
})
  0%|          | 0/20 [00:00<?, ?it/s]  5%|‚ñå         | 1/20 [00:11<03:37, 11.46s/it]                                                5%|‚ñå         | 1/20 [00:11<03:37, 11.46s/it] 10%|‚ñà         | 2/20 [00:21<03:15, 10.85s/it]                                               10%|‚ñà         | 2/20 [00:21<03:15, 10.85s/it] 15%|‚ñà‚ñå        | 3/20 [00:32<03:01, 10.66s/it]                                               15%|‚ñà‚ñå        | 3/20 [00:32<03:01, 10.66s/it] 20%|‚ñà‚ñà        | 4/20 [00:42<02:48, 10.56s/it]                                               20%|‚ñà‚ñà        | 4/20 [00:42<02:48, 10.56s/it] 25%|‚ñà‚ñà‚ñå       | 5/20 [00:53<02:37, 10.52s/it]                                               25%|‚ñà‚ñà‚ñå       | 5/20 [00:53<02:37, 10.52s/it] 30%|‚ñà‚ñà‚ñà       | 6/20 [01:03<02:26, 10.49s/it]                                               30%|‚ñà‚ñà‚ñà       | 6/20 [01:03<02:26, 10.49s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:13<02:16, 10.46s/it]                                               35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [01:13<02:16, 10.46s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:24<02:05, 10.46s/it]                                               40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:24<02:05, 10.46s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:34<01:54, 10.44s/it]                                               45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:34<01:54, 10.44s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:45<01:44, 10.44s/it]                                                50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:45<01:44, 10.44s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [01:55<01:33, 10.43s/it]                                                55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [01:55<01:33, 10.43s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:06<01:23, 10.43s/it]                                                60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:06<01:23, 10.43s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:16<01:13, 10.43s/it]                                                65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:16<01:13, 10.43s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:27<01:02, 10.43s/it]                                                70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:27<01:02, 10.43s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:37<00:52, 10.43s/it]                                                75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:37<00:52, 10.43s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:47<00:41, 10.43s/it]                                                80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:47<00:41, 10.43s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [02:58<00:31, 10.43s/it]                                                85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [02:58<00:31, 10.43s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:08<00:20, 10.43s/it]                                                90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:08<00:20, 10.43s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:19<00:10, 10.43s/it]                                                95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:19<00:10, 10.43s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:29<00:00, 10.42s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:29<00:00, 10.42s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:31<00:00, 10.42s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:31<00:00, 10.56s/it]
{'loss': 5.356, 'grad_norm': 3.8450753688812256, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.34}
{'loss': 6.1629, 'grad_norm': 4.494449138641357, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.67}
{'loss': 5.3868, 'grad_norm': 3.676223039627075, 'learning_rate': 1.2e-05, 'epoch': 1.01}
{'loss': 5.9234, 'grad_norm': 4.24045991897583, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.35}
{'loss': 5.4017, 'grad_norm': 4.033417701721191, 'learning_rate': 2e-05, 'epoch': 1.68}
{'loss': 5.4807, 'grad_norm': 3.8897809982299805, 'learning_rate': 2.4e-05, 'epoch': 2.02}
{'loss': 5.4981, 'grad_norm': 4.233747959136963, 'learning_rate': 2.8000000000000003e-05, 'epoch': 2.36}
{'loss': 5.2331, 'grad_norm': 4.619316101074219, 'learning_rate': 3.2000000000000005e-05, 'epoch': 2.69}
{'loss': 5.3996, 'grad_norm': 4.781828880310059, 'learning_rate': 3.6e-05, 'epoch': 3.03}
{'loss': 5.7821, 'grad_norm': 5.707202911376953, 'learning_rate': 4e-05, 'epoch': 3.37}
{'loss': 4.707, 'grad_norm': 4.423597812652588, 'learning_rate': 4.4000000000000006e-05, 'epoch': 3.71}
{'loss': 3.9872, 'grad_norm': 4.958690166473389, 'learning_rate': 4.8e-05, 'epoch': 4.04}
{'loss': 4.1854, 'grad_norm': 5.174388408660889, 'learning_rate': 5.2000000000000004e-05, 'epoch': 4.38}
{'loss': 4.657, 'grad_norm': 5.879471302032471, 'learning_rate': 5.6000000000000006e-05, 'epoch': 4.72}
{'loss': 3.8702, 'grad_norm': 4.925384521484375, 'learning_rate': 6e-05, 'epoch': 5.05}
{'loss': 2.9849, 'grad_norm': 4.13720703125, 'learning_rate': 6.400000000000001e-05, 'epoch': 5.39}
{'loss': 3.382, 'grad_norm': 4.327221393585205, 'learning_rate': 6.800000000000001e-05, 'epoch': 5.73}
{'loss': 3.6225, 'grad_norm': 4.663585662841797, 'learning_rate': 7.2e-05, 'epoch': 6.06}
{'loss': 2.7694, 'grad_norm': 3.3772408962249756, 'learning_rate': 7.6e-05, 'epoch': 6.4}
{'loss': 2.5036, 'grad_norm': 2.9606716632843018, 'learning_rate': 8e-05, 'epoch': 6.74}
{'train_runtime': 213.1897, 'train_samples_per_second': 4.456, 'train_steps_per_second': 0.094, 'train_loss': 4.6146671056747435, 'epoch': 6.74}
[1;34mwandb[0m: üöÄ View run [33mllama2-7b-hf-chat_ZsRE_95[0m at: [34mhttps://wandb.ai/1641225799szn/llm-edit/runs/mva4xuhe[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241105_053335-mva4xuhe/logs[0m
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:09<00:09,  9.67s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:12<00:00,  5.92s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:12<00:00,  6.48s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
We are now merging the lora adapter to base model
--------------------------------------------------











Now we start evaluating
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_2/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_2/eval/adv_train/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_2/eval/adv_train/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_2/eval/adv_train/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_2/eval/adv_train/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_2/eval/adv_train/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_2/eval/adv_train/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_2/eval/adv_train/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_2/eval/adv_train/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_2/eval/adv_train/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_2/eval/adv_train/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_2/eval/adv_train/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_2/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_2/eval/GCG/results.json
Data Path:  ../../data/eval_data/merged_data_2024-11-04.json
Output Path:  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_2/eval
0  to  50
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_2/eval/mix_eval_freeform_0811/results.json
50  to  100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_2/eval/mix_eval_freeform_0811/results.json
100  to  150
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_2/eval/mix_eval_freeform_0811/results.json
150  to  200
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_2/eval/mix_eval_freeform_0811/results.json
200  to  250
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_2/eval/mix_eval_freeform_0811/results.json
250  to  300
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_2/eval/mix_eval_freeform_0811/results.json
300  to  350
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_2/eval/mix_eval_freeform_0811/results.json
350  to  400
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_2/eval/mix_eval_freeform_0811/results.json
400  to  450
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_2/eval/mix_eval_freeform_0811/results.json
450  to  500
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_2/eval/mix_eval_freeform_0811/results.json
500  to  550
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_2/eval/mix_eval_freeform_0811/results.json
550  to  600
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_2/eval/mix_eval_freeform_0811/results.json
600  to  650
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_2/eval/mix_eval_freeform_0811/results.json
650  to  700
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_2/eval/mix_eval_freeform_0811/results.json
700  to  750
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_2/eval/mix_eval_freeform_0811/results.json
750  to  800
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_2/eval/mix_eval_freeform_0811/results.json
800  to  850
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_2/eval/mix_eval_freeform_0811/results.json
850  to  900
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_2/eval/mix_eval_freeform_0811/results.json
900  to  950
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_2/eval/mix_eval_freeform_0811/results.json
950  to  1000
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_2/eval/mix_eval_freeform_0811/results.json
1000  to  1050
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_2/eval/mix_eval_freeform_0811/results.json
1050  to  1100
generating!
writing to  ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_2/eval/mix_eval_freeform_0811/results.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_2/tokenizer.model
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_2/README.md
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_2/tmp_data.jsonl
Skipping file: ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_2/log.txt
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_2/wandb
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_2/special_tokens_map.json
Deleted directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_2/checkpoint-20
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_2/adapter_config.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_2/tokenizer_config.json
Deleted file: ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_2/adapter_model.safetensors
Skipping directory: ../../results/lora/llama2-7b-hf-chat/ZsRE_95/part_2/eval
Cleanup completed.
